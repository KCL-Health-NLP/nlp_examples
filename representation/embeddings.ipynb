{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OjBWQtuFo0w"
      },
      "source": [
        "## Word embeddings\n",
        "*(Credit: Leon Derczynski, IT University of Copenhagen)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jahAH9FcFo0z"
      },
      "source": [
        "Let's load some embeddings, and then use these to see which words are close to each other.\n",
        "We'll use the gensim package's word2vec implementation, and an nltk corpus. We also need to download punkt - an nltk tokeniser used by the movie_reviews corpus. And we'll use seaborn to visualise embeddings as heatmaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNhihIEDFo00"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "from nltk.corpus import brown, movie_reviews\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# We'll use seaborn to visualise embeddings as heatmaps\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL-pGzhdFo04"
      },
      "source": [
        "Let's generate word vectors over the Brown corpus text. We will have 20 dimensions, using a window of three for the context words in the skip-grams (e.g. c1, c2, w, c3, c4). This might be a little slow (maybe 1-2 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFlJ2Tz0Fo05"
      },
      "outputs": [],
      "source": [
        "# for the Brown corpus\n",
        "b = Word2Vec(brown.sents(), vector_size=20, window=3, min_count=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOpnc7g9Fo07"
      },
      "source": [
        "Now we have the vectors, we can see how good they are by measuring which words are similar to each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQIw3mJvFo08"
      },
      "outputs": [],
      "source": [
        "b.wv.most_similar('company', topn=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBSdnlU4Fo1C"
      },
      "source": [
        "Not great, eh? Try altering the window and the dimension size, to see if you get better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0b2OJTOFo1D"
      },
      "source": [
        "Try also with the movie reviews results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad2VWdpcFo1D"
      },
      "outputs": [],
      "source": [
        "# for the movie review corpus\n",
        "mr = Word2Vec(movie_reviews.sents(), vector_size=20, window=5, min_count=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01mNdhm8Fo1I"
      },
      "outputs": [],
      "source": [
        "mr.wv.most_similar('love', topn=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTYsddU5Fo1L"
      },
      "source": [
        "We can also do some arithmetic with the words. Let's try that classical result, king - man + woman."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjrmbgvCFo1M"
      },
      "outputs": [],
      "source": [
        "b.wv.most_similar(positive=['biggest', 'small'], negative=['big'], topn=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyFNDhcJFo1Q"
      },
      "source": [
        "Not a perfect result with the default model! Why don't we try loading a bigger dataset, based on a bigger vocabulary. This should give better results. You'll need the GloVe embeddings for this.\n",
        "\n",
        "We will download this from a github repository. If you are running this on your own local computer (rather then Colaboratory) you can download from www.derczynski.com/glove.twitter.27B.25d.txt.bz2 to your machine. In this case, there is no need to run the next cell - just replace the file name in the cell after next with the path to your downloaded file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqhoHpJWFo1R"
      },
      "outputs": [],
      "source": [
        "!git clone --quiet https://github.com/KCL-Health-NLP/nlp_examples.git\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "print(\"Done copying files\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrQFNE0oyYSU"
      },
      "source": [
        "Now let's load the model file. This might take a few minutes. If you are using a copy on your own local machine, change the file path below to that of your file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4qKmtOuypp6"
      },
      "outputs": [],
      "source": [
        "glove = KeyedVectors.load_word2vec_format(\"nlp_examples/representation/glove.twitter.27B.25d.txt.bz2\", binary=False)\n",
        "print(\"Done loading\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26LIcnK9Fo1V"
      },
      "source": [
        "Now, try the above again. Can you find any cool word combinations? What differences are there in the datasets?\n",
        "\n",
        "Here are some ideas to try, substitute your own words in to these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0tsWpenFo1W"
      },
      "outputs": [],
      "source": [
        "glove.most_similar('meat', topn=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwhIzDHeFo1Z"
      },
      "outputs": [],
      "source": [
        "glove.most_similar(positive=['biggest', 'small'], negative=['big'], topn=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsnpDVSOFo1d"
      },
      "outputs": [],
      "source": [
        "glove.most_similar(positive=['woman', 'king'], negative=['man'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9xvpKT2Fo1i"
      },
      "outputs": [],
      "source": [
        "glove.similarity('car', 'bike')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzUGyMgpFo1l"
      },
      "outputs": [],
      "source": [
        "glove.similarity('car', 'purple')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wv_i2NIyFo1p"
      },
      "outputs": [],
      "source": [
        "glove.similarity('red', 'purple')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNyv2H2NFo1t"
      },
      "outputs": [],
      "source": [
        "glove.doesnt_match(\"breakfast cereal dinner lunch\".split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPOmgAzXFo1x"
      },
      "outputs": [],
      "source": [
        "glove.doesnt_match(\"red green horse blue\".split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-vQBPAwadgI"
      },
      "source": [
        "What about ambiguous words? Can you think of any and try them? Past suggestions have been cancer, bank and play. Can you find any others, and explain what is going on? How does the embedding deal with ambiguity? What factors influence this?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE8jPa8Wa-xk"
      },
      "outputs": [],
      "source": [
        "glove.most_similar('word')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMjn2ZIbXFqM"
      },
      "source": [
        "What do these embeddings look like? We will display embeddings for four words: two colour adjectives, and two action verbs. Each column is the enbedding for one word. We have printed to two decimal places, using Python string formatting. Can you spot any similarities and differences?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNf_9myuFo12"
      },
      "outputs": [],
      "source": [
        "print(\"   red      green             walk    run\\n\")\n",
        "for i in range(len(glove['red'])):\n",
        "  print(\"%8.2f%8.2f          %8.2f%8.2f\" % (glove['red'][i], glove['green'][i], glove['walk'][i], glove['run'][i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4Zj6_21B8tE"
      },
      "source": [
        "Let's visualise this as a heatmap, using seaborn (imported as sns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3E_X69kEB8tE"
      },
      "outputs": [],
      "source": [
        "\n",
        "sns.heatmap([glove['red'], glove['green'], glove['walk'], glove['run']],\n",
        "            cmap = 'coolwarm', vmin = -2, vmax = 1.5,\n",
        "            yticklabels=['red', 'green', 'walk', 'run'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjo-IFo5X-54"
      },
      "source": [
        "How do we use these embeddings in NLP? The usual way is to replace each occurence of a word with an embedding - it represents our word. The example below displays what we would pass to our algorithm for a sentence. We show one line for each word, with each value formatted to two decimal places again. The word is displayed at the start of the line for convenience only - this would not be passed to our algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b-1ZJ9kP1uX"
      },
      "outputs": [],
      "source": [
        "sentence=[\"the\", \"quick\", \"brown\", \"fox\", \"jumped\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
        "embeddings = []\n",
        "for i in sentence:\n",
        "  embeddings.append(glove[i])\n",
        "\n",
        "for i, val in enumerate(embeddings):\n",
        "  print(sentence[i].ljust(10), ''.join(\"{:6.2f}\".format(x) for x in val))\n",
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "8-embeddings.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}