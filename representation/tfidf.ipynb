{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "opW3IYevdSPH"
   },
   "source": [
    "# Term frequency x Inverse document frequency - TfIdf\n",
    "\n",
    "With acknowledgement to Mayank Tripathi https://github.com/mayank408\n",
    "\n",
    "We will build on the BoW model creating a TfIdf model from first principles.\n",
    "\n",
    "First a couple imports: \"string\" to do some string manipulation, pprint and pandas to help us print our data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IO63wordf-EM"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import pprint as pp\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yV4f7Rqj5ppo"
   },
   "source": [
    "Now our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lrVUGUlSdYXT"
   },
   "outputs": [],
   "source": [
    "documents = ['Klonopin 0.25 mg po every evening, Fluconazole 200 mg po daily, Synthroid 125 mcg po every day',\n",
    "             'she will not consider switching to clozapine',\n",
    "             'lovastatin 40 mg one half tab po daily, multivitamin daily, metformin 500 mg one tab po twice a day',\n",
    "             'Aspirin 81 mg po once daily, Zoloft 25 mg po once daily, Calcium with vitamin D two tablets po once daily']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cxiuMhKR55EJ"
   },
   "source": [
    "We will \"normalise\" our documents, to lower case them and remove punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qLVLgHlCfyFN"
   },
   "outputs": [],
   "source": [
    "normalised_documents = []\n",
    "for i in documents:\n",
    "    no_punctuation = ''.join(c for c in i if c not in string.punctuation)\n",
    "    normalised_documents.append(no_punctuation.lower())\n",
    "    \n",
    "for i in normalised_documents:\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m1tQ6ulj6Mhv"
   },
   "source": [
    "Let's split in to tokens, to give our \"bags\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jT5GUoqdfzW2"
   },
   "outputs": [],
   "source": [
    "bows = []\n",
    "for i in normalised_documents:\n",
    "    bows.append(i.split(' '))\n",
    "\n",
    "print(bows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BRg3KbQG6Yh3"
   },
   "source": [
    "We need to get a set containing all of our unique words, so that we can calculate their relative \n",
    "frequencies in each document and across all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6-xP7G2hgJ7n"
   },
   "outputs": [],
   "source": [
    "word_set = set()\n",
    "for i in bows:\n",
    "  word_set = word_set.union(set(i))\n",
    "print(word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R-0xW8NyhDpY"
   },
   "source": [
    "Let's count how many of each word we have for each of our bags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJ_E4ShfhD-Z"
   },
   "outputs": [],
   "source": [
    "wordCounts = []\n",
    "for i in bows:\n",
    "  thisWordCount = dict.fromkeys(word_set, 0)\n",
    "  for word in i:\n",
    "    thisWordCount[word]+=1\n",
    "  wordCounts.append(thisWordCount)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUUrSFi4iw9F"
   },
   "source": [
    "Let's take a look at these counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6a_S6GaOjCuo"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(wordCounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NNNNFp1vjQ9P"
   },
   "source": [
    "Now we will define ***Term Frequency*** (TF) as the relative frequency of a word in a bag (document) - i.e. what fraction of all words in a document is a particular word? We will define a function to compute this for all of the words in a bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-zV64MVnjRc4"
   },
   "outputs": [],
   "source": [
    "def computeTF(wordCount, bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordCount.items():\n",
    "        tfDict[word] = count/float(bowCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zvu_e1kfjThC"
   },
   "source": [
    "We will run this function over all of our bags (documents), and put the resulting TFs in a single data structure. Tale a look and see how documents differ, and how the TFs reflect relative occurence of a word in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p3Ip44KXjT6w"
   },
   "outputs": [],
   "source": [
    "termFreqs = []\n",
    "for i in range(0,len(bows)): \n",
    "  termFreqs.append(computeTF(wordCounts[i],bows[i]))\n",
    "\n",
    "pd.DataFrame(termFreqs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "29ooQaFts7FC"
   },
   "source": [
    "Our next function defines ***Inverse Docuemnt Frequency*** - IDF. This measures the rareness of a word across our whole collection of documents. For each word, we divide the total number of documents by the number containing that word. We take the log of this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M-Htg6sss7Wh"
   },
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "        \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GObZQfcxs9Ip"
   },
   "source": [
    "Now we compute IDF for our words. Take a look at the difference between common words like \"mg\" and rare ones like drug names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anbbuwGSs9re"
   },
   "outputs": [],
   "source": [
    "idfs = computeIDF(wordCounts)\n",
    "pp.pprint(idfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the IDFs look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(idfs).sort_index(ascending=False).plot.barh(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you explain the range of values that IDF takes, and the maximum value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kx_lMfqg4heF"
   },
   "source": [
    "Let's define a function to put TF and IDF together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OmQ0Vc_t4iGZ"
   },
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zIakOU0o4rJx"
   },
   "source": [
    "And now run this over all of the documents in our term frequency list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YKwEFIRb4rvy"
   },
   "outputs": [],
   "source": [
    "tfidfs = []\n",
    "for i in termFreqs:\n",
    "  tfidfs.append(computeTFIDF(i, idfs))\n",
    "  \n",
    "\n",
    "  \n",
    "pd.DataFrame(tfidfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1QH3pqWI8IU3"
   },
   "source": [
    "How do these compare to the term frequencies? Run the next line to get just the TFs. What differences are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7_b7x7kv8Iyk"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(termFreqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q9iSIllZ-wSZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tfidf.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
