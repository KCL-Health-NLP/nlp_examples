{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1SopxIqBOXf"
      },
      "source": [
        "# Introduction to NLP with Python\n",
        "\n",
        "A brief introduction to useful Python packages and functions for NLP and data manipulation, and some initial exploration of some example data.\n",
        "\n",
        "Written by Sumithra Velupillai June 2019, updated January 2021\n",
        "\n",
        "Some of the material from or inspired by: https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNViyxnRBOXg"
      },
      "source": [
        "## Working with datasets and packages\n",
        "\n",
        "Importing packages you need for a project is the first step.\n",
        "\n",
        "pandas is a very useful package for working with datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtDkRURZubA6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dat55398BOXi"
      },
      "source": [
        "## Data visualization\n",
        "\n",
        "There are many packages for visualisations in Python - matplotlib is one of the core packages, \n",
        "\n",
        "seaborn has some very nice functionalities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPuy-5MWBOXi"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxg_atjNBOXi"
      },
      "source": [
        "## NLP\n",
        "\n",
        "Two main NLP packages are commonly used when working in the Python environment:\n",
        "\n",
        "spaCy: https://spacy.io/\n",
        "\n",
        "nltk: https://www.nltk.org/\n",
        "\n",
        "We will load some of their functions and resources too for the English language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F66NQ8y4BOXj"
      },
      "source": [
        "spaCy has a default language model for English that we will load into the variable 'nlp'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeOOoBxqBOXj"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrNF6KfBBOXj"
      },
      "source": [
        "nltk has many functions for language processing tasks. Here we will use their stopwords, their definition of punctuation, and one of their word tokenizers and one of their lemmatizers, as well as data needed for these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzlLmzcIBOXj"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMpQso98BOXj"
      },
      "source": [
        "## Other useful packages\n",
        "\n",
        "We will also load some packages from one of the most commonly used machine learning libraries: \n",
        "\n",
        "scikit-learn: https://scikit-learn.org/stable/\n",
        "\n",
        "And some other useful packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0v5i2ysxBOXk"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline\n",
        "#matplotlib.rcParams['figure.figsize'] = (10.0, 6.0)\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW4LArs-BOXk"
      },
      "source": [
        "## Data\n",
        "\n",
        "We'll start by opening some data. This is data we will also use in the machine learning classification practical later this week. The data are medical transcriptions from mtsamples: https://www.mtsamples.com/\n",
        "\n",
        "We have selected a subset of documents just to explore and try to get an idea of what is in them in terms of texts. The data has been saved in a spreadsheet that we'll open from a URL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyNps4Wkubvg"
      },
      "outputs": [],
      "source": [
        "xlds = 'https://github.com/KCL-Health-NLP/nlp_examples/blob/master/classification/classification_trainingdata.xlsx?raw=true'\n",
        "df = pd.read_excel(xlds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6TPJXfwBOXl"
      },
      "source": [
        "How many documents do we have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYbEUgAvumK1"
      },
      "outputs": [],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjx7QLdwBOXl"
      },
      "source": [
        "What does this dataframe look like? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1z90-joBOXl"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLTa5VNSBOXl"
      },
      "source": [
        "There are a lot of newline characters that we can remove and replace with just one. This makes the data more readable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6skWq3qBOXl"
      },
      "outputs": [],
      "source": [
        "df['txt'] = df['txt'].str.replace('_x000D_\\n','\\n')\n",
        "df['txt'] = df['txt'].str.replace('\\r\\n+','\\n')\n",
        "df['txt'] = df['txt'].str.replace('\\n +','\\n')\n",
        "df['txt'] = df['txt'].str.replace('\\n+','\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0yGH58QBOXm"
      },
      "source": [
        "So there are two main columns: 'label' and 'txt'. Each row in the dataframe is thus a text with a label. What does the label indicate?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLRAcAmiBOXm"
      },
      "source": [
        "Let's look at one of the documents. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9MKvF2wxV6K"
      },
      "outputs": [],
      "source": [
        "df['txt'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inWEizBtBOXm"
      },
      "source": [
        "Do you get a sense for what these texts might look like? Try looking at some other examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Mk_7FpgBOXm"
      },
      "source": [
        "One way of exploring the data and trying to understand more about this corpus is to tokenize the corpus into its individual words, and looking at the most frequent words. Let's see how we can use spaCy and nltk for this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEo5wqJzBOXm"
      },
      "source": [
        "With spaCy, we can apply their default English language model on all texts in one go. Remember that we loaded their language model in the variable 'nlp' above. With pandas, we can apply this on all rows in the dataframe, and save in a new column 'spacynlp':"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgDrVZE0waqX"
      },
      "outputs": [],
      "source": [
        "df['spacynlp'] = df['txt'].apply(nlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUY09frYBOXn"
      },
      "source": [
        "Let's look at the first rows in the dataframe again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvcpQIqRBOXn"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg_H5E-_BOXn"
      },
      "source": [
        "In the 'spacynlp' column, we now have a spaCy representation of each text. From this representation, we can retrieve the tokens, lemmas, part-of-speech (POS) tags, and other units that the default model gives us. Information about the different lignuistic features the spaCy model currently handles can be found here: https://spacy.io/usage/linguistic-features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kULv5iqHBOXn"
      },
      "source": [
        "For this exercise, we will look at lemma and POS frequencies. We'll write some functions. We'll start by tokenizing the data into sentences and save as a column, then we'll do the same for lemmas and save in another column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f3FJUoHBOXn"
      },
      "source": [
        "The first function takes a row in a dataframe and returns a list of sentences from the SpaCy model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Onk1uer_BOXn"
      },
      "outputs": [],
      "source": [
        "def get_spacy_sentences(row):\n",
        "  return [sentence for sentence in row.doc.sents]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A8Q3jxoBOXn"
      },
      "source": [
        "We'll not apply this function and save in a new column: 'spacysentence'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glgrNv4RBOXo"
      },
      "outputs": [],
      "source": [
        "df['spacysentence'] = df['spacynlp'].apply(get_spacy_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUyRTeogBOXo"
      },
      "source": [
        "Let's define a similar function but this time we'll extract lemmas from the SpaCy model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36b08BSxCA6m"
      },
      "outputs": [],
      "source": [
        "def get_spacy_lemmas(row):\n",
        "  return [token.lemma_ for token in row.doc]\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVl4mw7lBOXo"
      },
      "source": [
        "Let's apply this function on each row in the dataframe and save in a new column: 'spacylemma'. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyjtKJRe3k9c"
      },
      "outputs": [],
      "source": [
        "df['spacylemma'] = df['spacynlp'].apply(get_spacy_lemmas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inCHzxweBOXo"
      },
      "source": [
        "Let's do something similar to get the POS tags:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iO1hlMM14Oin"
      },
      "outputs": [],
      "source": [
        "def get_spacy_pos(row):\n",
        "  return [token.pos_ for token in row.doc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDVdg5J74Sb9"
      },
      "outputs": [],
      "source": [
        "df['spacypos'] = df['spacynlp'].apply(get_spacy_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adpVh8OgBOXp"
      },
      "source": [
        "What does the dataframe look like now?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVK2q5O9BOXp"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFuPvSRSBOXp"
      },
      "outputs": [],
      "source": [
        "len(df['spacylemma'][20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4e5206uBOXp"
      },
      "source": [
        "We can look at an example column with the new list of lemmas for a random document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SatwEY3SBOXp"
      },
      "outputs": [],
      "source": [
        "df['spacylemma'][10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Tl-N2CrBOXp"
      },
      "source": [
        "Let's do this also with nltk, so that we can compare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9A4lmIUBOXp"
      },
      "source": [
        "With nltk, there are many options for different NLP tasks, including sentence tokenization, word tokenization, POS tagging, etc. The range of functions and options can be found on their webpage: https://www.nltk.org/\n",
        "\n",
        "Let's use one of their sentence and word tokenizers, and lemmatizers to get lemmas. We'll start with sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQwFF9hBBOXq"
      },
      "outputs": [],
      "source": [
        "def get_nltk_sentences(row):\n",
        "    sentences = [sent_tokenize(row)]\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hHBxWyzBOXq"
      },
      "source": [
        "Let's apply this on the datafame and save in a new column 'nltksentence' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMQ9JtoHBOXq"
      },
      "outputs": [],
      "source": [
        "df['nltksentence'] = df['txt'].apply(get_nltk_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH2lK06TBOXq"
      },
      "source": [
        "Now let's write a function for lemmas. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3dMmOlABOXq"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_nltk_lemmas(row):\n",
        "    tk = WordPunctTokenizer() \n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = tk.tokenize(row)\n",
        "    lemmas = [lemmatizer.lemmatize(l) for l in tokens]\n",
        "    return lemmas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3cLigIbBOXq"
      },
      "source": [
        "Let's apply this on the dataframe and save in a new column 'nltklemma'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qL-vhGvxBOXq"
      },
      "outputs": [],
      "source": [
        "df['nltklemma'] = df['txt'].apply(get_nltk_lemmas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJxxmnwKBOXr"
      },
      "source": [
        "Let's do the same to get the nltk POS tags and save in a new column 'nltkpos'. We will pass the list of lemmas to the nltk function for POS tagging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGmxXqaxBOXr"
      },
      "outputs": [],
      "source": [
        "def get_nltk_pos(row):\n",
        "    tags = nltk.pos_tag(row)\n",
        "    return [t[1] for t in tags]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08_UjArZBOXr"
      },
      "outputs": [],
      "source": [
        "df['nltkpos'] = df['nltklemma'].apply(get_nltk_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYJDK54bBOXr"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LmOJULWBOXr"
      },
      "source": [
        "You can now take a look at some examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51cw5wqV3vPP"
      },
      "outputs": [],
      "source": [
        "df['nltkpos'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj1fisv4BOXr"
      },
      "source": [
        "We can also combine these into tuples, to see what POS tag spaCy and nltk have assigned to each lemma in their models. Have a look at some examples from both models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RKolnu3BOXr"
      },
      "outputs": [],
      "source": [
        "## remember how many documents there are in the dataset? You can choose any id within this range, starting with 0.\n",
        "example_id = 43\n",
        "\n",
        "## the models have been saved with the prefix 'nltk' or 'spacy' - try looking at some examples from both\n",
        "model = 'spacy'\n",
        "\n",
        "\n",
        "lemma_example = df[model+'lemma'][example_id]\n",
        "pos_example = df[model+'pos'][example_id]\n",
        "list(zip(lemma_example, pos_example))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS1uIHS6BOXs"
      },
      "source": [
        "Now you have some basic preprocessed text data from two off-the-shelf NLP packages!\n",
        "\n",
        "Let's look at some basic descriptives - the most common lemmas for each model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyXOhB7RBOXs"
      },
      "source": [
        "We'll create a new representation just to count all the lemmas, and save in two new variables: 'counts_nltk' and 'counts_spacy'.\n",
        "\n",
        "Each row in this new representation is simply each lemma in each text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kX_r-qoA8JMA"
      },
      "outputs": [],
      "source": [
        "counts_nltk = pd.Series([item for sublist in df.nltklemma for item in sublist])\n",
        "counts_spacy = pd.Series([item for sublist in df.spacylemma for item in sublist])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nPpVyyKBOXs"
      },
      "source": [
        "How many lemmas in total are there for each of these models?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kq3cdgimBOXs"
      },
      "outputs": [],
      "source": [
        "len(counts_nltk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRYCzYGABOXs"
      },
      "outputs": [],
      "source": [
        "len(counts_spacy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuO7m_CGBOXs"
      },
      "source": [
        "Why might these numbers differ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_6b4QJqBOXs"
      },
      "source": [
        "Let's create a frequency list for each of the models to count the unique lemmas (types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vvdfi2vV4r0z"
      },
      "outputs": [],
      "source": [
        "counts_nltk = counts_nltk.groupby(counts_nltk).size().rename_axis('nltk_lemma').reset_index(name='count')\n",
        "counts_spacy = counts_spacy.groupby(counts_spacy).size().rename_axis('spacy_lemma').reset_index(name='count')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXich3ZcBOXt"
      },
      "source": [
        "Take a look at the new frequency lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmCFI2YLBOXt"
      },
      "outputs": [],
      "source": [
        "counts_spacy.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSq4M8ffBOXt"
      },
      "outputs": [],
      "source": [
        "counts_nltk.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI297JZkBOXt"
      },
      "source": [
        "Let's sort these by frequency and look at the similarities and differences between the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jB8lYcZZ4xL_"
      },
      "outputs": [],
      "source": [
        "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
        "pd_df_nltk = counts_nltk.sort_values(['count'], ascending=False)\n",
        "pd_df_spacy = counts_spacy.sort_values(['count'], ascending=False)\n",
        "fig, ax =plt.subplots(1,2)\n",
        "sns.barplot(x='nltk_lemma', y='count', data=pd_df_nltk[:10], ax=ax[0], palette='colorblind')\n",
        "sns.barplot(x='spacy_lemma', y='count', data=pd_df_spacy[:10], ax=ax[1], palette='dark')\n",
        "fig.autofmt_xdate()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ifdPhrNBOXt"
      },
      "source": [
        "What observations do you have? \n",
        "\n",
        "We might want to remove punctuation and very common words, so that we can better understand which the most frequent content words are instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kva5CRDMBOXt"
      },
      "source": [
        "There are many ways to filter text collections from non-content bearing words. The most common baseline approach is to use a predefined list of stopwords and to only keep alphabetic characters.\n",
        "\n",
        "!NOTE! This step is probably worth spending some time on in your own use-cases!\n",
        "\n",
        "We'll use a baseline approach here using the nltk stopword list, keeping only words without numericals, and lowercasing all words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnzlJXPI5O_V"
      },
      "outputs": [],
      "source": [
        "counts_nltk = pd.Series([item.lower() for sublist in df.nltklemma for item in sublist if item.lower() not in stopwords.words('english') and item.isalpha()])\n",
        "counts_spacy = pd.Series([item for sublist in df.spacylemma for item in sublist if item not in stopwords.words('english') and item.isalpha()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTMPlYxBBOXu"
      },
      "outputs": [],
      "source": [
        "counts_nltk = counts_nltk.groupby(counts_nltk).size().rename_axis('nltk_lemma').reset_index(name='count')\n",
        "counts_spacy = counts_spacy.groupby(counts_spacy).size().rename_axis('spacy_lemma').reset_index(name='count')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INZu1NOrBOXu"
      },
      "outputs": [],
      "source": [
        "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
        "pd_df_nltk = counts_nltk.sort_values(['count'], ascending=False)\n",
        "pd_df_spacy = counts_spacy.sort_values(['count'], ascending=False)\n",
        "fig, ax =plt.subplots(1,2)\n",
        "sns.barplot(x='nltk_lemma', y='count', data=pd_df_nltk[:20], ax=ax[0], palette='colorblind')\n",
        "sns.barplot(x='spacy_lemma', y='count', data=pd_df_spacy[:20], ax=ax[1], palette='dark')\n",
        "fig.autofmt_xdate()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGadMrraBOXu"
      },
      "source": [
        "What observations do you now make on this? Do you notice differences between the two NLP packages? Which one do you think seems to work best? Why or why not are there differences?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBXuhfSuBOXu"
      },
      "source": [
        "Let's also look at POS tag differences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgyqkmdj4HR-"
      },
      "outputs": [],
      "source": [
        "counts_nltk = pd.Series([item for sublist in df.nltkpos for item in sublist])\n",
        "counts_spacy = pd.Series([item for sublist in df.spacypos for item in sublist])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIIFxHOCBOXu"
      },
      "outputs": [],
      "source": [
        "counts_nltk = counts_nltk.groupby(counts_nltk).size().rename_axis('nltk_pos').reset_index(name='count')\n",
        "counts_spacy = counts_spacy.groupby(counts_spacy).size().rename_axis('spacy_pos').reset_index(name='count')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyCu2tAWBOXu"
      },
      "outputs": [],
      "source": [
        "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
        "pd_df_nltk = counts_nltk.sort_values(['count'], ascending=False)\n",
        "pd_df_spacy = counts_spacy.sort_values(['count'], ascending=False)\n",
        "fig, ax =plt.subplots(1,2)\n",
        "sns.barplot(x='nltk_pos', y='count', data=pd_df_nltk[:10], ax=ax[0], palette='colorblind')\n",
        "sns.barplot(x='spacy_pos', y='count', data=pd_df_spacy[:10], ax=ax[1], palette='dark')\n",
        "fig.autofmt_xdate()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDC_tK2gBOXu"
      },
      "source": [
        "What observations do you make? How can you use this information to inform your future NLP development for new use-cases?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbjZdgNyBOXv"
      },
      "source": [
        "## Ngrams\n",
        "\n",
        "Another common representation in different NLP tasks is to look at 'ngrams'. These are n consecutive words in documents, which can be very useful to use for capturing common phrases and expressions.\n",
        "\n",
        "scikit-learn has a function to generate ngrams that we can use to look at some different frequency distributions. We'll create a function to get common ngrams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLFy4euFxZ1O"
      },
      "outputs": [],
      "source": [
        "def get_top_n_ngram(corpus, n=None, ngram_range=1, sw=None):\n",
        "    vec = CountVectorizer(ngram_range=(ngram_range, ngram_range), stop_words=sw).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH8lFadeBOXv"
      },
      "source": [
        "Let's use this function to get the most common uni-, bi-, and trigrams in our corpus, and look at these.\n",
        "\n",
        "Note that you can pass any ngram range you want to this function.\n",
        "\n",
        "Also: note that you can choose to include or exclude stopwords for generating these ngrams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLN7wjcoBOXv"
      },
      "outputs": [],
      "source": [
        "stopwords=None\n",
        "#stopwords='english'\n",
        "\n",
        "common_words_unigram = get_top_n_ngram(df['txt'], 20, ngram_range=1, sw=stopwords)\n",
        "common_words_bigram = get_top_n_ngram(df['txt'], 20, ngram_range=2, sw=stopwords)\n",
        "common_words_trigram = get_top_n_ngram(df['txt'], 20, ngram_range=3, sw=stopwords)\n",
        "\n",
        "df1 = pd.DataFrame(common_words_unigram, columns = ['unigram' , 'count'])\n",
        "df2 = pd.DataFrame(common_words_bigram, columns = ['bigram' , 'count'])\n",
        "df3 = pd.DataFrame(common_words_trigram, columns = ['trigram' , 'count'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pQ8Tqaa0n5L",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df1.groupby('unigram').sum()['count'].sort_values(ascending=False)\n",
        "df2.groupby('bigram').sum()['count'].sort_values(ascending=False)\n",
        "df3.groupby('trigram').sum()['count'].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GO2Wdf83BOXv"
      },
      "outputs": [],
      "source": [
        "fig, ax =plt.subplots(1,3)\n",
        "sns.barplot(x='unigram', y='count', data=df1[:10], ax=ax[0], palette='colorblind')\n",
        "sns.barplot(x='bigram', y='count', data=df2[:10], ax=ax[1], palette='colorblind')\n",
        "sns.barplot(x='trigram', y='count', data=df3[:10], ax=ax[2], palette='colorblind')\n",
        "fig.autofmt_xdate()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RHDQkGzBOXv"
      },
      "source": [
        "Try including stopwords and run again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SU1v8XHBOXv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Intro NLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:root] *",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}