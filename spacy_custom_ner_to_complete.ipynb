{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KCL-Health-NLP/nlp_examples/blob/master/spacy_custom_ner_to_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kswfly0s7Ltx"
      },
      "source": [
        "# spaCy for named entity recognition of clinical concepts\n",
        "\n",
        "In this practical, we will build a named entity recognition (NER) classifier using spaCy.\n",
        "\n",
        "Named entity recognition is a structured learning problem, i.e., we want to learn sequence patterns.\n",
        "\n",
        "We will use data from mtsamples again, and build classifiers that find clinical concepts. \n",
        "\n",
        "The 'gold' standard data is *not* manually annotated, it is the output of a clinical concept recognition system developed by Zeljko Kraljevic called 'CAT' (a predecessor to MedCAT), thus this data is not perfect, but it will do to illustrate the idea. MedCAT matches concepts to the entire UMLS. We will only use a few example concepts here.\n",
        "\n",
        "Part of this material is adapted, inspired etc from:\n",
        "\n",
        "https://spacy.io/usage/training\n",
        "\n",
        "\n",
        "Written by Angus Roberts, May 2023, for spaCy 3. Based on an earlier version for spaCy 2 written by Sumithra Velupillai, March 2019. Acknowledgements and many thanks to Zeljko Kraljevic for the data preparation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8cmNaCP7Lt1"
      },
      "outputs": [],
      "source": [
        "# We'll use spaCy for NER.\n",
        "try:\n",
        "    import spacy\n",
        "except ImportError as e:\n",
        "    !pip install spacy\n",
        "    import spacy\n",
        "\n",
        "# DocBin is a serialisable collection of spacy\n",
        "# Documents.\n",
        "from spacy.tokens import DocBin\n",
        "\n",
        "# Displacy provides a graphic display of\n",
        "# documents and annotations, and Scorer scores...\n",
        "from spacy import displacy\n",
        "from spacy.scorer import Scorer\n",
        "\n",
        "# Example holds spacy documents,\n",
        "# one with predicted annotations\n",
        "# and one with gold standard .\n",
        "# We will use it when evalusating.\n",
        "from spacy.training import Example\n",
        "\n",
        "\n",
        "# requests is a package to submit requests to URLs\n",
        "# We will use it to fetch our data\n",
        "import requests\n",
        "\n",
        "# we use sklearn to split our training data in to train\n",
        "# and dev portions (we have a separate, held out\n",
        "# final test set)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# We will generate warnings for some thing\n",
        "# You might uncomment to ignore them\n",
        "import warnings\n",
        "#warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1: What version of spaCy do we have?\n",
        "SpaCy has changed a lot between V2 and V3, let's check we have the right version - we want V3"
      ],
      "metadata": {
        "id": "jpml9GIz8WnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy info"
      ],
      "metadata": {
        "id": "vOHPAk-i8Q8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTV0vQIZ7Lt3"
      },
      "source": [
        "# 2: Reading in the corpus\n",
        "Our data is in a json format which derives from an older version of spaCy. We will start by reading it in to a Python compund data structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jdWX2xx7Lt3"
      },
      "outputs": [],
      "source": [
        "data_url = 'https://github.com/KCL-Health-NLP/nlp_examples/blob/master/chunking/chunking_traindata_CAT_updated_2021.json?raw=true'\n",
        "r = requests.get(data_url)\n",
        "data = r.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How big is our dataset?"
      ],
      "metadata": {
        "id": "P9eaSZU1TRgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write this code yourself"
      ],
      "metadata": {
        "id": "eYjmadGhVHFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RL9GRmi7Lt4"
      },
      "source": [
        "Let's take a look at a random document and its annotations. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write this code yourself"
      ],
      "metadata": {
        "id": "TeHglxkJVKfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The json format is a list of documents, each document being a list that contains:\n",
        "* [the text itself,\n",
        "* {a dictionary with\n",
        "  * a key string 'entities'\n",
        "  * [a value that is a list containing all the annotated entities\n",
        "    * [each annotated entity is itself a list with\n",
        "      * the start character offset for the entity \n",
        "      * the end character offsets for the entity\n",
        "      * the type of the entity]]}]\n",
        "\n",
        "What are the instances we want to learn?"
      ],
      "metadata": {
        "id": "4LqaWn5HTcZy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will split our data 80:20 in to a train set for training and a dev set for testing at each training iteration. We will do this with scikit learn's train_test_split function. Note that we also have a separate, held out test set that we will keep blind, and read in later on in the notebook.\n",
        "\n",
        "Take a look at the documentation for [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) and work out how to do this. Write the code below, calling the two datasets train_data and dev_data."
      ],
      "metadata": {
        "id": "ysuLV8MSVqCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete this code yourself\n",
        "train_data, dev_data = "
      ],
      "metadata": {
        "id": "Go21IyNoVouS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3: Converting the corpus to .spacy format\n",
        "When training spaCy, we need to pass it a binary file of serialised spaCy *Document* objects. SpaCy *DocBin* objects help with this. They are iterable collections of Documents which can be serialised.\n",
        "\n",
        "We will write a function that takes our json corpus and converts it to a DocBin."
      ],
      "metadata": {
        "id": "17BzW1ouC5qk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # SpaCy documents record the token spans of annotations. \n",
        "  # We therefore need to get the token spans of our\n",
        "  # annotations, so we can save these in the Documents.\n",
        "  # We can do this with a blank spaCy pipeline with no\n",
        "  # components. No need to do any other processing.\n",
        "  nlp = spacy.blank('en')"
      ],
      "metadata": {
        "id": "UqG0oYyCZOdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A DocBin is a serialisable SpaCy container that holds\n",
        "# SpaCy documents, and which can be used in SpaCy training.\n",
        "# This function converts our data format in to a DocBin\n",
        "def data_to_docbin(json_corpus):\n",
        "  \n",
        "  # We create a DocBin to hold out Documents\n",
        "  db = DocBin()\n",
        "  \n",
        "  # The json_corpus contains text and annotations\n",
        "  for text, annot in json_corpus:\n",
        "\n",
        "    # create Document object from text\n",
        "    # this will conatin the tokens and\n",
        "    # their spans\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Now let's get the entities in to a list \n",
        "    ents = []\n",
        "\n",
        "    # The annotations from our data have a start offset,\n",
        "    # an end offset and a label\n",
        "    for start, end, label in annot[\"entities\"]:\n",
        "\n",
        "      # Make a span in our document for these\n",
        "      span = doc.char_span(start, end, label=label)\n",
        "\n",
        "      # If the Document can't align the character offsets with tokens,\n",
        "      # it will return None. We will ignore any entities like this,\n",
        "      # as they could break our training\n",
        "      if span is None:\n",
        "        warnings.warn(f'Skipping entity [{start}, {end}, {label}] : span does not align with token boundaries')\n",
        "      else:\n",
        "        ents.append(span)\n",
        "\n",
        "    # Add the entities to the document\n",
        "    # and add the document to the DocBin\n",
        "    doc.set_ents(ents)\n",
        "    db.add(doc)\n",
        "\n",
        "  # return the DocBin containing all the Documents\n",
        "  # with their text and entities\n",
        "  return db"
      ],
      "metadata": {
        "id": "_KH0dL8objfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's use this function to create DocBins for our train data and our dev data, and write these to disk. Complete the code below to do this."
      ],
      "metadata": {
        "id": "mF5AaesKYLbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now convert our two datasets and serialise them\n",
        "# to disk ready for training - you need to\n",
        "# complete the code to do this\n",
        "\n",
        "# Complete the code below\n",
        "train_doc_bin = \n",
        "train_doc_bin.to_disk(\"./train.spacy\") \n",
        "\n",
        "# Complete the code below\n",
        "dev_doc_bin = \n",
        "dev_doc_bin.to_disk(\"./dev.spacy\") "
      ],
      "metadata": {
        "id": "SB_aNXhOYjiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can read one of them in to check it's worked"
      ],
      "metadata": {
        "id": "epJr-_n7YaRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check we can deserialise\n",
        "doc_bin = DocBin().from_disk(\"./train.spacy\")\n",
        "docs = list(doc_bin.get_docs(nlp.vocab))\n",
        "print(len(docs))"
      ],
      "metadata": {
        "id": "ToJbGQ7NKDAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7Qngaj67Lt4"
      },
      "source": [
        "# 4: Training a named entity model with spaCy\n",
        "The default named entity pipe in spaCy is trained to recognise traditional named entities, such as people and places. We have our own medical named entities that we want to develop a model for.\n",
        "\n",
        "You can train spaCy models in Python code, passing samples of your data to the pipeline in a training loop. However spaCy V3 is designed around running many common operations, including training, from a command line interface (CLI). You run spaCy commands from a command line, passing in any data and a configuration file. The configuration file lets you set parameters, data paths, scoring methods, and many more things. It's a flexible approach, and means that you will always be using the most efficient code, and can concentrate on configuring your pipeline.\n",
        "\n",
        "The first thing we need to do is create a config file. There are lots of options, so the best way is to generate a template using spaCy's online tool, which is here:\n",
        "\n",
        "https://spacy.io/usage/training#quickstart\n",
        "\n",
        "You can play with different settings, but to start we recommend:\n",
        "\n",
        "* Language: English\n",
        "* Components: parser and ner (you need both!)\n",
        "* Hardware: CPU\n",
        "* Optimise for: efficiency\n",
        "\n",
        "Do the following:\n",
        "1.   Go to https://spacy.io/usage/training#quickstart\n",
        "2.   Choosen the above settings\n",
        "3.   Click the download icon on the config file display (bottom right)\n",
        "4.   Save the config file to your local computer\n",
        "5.   Upload the config to colab.\n",
        "\n",
        "Once you have done this, you will have a base_config.cfg file in your colab file space. This is not a complete config, and needs some parameters filling in and initialising for your system. Do this by running the following command to the spacy CLI:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise spacy config file\n",
        "!python -m spacy init fill-config base_config.cfg config.cfg"
      ],
      "metadata": {
        "id": "h12ZoX30g0j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You now have a config.cfg file. SpaCy will use this for all settings during training. However, it will still need a couple of settings, which you can supply at the command line (overriding values in the config.cfg file):\n",
        "\n",
        "* output directory for the trained model\n",
        "* path to the train dataset\n",
        "* path to the dev dataset\n",
        "\n",
        "The following command runs spacy's training using config.cfg. You need to complete it to pass in the paths to our prepared train and dev set to it. Edit it to include these paths, and then run it:"
      ],
      "metadata": {
        "id": "DZWmges2ejsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You need to fill in the training and dev dataset paths before running!\n",
        "!python -m spacy train config.cfg --output ./output --paths.train PUT-PATH_HERE --paths.dev PUT-PATH_HERE"
      ],
      "metadata": {
        "id": "0ETCtSRng1Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2hWekbL7Lt5"
      },
      "source": [
        "What do the different parts of the training report mean?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-_Wt0Wg7Lt8"
      },
      "source": [
        "We have now trained a clinical concept entity recognizer, and saved it to the model directory in our workspace."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5: Take a look at some examples\n",
        "\n",
        " Let's look at an example document and the predicted entities from the new model. We will do this with the displacy package.\n",
        " \n",
        "* Try a few documents\n",
        "* Are they right?\n",
        "* Any problems?"
      ],
      "metadata": {
        "id": "WkOu3wmbfoRc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiKPjQRV7Lt9"
      },
      "outputs": [],
      "source": [
        "# Get a document \n",
        "text = train_data[17][0]\n",
        "\n",
        "# Load the model that was saved to disk by spacy train\n",
        "ner = spacy.load('./output/model-best')\n",
        "\n",
        "# Process the document\n",
        "doc = ner(text)\n",
        "\n",
        "# Set up some colours for displacy\n",
        "colors = {'ANATOMY': 'lightyellow',\n",
        "           'DISEASESYNDROME': 'pink',  \n",
        "           'SIGNSYMPTOM': 'lightgreen'}\n",
        "\n",
        "# Display in displacy\n",
        "displacy.render(doc, style='ent', jupyter=True, options={'colors':colors})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKUxtC917Lt9"
      },
      "source": [
        "What are we actually learning? What are the instances? We can take look at the underlying representation - let's look at the tokens in this document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwl2XL3m7Lt9"
      },
      "outputs": [],
      "source": [
        "print([(t, t.ent_iob_, t.ent_type_) for t in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14UI4Ua97Lt9"
      },
      "source": [
        "What do you think? Does it seem like the model works well on this document? Are there concepts that are missed? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmsyehfk7Lt9"
      },
      "source": [
        "# 6: Evaluation\n",
        "How do we know how good this model is? Let's compare with the held out test data. First you will need to load this in, just like we loaded in the training data. Complete the code below to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTCi4RSY7Lt9"
      },
      "outputs": [],
      "source": [
        "data_url = 'https://github.com/KCL-Health-NLP/nlp_examples/blob/master/chunking/chunking_testdata_CAT_updated_2021.json?raw=true'\n",
        "\n",
        "\n",
        "# Complete the code to load the test data, in to a variable called test_data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spacy's *Scorer* takes a list of *Examples*. An Example contains two Documents, one with the fold standard annotations, and one with the predicted annotations."
      ],
      "metadata": {
        "id": "RFR7D7Cus3Yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We create a list to hold our Examples\n",
        "examples = []\n",
        "\n",
        "# The scorer\n",
        "scorer = Scorer()\n",
        "\n",
        "#Â Iterate over the text and annotations in our test data\n",
        "for text, annotations in test_data:\n",
        "\n",
        "    # Run the ner over the text to make predictions\n",
        "    doc = ner(text)\n",
        "  \n",
        "    # Create the Example from the predicted doc\n",
        "    # and the gold annotations, add it to our list\n",
        "    example = Example.from_dict(doc, annotations)\n",
        "    examples.append(example)\n",
        "\n",
        "# Score the examples\n",
        "scores = scorer.score(examples)\n",
        "\n",
        "print('Precision: ', scores['ents_p'])\n",
        "print('Recall: ', scores['ents_r'])\n",
        "print('F1: ', scores['ents_f'])"
      ],
      "metadata": {
        "id": "jWIrpBleiyAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6paQNQ6M7Lt-"
      },
      "source": [
        "Are these good results do you think? Can this be improved? What happens if you increase the number of iterations in the training?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scores dictionary also contains a dictionary of scores per entity type. You can access this from scores['ents_per_type']. Write code to print out these scores in an easy to red format."
      ],
      "metadata": {
        "id": "ETtgGbA9tbRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write code to print out scores['ents_per_type'] in an easy to read format\n",
        "# e.g. you might iterate over the labels and the metrics in two nested loops\n"
      ],
      "metadata": {
        "id": "pMTjb-MMtnkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MbIztvE7Lt-"
      },
      "source": [
        "Let's look at a document from the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoFstwct7Lt-"
      },
      "outputs": [],
      "source": [
        "text = test_data[37][0]\n",
        "doc = ner(text)\n",
        "\n",
        "# We use the colours that we set up before\n",
        "displacy.render(doc, style='ent', jupyter=True, options={'colors':colors})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH52PANt7Lt-"
      },
      "source": [
        "What does the underlying representation look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSlYISKH7Lt-"
      },
      "outputs": [],
      "source": [
        "print([(t, t.ent_iob_, t.ent_type_) for t in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekHbvGlg7Lt-"
      },
      "source": [
        "There are other options available using spaCy, training models etc. If interested, look at their website, e.g. https://spacy.io/usage/training"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}