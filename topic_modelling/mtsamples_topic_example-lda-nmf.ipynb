{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling example\n",
    "\n",
    "This exercise introduces topic modeling using the LDA (Latent Dirichlet Allocation) algorithm and the Non-negative Matrix Factorisation (NMF) algorithm.\n",
    "\n",
    "Topic modeling is an unsupervised approach that allows you to explore large text collections.\n",
    "\n",
    "In this example, we use the gensim LDA and pyLDAvis implementations for one type of analysis, as well as the sklearn implementations of LDA and NMF to look at how the models relate to categories in the data. \n",
    "\n",
    "The main packages that are used in this example are:\n",
    "\n",
    "nltk: http://www.nltk.org/ - for preprocessing\n",
    "\n",
    "gensim: https://radimrehurek.com/gensim/ - for building the LDA model\n",
    "\n",
    "pyLDAvis: https://github.com/bmabey/pyLDAvis - for visualization and easier exploration of the generated topics\n",
    "\n",
    "sklearn: https://scikit-learn.org/stable/, https://scikit-learn.org/stable/modules/decomposition.html#nmf - for building LDA and NMF models\n",
    "\n",
    "The example is inspired by, and uses functions from: http://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/Gensim%20Newsgroup.ipynb\n",
    "and\n",
    "https://github.com/derekgreene/topic-model-tutorial/blob/master/2%20-%20NMF%20Topic%20Models.ipynb\n",
    "\n",
    "Written by: Sumithra Velupillai, with input from Sonia Priou, February 2019 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we need to import all the necessary packages\n",
    "\n",
    "import string\n",
    "from gensim import models\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import itertools\n",
    "import zipfile\n",
    "\n",
    "try:\n",
    "    import pyLDAvis\n",
    "except ImportError as e:\n",
    "    !pip install pyldavis\n",
    "    import pyLDAvis\n",
    "\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import codecs\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: corpus\n",
    "The first step in building a topic model is to read a corpus, or a collection of documents.\n",
    "\n",
    "In this example, we are using documents from http://www.mtsamples.com/. \n",
    "\n",
    "These are transcribed medical transcriptions sample reports and examples from a variety of clinical disciplines, such as radiology, surgery, discharge summaries. Note that one document can belong to several categories. \n",
    "\n",
    "We will save each document, all its words, and which clinical specialty it belongs to, in a dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlds = 'https://github.com/KCL-Health-NLP/nlp_examples/blob/master/topic_modelling/mtsamples_for_topic_modelling.xlsx?raw=true'\n",
    "df = pd.read_excel(xlds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many documents are in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many clinical specialties are in the data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the texts to words - let's use a very simple approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWords(row):\n",
    "    \n",
    "    return [''.join(c.lower() for c in s if c not in string.punctuation) for s in nltk.word_tokenize(row)]\n",
    "\n",
    "df['Document words'] = df['Document Content'].apply(getWords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Using gensim and pyLDAVis\n",
    "\n",
    "We now need to generate representations for the vocabulary (dictionary) and the text collection (corpus)\n",
    "\n",
    "Let's use some functions that we can call later, and that we can modify later if we want.\n",
    "\n",
    "(Using all the words in the whole corpus or text collection is not typically what you want, because very common words, \n",
    "\n",
    "or very rare words will not generate good topic representations. Why?\n",
    "\n",
    "What parameters and configurations could be interesting to change below?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## functions from http://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/Gensim%20Newsgroup.ipynb\n",
    "\n",
    "## this function returns a set of stopwords predefined in the nltk package\n",
    "\n",
    "def nltk_stopwords():\n",
    "    return set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "## this function prepares the data and returns a dictionary and a corpus.\n",
    "## which parameters do you think would be worth modifying/experimenting with?\n",
    "\n",
    "def prep_corpus(docs, additional_stopwords=set(), no_below=5, no_above=0.5):\n",
    "  print('Building dictionary...')\n",
    "  dictionary = Dictionary(docs)\n",
    "  stopwords = nltk_stopwords().union(additional_stopwords)\n",
    "  stopword_ids = map(dictionary.token2id.get, stopwords)\n",
    "  dictionary.filter_tokens(stopword_ids)\n",
    "  dictionary.compactify()\n",
    "  dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=None)\n",
    "  dictionary.compactify()\n",
    "\n",
    "  print('Building corpus...')\n",
    "  corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "  return dictionary, corpus\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now, let's use the functions we defined above to get our dictionary and corpus\n",
    "dictionary, corpus = prep_corpus(df['Document words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you want, you can save your corpus and dictionary to disk for quicker processing later\n",
    "\n",
    "#MmCorpus.serialize('/Users/sumithra/DSV/MeDESTO/teaching/Farr2017/data/gensim_topic_model_data/mtsamples.mm', corpus)\n",
    "#dictionary.save('/Users/sumithra/DSV/MeDESTO/teaching/Farr2017/data/gensim_topic_model_data/mtsamples.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we have our dictionary and corpus, let's generate an LDA model.\n",
    "## The LDA model has many parameters that can be set, all available parameters can be found here:\n",
    "## https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "\n",
    "## Here, we've set the number of topics to 10.\n",
    "\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, passes=10)\n",
    "\n",
    "## You can also save the generated model to disk if you want\n",
    "#lda.save('/Users/sumithra/DSV/MeDESTO/teaching/Farr2017/data/gensim_topic_model_data/mtsamples_20_lda.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## you can now look at these topics by printing them from the generated model\n",
    "\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## It can be hard to get a good understanding of what's actually in these topics\n",
    "## Visualizations are very helpful for this, let's use a package that does this:\n",
    "\n",
    "vis_data = gensimvis.prepare(lda, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the results. What observations do you have? What happens if you change the number of topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Using sklearn and comparing with 'existing' categories\n",
    "\n",
    "Now you have seen how you can build a topic models with gensim and look at the contents visually with pyLDAVis.\n",
    "\n",
    "You can also use sklearn for topic modeling, both lda and nmf, and analyse results visually by comparing with existing categories, if you have them.\n",
    "\n",
    "NMF approaches can be very efficient, particularly with smaller datasets. Let's see what you think.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a couple of functions to visualise the data\n",
    "# Preparation for visualisation \n",
    "# Written by Sonia Priou\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "    \n",
    "def display_topic_representation (model,dataframe):\n",
    "    doc_topic = model    #example : model = lda_Tfidf.transform(tfidf)\n",
    "    doc = np.arange(doc_topic.shape[0])\n",
    "    no_topics = doc_topic.shape[1]\n",
    "    dico = {'index': doc}\n",
    "    for n in range(no_topics):\n",
    "        dico[\"topic\" + str(n)] = doc_topic[:,n]\n",
    "    \n",
    "    #Max topic \n",
    "    Topic_max = []\n",
    "    for i in range(doc_topic.shape[0]):\n",
    "        Topic_max.append(doc_topic[i].argmax())\n",
    "    dico[\"Topic most represented\"] = Topic_max\n",
    "    df_topic = pd.DataFrame(dico)\n",
    "\n",
    "    \n",
    "    #Link both DataFrame\n",
    "    df_result = pd.merge(dataframe,df_topic, on='index')\n",
    "    \n",
    "    #Finding within the cluster found by LDA the original file\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.set_style('whitegrid')\n",
    "    sns.countplot(x='Topic most represented', data = df_result, hue='Category')\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "def display_file_representation (model,dataframe):\n",
    "    #Within a file, what is the slipt between topics found\n",
    "    doc_topic = model    #example : model = lda_Tfidf.transform(tfidf)\n",
    "    doc = np.arange(doc_topic.shape[0])\n",
    "    no_topics = doc_topic.shape[1]\n",
    "    topic = np.arange(no_topics)\n",
    "    dico = {'index': doc}\n",
    "    for n in range(no_topics):\n",
    "        dico[\"topic\" + str(n)] = doc_topic[:,n]\n",
    "    #Max topic \n",
    "    Topic_max = []\n",
    "    for i in range(doc_topic.shape[0]):\n",
    "        Topic_max.append(doc_topic[i].argmax())\n",
    "    dico[\"Topic most represented\"] = Topic_max\n",
    "    df_topic = pd.DataFrame(dico)\n",
    "    \n",
    "    \n",
    "    #Link both DataFrame\n",
    "    df_result = pd.merge(dataframe,df_topic, on='index')\n",
    "    \n",
    "    dico2 = {'Topic': topic}\n",
    "    for i in df_result['Category'].value_counts().index:\n",
    "        ser = df_result.loc[df_result['Category']==i].mean()\n",
    "        score = ser[1:no_topics+1]\n",
    "        dico2[i]=score\n",
    "\n",
    "    df_score = pd.DataFrame(dico2)\n",
    "    print('For each given file, we calculate the mean percentage of the documents depence to each topic')\n",
    "    print('')\n",
    "    print(df_score.head())\n",
    "\n",
    "    fig, axs = plt.subplots(ncols=len(df_smaller['Category'].value_counts()))\n",
    "    count = 0\n",
    "    for i in df_result['Category'].value_counts().index:\n",
    "        sns.barplot(x='Topic', y =i ,data = df_score, ax=axs[count])\n",
    "        count = count + 1\n",
    "        \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a smaller sample, to make the analysis a bit easier. You can choose other categories of course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_to_keep = ['17-dentistry', '46-ophthalmology', '72-psychiatrypsychology', '71-podiatry']\n",
    "df_smaller = df.loc[df['Category'].isin(categories_to_keep)]\n",
    "df_smaller['index'] = range(0,len (df_smaller))\n",
    "df_smaller.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use sklearn's function for converting corpora to document-term-matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "min_df = 5\n",
    "max_df = 100000\n",
    "lowercase = True\n",
    "ngram_range = 2\n",
    "\n",
    "bow_transformer = CountVectorizer(stop_words=stopwords, \n",
    "                                  min_df=min_df, \n",
    "                                  max_df=max_df, \n",
    "                                  lowercase = lowercase).fit(df['Document Content'])\n",
    "document_bow = bow_transformer.transform(df_smaller['Document Content'])\n",
    "feature_names = bow_transformer.get_feature_names()\n",
    "\n",
    "tfidf_transformer = TfidfTransformer().fit(document_bow)\n",
    "document_tfidf= tfidf_transformer.transform(document_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many features do you now have? What parameters can you change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many topics do you want the model to generate?\n",
    "How many discriminative words from each topic do you want to look at? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_topics = 4\n",
    "no_top_words = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build an lda model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=no_topics).fit(document_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the most discriminative words for each topic. Do you see a pattern? Do you think more work needs to be done with the underlying representation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lda,feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at the main topic for each document. Does this look reasonable to you? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Representation of the main topic for each document')\n",
    "display_topic_representation(lda.transform(document_tfidf),df_smaller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the distribution of topics in the files in relation to the 'existing' categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_file_representation(lda.transform(document_tfidf),df_smaller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare with NMF.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=no_topics, \n",
    "          random_state=1, \n",
    "          alpha=.1, \n",
    "          l1_ratio=.5, \n",
    "          init='nndsvd').fit(document_tfidf)\n",
    "\n",
    "W = nmf.transform(document_tfidf)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(nmf, feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Representation of the main topic for each document')\n",
    "display_topic_representation(W,df_smaller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What main differences do you notice when comparing NMF and LDA results? Do you think one is better than the other? What parameters might be worth changing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
