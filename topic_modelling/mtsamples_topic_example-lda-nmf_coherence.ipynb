{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3qLZuJu6xjB"
      },
      "source": [
        "# Topic modeling example\n",
        "\n",
        "This exercise introduces topic modeling using the LDA (Latent Dirichlet Allocation) algorithm and the Non-negative Matrix Factorisation (NMF) algorithm.\n",
        "\n",
        "Topic modeling is an unsupervised approach that allows you to explore large text collections.\n",
        "\n",
        "In this example, we use the gensim LDA and pyLDAvis implementations for one type of analysis, as well as the sklearn implementations of LDA and NMF to look at how the models relate to categories in the data. \n",
        "\n",
        "The main packages that are used in this example are:\n",
        "\n",
        "nltk: http://www.nltk.org/ - for preprocessing\n",
        "\n",
        "gensim: https://radimrehurek.com/gensim/ - for building the LDA model\n",
        "\n",
        "pyLDAvis: https://github.com/bmabey/pyLDAvis - for visualization and easier exploration of the generated topics\n",
        "\n",
        "sklearn: https://scikit-learn.org/stable/, https://scikit-learn.org/stable/modules/decomposition.html#nmf - for building LDA and NMF models\n",
        "\n",
        "The example is inspired by, and uses functions from: http://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/Gensim%20Newsgroup.ipynb\n",
        "and\n",
        "https://github.com/derekgreene/topic-model-tutorial/blob/master/2%20-%20NMF%20Topic%20Models.ipynb\n",
        "\n",
        "Written by: Sumithra Velupillai, with input from Sonia Priou, February 2019 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UnZReMK6xjD"
      },
      "outputs": [],
      "source": [
        "## First we need to import all the necessary packages\n",
        "\n",
        "# There is an incompatibility between the current pyLDAvis and pandas,\n",
        "# we will downgrade to solve it\n",
        "!pip install --upgrade pyLDAvis==2.1.2\n",
        "\n",
        "import string\n",
        "from gensim import models\n",
        "from gensim.corpora import Dictionary, MmCorpus\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import itertools\n",
        "import zipfile\n",
        "\n",
        "try:\n",
        "    import pyLDAvis\n",
        "except ImportError as e:\n",
        "    !pip install pyldavis\n",
        "    import pyLDAvis\n",
        "\n",
        "import pyLDAvis.gensim as gensimvis\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "import codecs\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "import zipfile\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings; warnings.simplefilter('ignore')\n",
        "\n",
        "from datetime import datetime\n",
        "print(datetime.now())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtKhSKi26xjE"
      },
      "source": [
        "# 1: corpus\n",
        "The first step in building a topic model is to read a corpus, or a collection of documents.\n",
        "\n",
        "In this example, we are using documents from http://www.mtsamples.com/. \n",
        "\n",
        "These are transcribed medical transcriptions sample reports and examples from a variety of clinical disciplines, such as radiology, surgery, discharge summaries. Note that one document can belong to several categories. \n",
        "\n",
        "We will save each document, all its words, and which clinical specialty it belongs to, in a dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2yvrriT6xjF"
      },
      "outputs": [],
      "source": [
        "xlds = 'https://github.com/KCL-Health-NLP/nlp_examples/blob/master/topic_modelling/mtsamples_for_topic_modelling.xlsx?raw=true'\n",
        "df = pd.read_excel(xlds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8OKWscR6xjF"
      },
      "source": [
        "How many documents are in the data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d68eaJxt6xjF"
      },
      "outputs": [],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRR0bxm96xjG"
      },
      "source": [
        "How many clinical specialties are in the data? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bz2UqFGK6xjG"
      },
      "outputs": [],
      "source": [
        "df['Category'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAr61Npw6xjG"
      },
      "source": [
        "We need to convert the texts to words - let's use a very simple approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gV4cqdwK6xjG"
      },
      "outputs": [],
      "source": [
        "def getWords(row):\n",
        "    \n",
        "    return [''.join(c.lower() for c in s if c not in string.punctuation) for s in nltk.word_tokenize(row)]\n",
        "\n",
        "df['Document words'] = df['Document Content'].apply(getWords)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEtSk7iq6xjH"
      },
      "source": [
        "# 2 Using gensim and pyLDAVis\n",
        "\n",
        "We now need to generate representations for the vocabulary (dictionary) and the text collection (corpus)\n",
        "\n",
        "Let's use some functions that we can call later, and that we can modify later if we want.\n",
        "\n",
        "(Using all the words in the whole corpus or text collection is not typically what you want, because very common words, \n",
        "\n",
        "or very rare words will not generate good topic representations. Why?\n",
        "\n",
        "What parameters and configurations could be interesting to change below?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Xd3Id6U6xjH"
      },
      "outputs": [],
      "source": [
        "\n",
        "## functions from http://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/Gensim%20Newsgroup.ipynb\n",
        "\n",
        "## this function returns a set of stopwords predefined in the nltk package\n",
        "\n",
        "def nltk_stopwords():\n",
        "    return set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "## this function prepares the data and returns a dictionary and a corpus.\n",
        "## which parameters do you think would be worth modifying/experimenting with?\n",
        "\n",
        "def prep_corpus(docs, additional_stopwords=set(), no_below=5, no_above=0.5):\n",
        "  print('Building dictionary...')\n",
        "  dictionary = Dictionary(docs)\n",
        "  stopwords = nltk_stopwords().union(additional_stopwords)\n",
        "  stopword_ids = map(dictionary.token2id.get, stopwords)\n",
        "  dictionary.filter_tokens(stopword_ids)\n",
        "  dictionary.compactify()\n",
        "  dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=None)\n",
        "  dictionary.compactify()\n",
        "\n",
        "  print('Building corpus...')\n",
        "  corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
        "\n",
        "  return dictionary, corpus\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOBxeKPp6xjH"
      },
      "outputs": [],
      "source": [
        "## now, let's use the functions we defined above to get our dictionary and corpus\n",
        "dictionary, corpus = prep_corpus(df['Document words'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZmtHEqY6xjH"
      },
      "outputs": [],
      "source": [
        "## If you want, you can save your corpus and dictionary to disk for quicker processing later\n",
        "## For colab, try this:\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive') # to mount the google drive\n",
        "# MmCorpus.serialize(\"gdrive/My Drive/Colab Notebooks/mtsamples.mm\", corpus)\n",
        "# dictionary.save(\"gdrive/My Drive/Colab Notebooks/mtsamples.dict\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qf7W17Ik6xjH"
      },
      "outputs": [],
      "source": [
        "## Now we have our dictionary and corpus, let's generate an LDA model.\n",
        "## The LDA model has many parameters that can be set, all available parameters can be found here:\n",
        "## https://radimrehurek.com/gensim/models/ldamodel.html\n",
        "\n",
        "## Here, we've set the number of topics to 10.\n",
        "\n",
        "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, passes=10)\n",
        "\n",
        "## You can also save the generated model to disk if you want\n",
        "#lda.save('/Users/sumithra/DSV/MeDESTO/teaching/Farr2017/data/gensim_topic_model_data/mtsamples_20_lda.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9c9XFGa6xjI"
      },
      "outputs": [],
      "source": [
        "## you can now look at these topics by printing them from the generated model\n",
        "\n",
        "lda.print_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isb8dunX6xjI"
      },
      "outputs": [],
      "source": [
        "\n",
        "## It can be hard to get a good understanding of what's actually in these topics\n",
        "## Visualizations are very helpful for this, let's use a package that does this:\n",
        "\n",
        "vis_data = gensimvis.prepare(lda, corpus, dictionary)\n",
        "pyLDAvis.display(vis_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35fanwST6xjI"
      },
      "source": [
        "Take a look at the results. What observations do you have? What happens if you change the number of topics?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT7dN6fJ6xjI"
      },
      "source": [
        "# 3 Using sklearn and comparing with 'existing' categories\n",
        "\n",
        "Now you have seen how you can build a topic models with gensim and look at the contents visually with pyLDAVis.\n",
        "\n",
        "You can also use sklearn for topic modeling, both lda and nmf, and analyse results visually by comparing with existing categories, if you have them.\n",
        "\n",
        "NMF approaches can be very efficient, particularly with smaller datasets. Let's see what you think.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUDs8pbj6xjI"
      },
      "outputs": [],
      "source": [
        "# We need a couple of functions to visualise the data\n",
        "# Preparation for visualisation \n",
        "# Written by Sonia Priou, adaptations by Sumithra Velupillai\n",
        "\n",
        "def display_topics(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"Topic %d:\" % (topic_idx))\n",
        "        print(\", \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "\n",
        "def get_topic_list(model, feature_names, no_top_words):\n",
        "    tlist = {}\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        tlist[topic_idx]= str(\"%d: \" % (topic_idx)+\", \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "    return tlist\n",
        "\n",
        "    \n",
        "def display_topic_representation (model,dataframe,tlist):\n",
        "    doc_topic = model    #example : model = lda_Tfidf.transform(tfidf)\n",
        "    doc = np.arange(doc_topic.shape[0])\n",
        "    num_topics = doc_topic.shape[1]\n",
        "    dico = {'index': doc}\n",
        "    for n in range(num_topics):\n",
        "        dico[\"topic\" + str(n)] = doc_topic[:,n]\n",
        "    \n",
        "    #Max topic \n",
        "    Topic_max = []\n",
        "    for i in range(doc_topic.shape[0]):\n",
        "        if len(set(doc_topic[i])) == 1:\n",
        "            Topic_max.append(num_topics+1)\n",
        "        else:\n",
        "            Topic_max.append(doc_topic[i].argmax())\n",
        "    dico[\"Topic most represented\"] = Topic_max\n",
        "    #print(Topic_max)\n",
        "    tlist[num_topics+1] = 'NONE'\n",
        "    dico[\"Topic and its most representative words\"] = [tlist[x] for x in Topic_max]\n",
        "    df_topic = pd.DataFrame(dico)\n",
        "\n",
        "    \n",
        "    #Link both DataFrame\n",
        "    df_result = pd.merge(dataframe,df_topic, on='index')\n",
        "    df_result = df_result.sort_values('Topic most represented')\n",
        "    \n",
        "    #Finding within the cluster found by LDA the original file\n",
        "    fig, ax = plt.subplots()\n",
        "    fig.set_size_inches(11.7, 8.27)\n",
        "    sns.set_style('whitegrid')\n",
        "    sns.countplot(y='Topic and its most representative words', data = df_result)\n",
        "    return df_result\n",
        "\n",
        "def display_file_representation (model,dataframe):\n",
        "    #Within a file, what is the slipt between topics found\n",
        "    doc_topic = model    #example : model = lda_Tfidf.transform(tfidf)\n",
        "    doc = np.arange(doc_topic.shape[0])\n",
        "    no_topics = doc_topic.shape[1]\n",
        "    topic = np.arange(no_topics)\n",
        "    dico = {'index': doc}\n",
        "    for n in range(no_topics):\n",
        "        dico[\"topic\" + str(n)] = doc_topic[:,n]\n",
        "    #Max topic \n",
        "    Topic_max = []\n",
        "    for i in range(doc_topic.shape[0]):\n",
        "        Topic_max.append(doc_topic[i].argmax())\n",
        "    dico[\"Topic most represented\"] = Topic_max\n",
        "    df_topic = pd.DataFrame(dico)\n",
        "    #print(df_topic)\n",
        "    \n",
        "    \n",
        "    #Link both DataFrame\n",
        "    df_result = pd.merge(dataframe,df_topic, on='index')\n",
        "    \n",
        "    dico2 = {'Topic': topic}\n",
        "    for i in df_result['Category'].value_counts().index:\n",
        "        ser = df_result.loc[df_result['Category']==i].mean()\n",
        "        score = ser[2:no_topics+2]\n",
        "        dico2[i]=score\n",
        "\n",
        "    df_score = pd.DataFrame(dico2)\n",
        "    print('For each given file, we calculate the mean percentage of the documents depence to each topic')\n",
        "    print('')\n",
        "    print(df_score)\n",
        "    print(df_result['Category'].value_counts().index)\n",
        "\n",
        "    fig, axs = plt.subplots(ncols=len(dataframe['Category'].value_counts()))\n",
        "    count = 0\n",
        "    for i in df_result['Category'].value_counts().index:\n",
        "        sns.barplot(x='Topic', y =i ,data = df_score, ax=axs[count])\n",
        "        count = count + 1\n",
        "        \n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma8Btgkm6xjJ"
      },
      "source": [
        "Let's look at a smaller sample, to make the analysis a bit easier. You can choose other categories of course!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDqHuBqM6xjJ"
      },
      "outputs": [],
      "source": [
        "categories_to_keep = ['17-dentistry', '46-ophthalmology', '72-psychiatrypsychology', '71-podiatry']\n",
        "df_smaller = df.loc[df['Category'].isin(categories_to_keep)]\n",
        "#new_df = df.drop(df_smaller)\n",
        "new_df = df[~df.isin(df_smaller)]\n",
        "df_smaller['index'] = range(0,len (df_smaller))\n",
        "df_smaller.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU_8VEXO6xjJ"
      },
      "source": [
        "Now let's use sklearn's function for converting corpora to document-term-matrices. We'll define a function for this, which takes as parameters a dataframe, the name of the text column that should be transformed to tf-idf, and some optional parameters for thresholds in creating the model. These thresholds can of course be changed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6jFEIwB6xjJ"
      },
      "outputs": [],
      "source": [
        "def get_tfidf_model(dataframe, text_column, min_df = 5, max_df=100000, lowercase = True):\n",
        "    stopwords = nltk.corpus.stopwords.words('english')\n",
        "    min_df = 5\n",
        "    max_df = 100000\n",
        "    lowercase = True\n",
        "    \n",
        "\n",
        "    bow_transformer = CountVectorizer(stop_words=stopwords, \n",
        "                                  min_df=min_df, \n",
        "                                  max_df=max_df, \n",
        "                                  lowercase = lowercase).fit(dataframe[text_column])\n",
        "    document_bow = bow_transformer.transform(dataframe[text_column])\n",
        "    feature_names = bow_transformer.get_feature_names_out()\n",
        "\n",
        "    tfidf_transformer = TfidfTransformer().fit(document_bow)\n",
        "    document_tfidf= tfidf_transformer.transform(document_bow)\n",
        "    return feature_names, document_tfidf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avZNE_S_6xjK"
      },
      "source": [
        "Let's also define some functions to train the different topic models - we're using NMF and LDA with some preset parameters, these can of course be changed - check out the documentation to see what parameters are available. Note that the two functions return different variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DF4FC_LZ6xjK"
      },
      "outputs": [],
      "source": [
        "def getNMFModel(no_topics, document_tfidf):\n",
        "    nmf = NMF(n_components=no_topics)\n",
        "    W = nmf.fit_transform(document_tfidf)\n",
        "    H = nmf.components_\n",
        "    return nmf, W, H\n",
        "\n",
        "def getLDAModel(no_topics, document_tfidf):\n",
        "    lda = LatentDirichletAllocation(n_components=no_topics).fit(document_tfidf)\n",
        "    return lda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBiqzcP16xjK"
      },
      "source": [
        "Now let's convert our data to a tfidf model and get the feature names from that model (i.e. the vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUFoWOch6xjK"
      },
      "outputs": [],
      "source": [
        "feature_names, document_tfidf = get_tfidf_model(df_smaller, 'Document Content')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSUlN3pr6xjK"
      },
      "source": [
        "How many features does the model contain? What parameters can you change?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1mma8Gt6xjL"
      },
      "outputs": [],
      "source": [
        "print(len(feature_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7pWIP-G6xjL"
      },
      "source": [
        "### Optional\n",
        "What's in the variable feature_names? How can you take a look?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRyCmavS6xjL"
      },
      "outputs": [],
      "source": [
        "## Do something with the variable feature_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xapEOVqM6xjL"
      },
      "source": [
        "### Number of topics and top words for each topic\n",
        "* How many topics do you want the model to generate?\n",
        "* How many discriminative words from each topic do you want to look at? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yrym3JOx6xjL"
      },
      "outputs": [],
      "source": [
        "## In this case, we know that there are four categories in the dataset, \n",
        "## let's see if the models produce something coherent with that number\n",
        "no_topics = 4\n",
        "\n",
        "## Each topic is represented with a list of words, ranked according to how discriminative they are for that topic. \n",
        "## We can use the top ranked words to try to understand what the topic represents.\n",
        "no_top_words = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv--SfSA6xjL"
      },
      "source": [
        "Now let's build an lda model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ALQ8Hjz6xjL"
      },
      "outputs": [],
      "source": [
        "lda = getLDAModel(no_topics, document_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeO6AiCr6xjL"
      },
      "source": [
        "### Most discriminative words - LDA\n",
        "Let's look at the most discriminative words for each topic generated from our LDA model. Do you see a pattern? Do they make sense? Do you think more work needs to be done, e.g. with parameters, with the underlying representation, or other things?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3NnLz6b6xjM"
      },
      "outputs": [],
      "source": [
        "display_topics(lda,feature_names, no_top_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AjRL5O_6xjM"
      },
      "source": [
        "We can now look at the distribution of the main topic (i.e. the topic with highest probability) for each document. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7pARKF56xjM"
      },
      "outputs": [],
      "source": [
        "print('Representation of the main topic for each document in the document collection')\n",
        "tlist = get_topic_list(lda,feature_names, 10)\n",
        "df_result = display_topic_representation(lda.transform(document_tfidf),df_smaller,tlist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1F7IYkx6xjM"
      },
      "source": [
        "Does this look reasonable to you? Are all the topics represented? How do you interpret these results?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJPW55b_6xjM"
      },
      "source": [
        "There's a new column saved in the dataframe that contains the topic number that had the highest probability, we can look at the distribution of those. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-W5_sDo16xjM"
      },
      "outputs": [],
      "source": [
        "df_result['Topic most represented'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls15ARla6xjM"
      },
      "source": [
        "We can also look at the probability scores each topic resulted in in the whole document collection and get some descriptive statistics. Look at some of the other topics!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2A9W1wOb6xjN"
      },
      "outputs": [],
      "source": [
        "df_result['topic1'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-LiIPBr6xjN"
      },
      "source": [
        "Now let's look at the distribution of topics in the files in relation to the 'existing' categories in the dataset. We'll use the function we defined earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdyijjEz6xjN"
      },
      "outputs": [],
      "source": [
        "display_file_representation(lda.transform(document_tfidf),df_smaller)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5kv3bYX6xjN"
      },
      "source": [
        "### NMF\n",
        "Now let's compare with NMF.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7brnyoO6xjN"
      },
      "outputs": [],
      "source": [
        "nmf, W, H = getNMFModel(no_topics, document_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvidL8J36xjN"
      },
      "outputs": [],
      "source": [
        "display_topics(nmf, feature_names, no_top_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiCpyzpI6xjN"
      },
      "outputs": [],
      "source": [
        "print('Representation of the main topic for each document')\n",
        "\n",
        "tlist = get_topic_list(nmf,feature_names, 10)\n",
        "df_result = display_topic_representation(W,df_smaller,tlist)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1tHqkPx6xjO"
      },
      "outputs": [],
      "source": [
        "display_file_representation(W,df_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80YvVm3M6xjO"
      },
      "source": [
        "## LDA vs NMF?\n",
        "What do you think about these results and models? What main differences do you notice when comparing NMF and LDA results? Do you think one is better than the other? What parameters might be worth changing?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cverR3lp6xjO"
      },
      "source": [
        "# Choosing number of topics ('k') - coherence score\n",
        "\n",
        "There are several metrics proposed for automatically calculating what the 'optimal' number of topics in a document collection is, by trying to measure how coherent generated topics are.\n",
        "\n",
        "Here, we will use the TC-W2W score proposed by O'Callaghan et al. \n",
        "\n",
        "Oâ€™Callaghan, D., Greene, D., Carthy, J., & Cunningham, P. (2015). \n",
        "An analysis of the coherence of descriptors in topic modeling. \n",
        "Expert Systems with Applications, 42(13), 5645-5657\n",
        "\n",
        "The code is from: https://github.com/derekgreene/topic-model-tutorial/blob/master/3%20-%20Parameter%20Selection%20for%20NMF.ipynb\n",
        "With slight adaptations for our example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdsnmpzQ6xjO"
      },
      "source": [
        "The main idea with this score is to calculate the pairwise embedding vector similarity for each term pair in the top ranked words in each generated topic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyg67c_H6xjO"
      },
      "source": [
        "We need to have a word2vec model first. We'll start by building one on the entire document collection we have. NOTE: this means that we have an 'in-domain' model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KLvi9BJ6xjO"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "\n",
        "w2v_model = gensim.models.Word2Vec(df['Document words'], size=300, min_count=2, batch_words=1000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDf9TGUW6xjO"
      },
      "source": [
        "How big is the vocabulary in this embedding model? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eo8_wnfI6xjO"
      },
      "outputs": [],
      "source": [
        "print( \"Model has %d terms\" % len(w2v_model.wv.vocab) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpGjRHef6xjP"
      },
      "source": [
        "Recap: we can get similarity scores for different word pairs in these types of models. We can try this on a pair of words from the generated topics in our previous NMF model, e.g. topic3 had the words 'teeth' and 'tooth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgfTPyxs6xjP"
      },
      "outputs": [],
      "source": [
        "w2v_model.similarity('teeth', 'tooth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k83Zp2fr6xjP"
      },
      "source": [
        "Next, we need to build topic models with different number of topics assigned. Let's define a range of k to try."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acPuyze86xjP"
      },
      "outputs": [],
      "source": [
        "kmin, kmax = 3, 15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em4R2PZi6xjP"
      },
      "source": [
        "Let's try our smaller dataset again, and generate a tfidf-representation to use in the topic models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aggF74pG6xjP"
      },
      "outputs": [],
      "source": [
        "feature_names, document_tfidf = get_tfidf_model(df_smaller, 'Document Content')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHaNeClg6xjP"
      },
      "source": [
        "Now, let's generate NMF topic models for each value of k in our range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4LAlWmC6xjP"
      },
      "outputs": [],
      "source": [
        "from sklearn import decomposition\n",
        "topic_models = []\n",
        "# try each value of k\n",
        "for k in range(kmin,kmax+1):\n",
        "    print(\"Applying NMF for k=%d ...\" % k )\n",
        "    # run NMF\n",
        "    model, W, H =  getNMFModel(k, document_tfidf)       \n",
        "    # store for later\n",
        "    topic_models.append( (k,W,H) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKyby19N6xjP"
      },
      "source": [
        "We need a couple of functions to calculate the coherence score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC-1NI606xjP"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "def calculate_coherence( w2v_model, term_rankings , print_pairs=False):\n",
        "    overall_coherence = 0.0\n",
        "    for topic_index in range(len(term_rankings)):\n",
        "        # check each pair of terms\n",
        "        pair_scores = []\n",
        "        for pair in combinations( term_rankings[topic_index], 2 ):\n",
        "            ## check if word in vocabulary first!! Added by Sumithra\n",
        "            if pair[0] in w2v_model.wv.vocab and pair[1] in w2v_model.wv.vocab:\n",
        "                pair_scores.append( w2v_model.similarity(pair[0], pair[1]) )\n",
        "                if print_pairs:\n",
        "                    print(pair[0], pair[1], w2v_model.similarity(pair[0], pair[1]))\n",
        "            else:\n",
        "                if print_pairs:\n",
        "                    print('word pair not in vocabulary', pair[0], pair[1])\n",
        "                pair_scores.append( 0.0 )\n",
        "        # get the mean for all pairs in this topic\n",
        "        topic_score = sum(pair_scores) / len(pair_scores)\n",
        "        overall_coherence += topic_score\n",
        "    # get the mean score across all topics\n",
        "    return overall_coherence / len(term_rankings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlZoQbQr6xjQ"
      },
      "outputs": [],
      "source": [
        "def get_descriptor( all_terms, H, topic_index, top ):\n",
        "    # reverse sort the values to sort the indices\n",
        "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
        "    # now get the terms corresponding to the top-ranked indices\n",
        "    top_terms = []\n",
        "    for term_index in top_indices[0:top]:\n",
        "        top_terms.append( all_terms[term_index] )\n",
        "    return top_terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM8Cgidj6xjQ"
      },
      "source": [
        "Let's calculate the coherence score for each model with k topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-tEMfAk6xjQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "k_values = []\n",
        "coherences = []\n",
        "for (k,W,H) in topic_models:\n",
        "    # Get all of the topic descriptors - the term_rankings, based on top 10 terms\n",
        "    term_rankings = []\n",
        "    for topic_index in range(k):\n",
        "        term_rankings.append( get_descriptor( feature_names, H, topic_index, 10 ) )\n",
        "    # Now calculate the coherence based on our Word2vec model\n",
        "    k_values.append( k )\n",
        "    coherences.append( calculate_coherence( w2v_model, term_rankings, print_pairs=False ) )\n",
        "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEFnTc4e6xjQ"
      },
      "source": [
        "We can plot this to visualise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaPguO5M6xjQ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"ggplot\")\n",
        "matplotlib.rcParams.update({\"font.size\": 14})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ1oD3eb6xjQ"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(13,7))\n",
        "# create the line plot\n",
        "ax = plt.plot( k_values, coherences )\n",
        "plt.xticks(k_values)\n",
        "plt.xlabel(\"Number of Topics\")\n",
        "plt.ylabel(\"Mean Coherence\")\n",
        "# add the points\n",
        "plt.scatter( k_values, coherences, s=120)\n",
        "# find and annotate the maximum point on the plot\n",
        "ymax = max(coherences)\n",
        "xpos = coherences.index(ymax)\n",
        "best_k = k_values[xpos]\n",
        "plt.annotate( \"k=%d\" % best_k, xy=(best_k, ymax), xytext=(best_k, ymax), textcoords=\"offset points\", fontsize=16)\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_D_tcmD6xjQ"
      },
      "source": [
        "What seems to be the 'optimal' number of topics? Does this make sense do you think?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt3UPgoC6xjQ"
      },
      "source": [
        "Now let's build a model with this 'k' and look at what the model produces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zim5f5XK6xjR"
      },
      "outputs": [],
      "source": [
        "\n",
        "## Assign a value to no_topics from the results above\n",
        "no_topics = # fill in here\n",
        "\n",
        "## How many words do you want as topic descriptors?\n",
        "no_top_words = # fill in here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BIT7ZCO6xjR"
      },
      "outputs": [],
      "source": [
        "nmf, W, H = getNMFModel(no_topics, document_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtHRMImU6xjR"
      },
      "outputs": [],
      "source": [
        "print('Representation of the main topic for each document')\n",
        "\n",
        "tlist = get_topic_list(nmf,feature_names, 10)\n",
        "df_result = display_topic_representation(W,df_smaller,tlist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnvTbL2u6xjR"
      },
      "outputs": [],
      "source": [
        "df_result.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSRhSrnS6xjR"
      },
      "outputs": [],
      "source": [
        "# We can also look at the distribution of topics per document.\n",
        "# We can extract the columns with topic probabilities\n",
        "# NOTE You will need to edit this to fit with the number of\n",
        "# topics you set above, and the resulting columm names!\n",
        "\n",
        "tmp = df_result[['index', 'topic0', 'topic1', 'topic2', 'topic3']]\n",
        "\n",
        "## Let's sample a few documents\n",
        "tmp = tmp.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdxNvtUb6xjR"
      },
      "source": [
        "Let's plot these"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jOPPQ866xjR"
      },
      "outputs": [],
      "source": [
        "ax = tmp.plot.bar(figsize=(10,10), x='index')\n",
        "ax.legend(tlist.items(), bbox_to_anchor=(1, 0.5));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB3A5qxl6xjR"
      },
      "source": [
        "Let's look at one of these documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGK-jitb6xjR"
      },
      "outputs": [],
      "source": [
        "## choose a document index and assign value to i\n",
        "i = # fill in value here\n",
        "tmp[tmp['index']==i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAci9SiP6xjS"
      },
      "outputs": [],
      "source": [
        "## which category did it belong to?\n",
        "df_smaller[df_smaller['index']==i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DPiOj-h6xjS"
      },
      "outputs": [],
      "source": [
        "## what's in the document? does the generated topic distribution make sense?\n",
        "df_smaller[df_smaller['index']==i]['Document Content'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oaQs-526xjS"
      },
      "outputs": [],
      "source": [
        "## this is another example\n",
        "i = 72\n",
        "tmp[tmp['index']==i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wpe0BPOD6xjS"
      },
      "outputs": [],
      "source": [
        "df_smaller[df_smaller['index']==i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ru2UO7f6xjS"
      },
      "outputs": [],
      "source": [
        "df_smaller[df_smaller['index']==i]['Document Content'].tolist()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}