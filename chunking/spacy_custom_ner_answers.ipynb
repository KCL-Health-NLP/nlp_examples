{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KCL-Health-NLP/nlp_examples/blob/master/chunking/spacy_custom_ner_answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kswfly0s7Ltx"
      },
      "source": [
        "# spaCy for named entity recognition of clinical concepts\n",
        "\n",
        "In this practical, we will try to build a named entity recognition classifier using spaCy.\n",
        "\n",
        "Named entity recognition is a structured learning problem, i.e., we want to learn sequence patterns.\n",
        "\n",
        "We will use data from mtsamples again, and build classifiers that find clinical concepts. \n",
        "\n",
        "The 'gold' standard data is *not* manually annotated, it is the output of a clinical concept recognition system developed by Zeljko Kraljevic called 'CAT' (a predecessor to MedCAT), thus this data is not perfect. This system matches concepts to the entire UMLS. We will only use a few example concepts here.\n",
        "\n",
        "Part of this material is adapted, inspired etc from:\n",
        "\n",
        "https://spacy.io/usage/training\n",
        "\n",
        "\n",
        "Written by Angus Roberts, May 2023, for spaCy 3. Based on an earlier version for spaCy 2 written by Sumithra Velupillai, March 2019. Acknowledgements and many thanks to Zeljko Kraljevic for the data preparation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8cmNaCP7Lt1"
      },
      "outputs": [],
      "source": [
        "# We'll use spaCy for NER.\n",
        "try:\n",
        "    import spacy\n",
        "except ImportError as e:\n",
        "    !pip install spacy\n",
        "    import spacy\n",
        "\n",
        "# Example holds spacy documents,\n",
        "# one with predicted annotations\n",
        "# and one with gold standard annotations\n",
        "from spacy.training import Example\n",
        "\n",
        "# DocBin is a serialiser for spacy documents\n",
        "from spacy.tokens import DocBin\n",
        "\n",
        "# Displacy provides a graphic display of\n",
        "# documents and annotations, and Scorer scores...\n",
        "from spacy import displacy\n",
        "from spacy.scorer import Scorer\n",
        "\n",
        "\n",
        "# requests is a package to submit requests to URLs\n",
        "# We will use it to fetch our data\n",
        "import requests\n",
        "\n",
        "# we use sklearn to split our training data in to train\n",
        "# and dev portions (we have a separate, held out\n",
        "# final test set)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# We will generate warnings for some thing\n",
        "# You might uncomment to ignore them\n",
        "#import warnings\n",
        "#warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1: What version of spaCy do we have?\n",
        "SpaCy has changed a lot between V2 and V3, let's check we have the right version - we want V3"
      ],
      "metadata": {
        "id": "jpml9GIz8WnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy info"
      ],
      "metadata": {
        "id": "vOHPAk-i8Q8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTV0vQIZ7Lt3"
      },
      "source": [
        "# 1: Prepare the corpus\n",
        "We have prepared the data in a json format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jdWX2xx7Lt3"
      },
      "outputs": [],
      "source": [
        "data_url = 'https://github.com/KCL-Health-NLP/nlp_examples/blob/master/chunking/chunking_traindata_CAT_updated_2021.json?raw=true'\n",
        "r = requests.get(data_url)\n",
        "data = r.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RL9GRmi7Lt4"
      },
      "source": [
        "Let's take a look at a random document and its annotations. The json format contains the text itself, and then the start and end offsets for each annotated entity. What are the instances we want to learn?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "eYjmadGhVHFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[35]"
      ],
      "metadata": {
        "id": "TeHglxkJVKfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will split our data 80:20 in to a train set for training and a dev set for testing at each training iteration. We will do this with scikit learn's train_test_split function. Note that we also have a separate, held out test set that we will use at the end."
      ],
      "metadata": {
        "id": "ysuLV8MSVqCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, dev_data = train_test_split(data, train_size=0.8) "
      ],
      "metadata": {
        "id": "Go21IyNoVouS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When training spaCy, we need to pass it a binary file. This can be created from spaCy *DocBin* objects, which is an iterable collection of spaCy *Document* objects."
      ],
      "metadata": {
        "id": "17BzW1ouC5qk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A DocBin is a serialisable SpaCy container that holds\n",
        "# SpaCy documents, and which can be used in SpaCy training.\n",
        "# This function converts our data format in to a DocBin\n",
        "def data_to_docbin(data):\n",
        "  \n",
        "  # The DocBin we will create for this data\n",
        "  db = DocBin()\n",
        "  \n",
        "  # We need to get the spans of our annotations.\n",
        "  # We can do this with a blank pipeline with no\n",
        "  # components. No need to do any other processing.\n",
        "  nlp = spacy.blank('en')\n",
        "\n",
        "  # The data contains text and annotations\n",
        "  for text, annot in data:\n",
        "\n",
        "    # create Document object from text\n",
        "    # this will conatin the tokens and\n",
        "    # their spans\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Now let's get the entities in to a list \n",
        "    ents = []\n",
        "\n",
        "    # The annotations from our data have a start offset,\n",
        "    # an end offset and a label\n",
        "    for start, end, label in annot[\"entities\"]:\n",
        "\n",
        "      # Make a span in our document for these\n",
        "      span = doc.char_span(start, end, label=label)\n",
        "\n",
        "      # ignore any entities with spans that do not align with tokens\n",
        "      # as they will break our training\n",
        "      if span is None:\n",
        "        warnings.warn(f'Skipping entity [{start}, {end}, {label}] : span does not align with token boundaries')\n",
        "      else:\n",
        "        ents.append(span)\n",
        "\n",
        "    # For each document, add the entities to it\n",
        "    # and add the document to the DocBin\n",
        "    doc.set_ents(ents)\n",
        "    db.add(doc)\n",
        "\n",
        "  # return the DocBin containing all the Documents\n",
        "  # with their text and entities\n",
        "  return db\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_KH0dL8objfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now convert our two datasets and serialise them\n",
        "# to disk ready for training\n",
        "train_db = data_to_docbin(train_data)\n",
        "train_db.to_disk(\"./train.spacy\") \n",
        "\n",
        "dev_db = data_to_docbin(dev_data)\n",
        "dev_db.to_disk(\"./dev.spacy\") "
      ],
      "metadata": {
        "id": "SB_aNXhOYjiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check we can deserialise\n",
        "doc_bin = DocBin().from_disk(\"./dev.spacy\")\n",
        "docs = list(doc_bin.get_docs(nlp.vocab))\n",
        "print(len(docs))"
      ],
      "metadata": {
        "id": "ToJbGQ7NKDAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7Qngaj67Lt4"
      },
      "source": [
        "# 2: Training a named entity model with spaCy\n",
        "We can use spaCy to train our own named entity recognition model using their training algorithm.\n",
        "First we need to load a spaCy English language model, so that we can sentence- and word tokenize."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config base_config.cfg config.cfg"
      ],
      "metadata": {
        "id": "h12ZoX30g0j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./dev.spacy"
      ],
      "metadata": {
        "id": "0ETCtSRng1Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2hWekbL7Lt5"
      },
      "source": [
        "What nlp preprocessing parts does this model contain? In spaCy, these are called 'pipes'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yI8mIPf7Lt6"
      },
      "source": [
        "The default named entity pipe in spaCy is not trained for our labels. We have our own named entities that we want to develop a model for. Let's add these entity labels to the spaCy ner pipe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhTOwmcj7Lt6"
      },
      "source": [
        "We don't want to retrain the other pipeline steps, so let's keep those. We only want to retrain the ner pipeline with our own labels and annotations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-_Wt0Wg7Lt8"
      },
      "source": [
        "We have now added a clinical concept entity recognizer in the spaCy nlp model! Let's look at an example document and the predicted entities from the new model. Is it right? Any problems?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlQnLVV37Lt9"
      },
      "outputs": [],
      "source": [
        "text = train_data[17][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiKPjQRV7Lt9"
      },
      "outputs": [],
      "source": [
        "ner = spacy.load('./output/model-best')\n",
        "doc = ner(text)\n",
        "colors = {'ANATOMY': 'lightyellow',\n",
        "           'DISEASESYNDROME': 'pink',  \n",
        "           'SIGNSYMPTOM': 'lightgreen'}\n",
        "displacy.render(doc, style='ent', jupyter=True, options={'colors':colors})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKUxtC917Lt9"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "We can also look at the underlying representation - let's look at one sentence in this document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwl2XL3m7Lt9"
      },
      "outputs": [],
      "source": [
        "print([(x, x.ent_iob_, x.ent_type_) for x in list(doc.sents)[4]])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ner = spacy.load('./output/model-best')\n",
        "ner.add_pipe('sentencizer')\n",
        "doc = ner(text)\n",
        "print([(x, x.ent_iob_, x.ent_type_) for x in list(doc.sents)[4]])"
      ],
      "metadata": {
        "id": "2QHyRtx1eGvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14UI4Ua97Lt9"
      },
      "source": [
        "What do you think? Does it seem like the model works well on this document? Are there concepts that are missed? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmsyehfk7Lt9"
      },
      "source": [
        "# 3: Evaluation\n",
        "How do we know how good this model is? Let's compare with the held out test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTCi4RSY7Lt9"
      },
      "outputs": [],
      "source": [
        "data_url = 'https://github.com/KCL-Health-NLP/nlp_examples/blob/master/chunking/chunking_testdata_CAT_updated_2021.json?raw=true'\n",
        "r = requests.get(data_url)\n",
        "test_data = r.json()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples = []\n",
        "scorer = Scorer()\n",
        "for text, annotations in test_data:\n",
        "    # Run the ner over the text to make predictions\n",
        "    doc = ner(text)\n",
        "    # Create the Example from the predicted doc\n",
        "    # and the gold annotations \n",
        "    example = Example.from_dict(doc, annotations)\n",
        "    examples.append(example)\n",
        "\n",
        "scores = scorer.score(examples)\n",
        "\n",
        "print('Precision: ', scores['ents_p'])\n",
        "print('Recall: ', scores['ents_r'])\n",
        "print('F1: ', scores['ents_f'])\n",
        "\n",
        "print('Per type: ', scores['ents_per_type'])"
      ],
      "metadata": {
        "id": "jWIrpBleiyAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6paQNQ6M7Lt-"
      },
      "source": [
        "Are these good results do you think? Can this be improved? What happens if you increase the number of iterations in the training?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MbIztvE7Lt-"
      },
      "source": [
        "Let's look at a document from the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoFstwct7Lt-"
      },
      "outputs": [],
      "source": [
        "text = test_data[37][0]\n",
        "doc = ner(text)\n",
        "# We use the colours from before:\n",
        "#colors = {'ANATOMY': 'lightyellow',\n",
        "#           'DISEASESYNDROME': 'pink',  \n",
        "#           'SIGNSYMPTOM': 'lightgreen'}\n",
        "displacy.render(doc, style='ent', jupyter=True, options={'colors':colors})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH52PANt7Lt-"
      },
      "source": [
        "What does the underlying representation look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSlYISKH7Lt-"
      },
      "outputs": [],
      "source": [
        "print([(x, x.ent_iob_, x.ent_type_) for x in list(doc.sents)[1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekHbvGlg7Lt-"
      },
      "source": [
        "There are other options available using spaCy, training models etc. If interested, look at their website, e.g. https://spacy.io/usage/training"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}