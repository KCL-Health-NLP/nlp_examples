{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entity recognition - clinical concepts\n",
    "\n",
    "In this practical, we will try to build a named entity recognition classifier using spaCy and crfsuite.\n",
    "\n",
    "Named entity recognition is a structured learning problem, i.e., we want to learn sequence patterns.\n",
    "\n",
    "We will use data from mtsamples again, and build classifiers that find clinical concepts. The gold standard data is not manually annotated, it is the output of a clinical concept recognition system developed by Zeljko Kraljevic called 'CAT'. This system matches concepts to the entire UMLS. We will only use a few example concepts here.\n",
    "\n",
    "Part of this material is adapted, inspired etc from:\n",
    "\n",
    "https://spacy.io/usage/training,\n",
    "\n",
    "https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html,\n",
    "\n",
    "https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da\n",
    "\n",
    "Written by Sumithra Velupillai, March 2019 - acknowledgements and many thanks to Zeljko for the data preparations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    import sklearn_crfsuite\n",
    "except ImportError as e:\n",
    "    !pip install sklearn_crfsuite\n",
    "    import sklearn_crfsuite\n",
    "\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "import scipy\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: corpus\n",
    "We have prepared training and test data in a json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = 'https://github.com/KCL-Health-NLP/nlp_examples/blob/master/chunking/chunking_trainingdata_CAT.json?raw=true'\n",
    "r = requests.get(data_url)\n",
    "train_data = r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a random document and its annotations. The json format contains the text itself, and then the start and stop offsets for each entity. What are the instances we want to learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': [[419, 428, 'ANATOMY'],\n",
       "  [457, 473, 'ANATOMY'],\n",
       "  [479, 490, 'ANATOMY'],\n",
       "  [597, 605, 'ANATOMY'],\n",
       "  [607, 621, 'ANATOMY'],\n",
       "  [634, 643, 'ANATOMY']]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[14][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Training a named entity model with spaCy\n",
    "We can use spaCy to train our own named entity recognition model using their training algorithm.\n",
    "First we need to load a spaCy English language model, so that we can sentence- and word tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What nlp preprocessing parts does this model contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our own named entities that we want to develop a model for. Let's add these entity labels to the spaCy ner pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "        \n",
    "labels = set()\n",
    "for _, annotations in train_data:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "            labels.add(ent[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want to retrain the other pipeline steps, so let's keep those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "other_pipes = [\"tagger\", \"parser\"]\n",
    "other_pipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What entities do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DISEASESYNDROME', 'SIGNSYMPTOM', 'ANATOMY'}\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our clinical concept ner model. Let's set the number of training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter=(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1987.194958943256}\n"
     ]
    }
   ],
   "source": [
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "#with nlp.disable_pipes(other_pipes):  # only train NER\n",
    "        # reset and initialize the weights randomly â€“ but only if we're\n",
    "        # training a new model\n",
    "        nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = spacy.util.minibatch(train_data, size=spacy.util.compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now added a clinical concept entity recognizer in the spaCy nlp model! Let's look at an example document and the predicted entities from the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, _ = train_data[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(text)\n",
    "colors = {'ANATOMY': 'lightyellow',\n",
    "           'DISEASESYNDROME': 'pink',  \n",
    "           'SIGNSYMPTOM': 'lightgreen'}\n",
    "displacy.render(doc2, style='ent', jupyter=True, options={'colors':colors})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the underlying representation - let's look at one sentence in this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(x, x.ent_iob_, x.ent_type_) for x in list(doc2.sents)[5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think? Does it seem like the model works well on this document? Are there concepts that are missed? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Evaluation\n",
    "How do we know how good this model is? Let's compare with the 'gold standard' test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = Scorer()\n",
    "\n",
    "data_url = 'https://github.com/KCL-Health-NLP/nlp_examples/blob/master/chunking/chunking_testdata_CAT.json?raw=true'\n",
    "r = requests.get(data_url)\n",
    "test_data = r.json()\n",
    "\n",
    "for text, entity_offsets in test_data:\n",
    "    doc = nlp.make_doc(text)\n",
    "    gold = GoldParse(doc, entities=entity_offsets.get('entities'))\n",
    "    doc = nlp(text)\n",
    "    scorer.score(doc, gold)\n",
    "print('Precision: ',scorer.scores['ents_p'])\n",
    "print('Recall: ',scorer.scores['ents_r'])\n",
    "print('F1: ',scorer.scores['ents_f'])\n",
    "#print(scorer.scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are these good results do you think? Can this be improved? What happens if you increase the number of iterations in the training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a document from the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, entity_offsets = test_data[37]\n",
    "doc2 = nlp(text)\n",
    "colors = {'ANATOMY': 'lightyellow',\n",
    "           'DISEASESYNDROME': 'pink',  \n",
    "           'SIGNSYMPTOM': 'lightgreen'}\n",
    "displacy.render(doc2, style='ent', jupyter=True, options={'colors':colors})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the underlying representation look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp.make_doc(text)\n",
    "gold = GoldParse(doc2, entities=entity_offsets.get('entities'))\n",
    "gold.ner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Training a model with crfsuite\n",
    "There are other machine learning algorithms that can be used for this sequence learning problem. Let's try crfsuite. If you don't have this package, install with: pip install sklearn-crfsuite\n",
    "\n",
    "Let's use some functions to get sentences and tokens in the right format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(filename, bio=False):\n",
    "    \n",
    "    r = requests.get(filename)\n",
    "    train_data = r.json()\n",
    "\n",
    "    sentences = []\n",
    "    nlp = spacy.load('en', entity=False, parser=False)\n",
    "\n",
    "    print('reading data: ', filename)\n",
    "    for text, entity_offsets in train_data:\n",
    "        doc = nlp(text)\n",
    "        gold = spacy.gold.biluo_tags_from_offsets(doc, entity_offsets.get('entities'))\n",
    "        counter = 0\n",
    "        for s in doc.sents:\n",
    "            sent = []\n",
    "            for t in s:\n",
    "                l = gold[counter]\n",
    "                if bio:\n",
    "                    l = l.replace('L-', 'I-')\n",
    "                    l = l.replace('U-', 'B-')\n",
    "                w = t.text, t.pos_, l\n",
    "                sent.append(w)\n",
    "                counter +=1\n",
    "            sentences.append(sent)\n",
    "    print('done')\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use these functions and read in the training and test data. \n",
    "There are different alternative token level representations that can be used.\n",
    "The BIO format (Begin, Inside, Outside) or the BILOU format (Begin, Inside, Last, Outside, Unit).\n",
    "What do you think is better or worse with each of these?\n",
    "In the function below, you can choose either format with the boolean flag 'bio'. Let's start with BIO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data:  https://github.com/KCL-Health-NLP/nlp_examples/blob/master/chunking/chunking_trainingdata_CAT.json?raw=true\n",
      "done\n",
      "reading data:  https://github.com/KCL-Health-NLP/nlp_examples/blob/master/chunking/chunking_testdata_CAT.json?raw=true\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "train_sents = get_sentences('https://github.com/KCL-Health-NLP/nlp_examples/blob/master/chunking/chunking_trainingdata_CAT.json?raw=true', bio=True)\n",
    "test_sents = get_sentences('https://github.com/KCL-Health-NLP/nlp_examples/blob/master/chunking/chunking_testdata_CAT.json?raw=true', bio=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the feature and label vectors for the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What labels do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-DISEASESYNDROME',\n",
       " 'I-DISEASESYNDROME',\n",
       " 'B-ANATOMY',\n",
       " '-',\n",
       " 'B-SIGNSYMPTOM',\n",
       " 'I-SIGNSYMPTOM',\n",
       " 'I-ANATOMY',\n",
       " 'O']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(set(x for l in y_test for x in l))\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    ")\n",
    "crf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5: evaluation\n",
    "How does this model perform on this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9819657932011259"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "B-DISEASESYNDROME       0.89      0.73      0.80       489\n",
      "I-DISEASESYNDROME       0.89      0.57      0.69       263\n",
      "        B-ANATOMY       0.95      0.85      0.90       950\n",
      "                -       0.00      0.00      0.00         1\n",
      "    B-SIGNSYMPTOM       0.93      0.73      0.82       308\n",
      "    I-SIGNSYMPTOM       0.92      0.69      0.79       125\n",
      "        I-ANATOMY       0.92      0.80      0.85       363\n",
      "                O       0.99      1.00      0.99     35888\n",
      "\n",
      "        micro avg       0.98      0.98      0.98     38387\n",
      "        macro avg       0.81      0.67      0.73     38387\n",
      "     weighted avg       0.98      0.98      0.98     38387\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.flat_classification_report(y_test, y_pred, labels = labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think? There's a huge imbalance in the number of instances. Do we really want to evaluate the 'O' label? There's also one instance with an erroneous label ('-') Let's look at the results without these labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-DISEASESYNDROME',\n",
       " 'I-DISEASESYNDROME',\n",
       " 'B-ANATOMY',\n",
       " 'B-SIGNSYMPTOM',\n",
       " 'I-SIGNSYMPTOM',\n",
       " 'I-ANATOMY']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(set(x for l in y_test for x in l if x !='O' and x!='-'))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "B-DISEASESYNDROME       0.89      0.73      0.80       489\n",
      "I-DISEASESYNDROME       0.89      0.57      0.69       263\n",
      "        B-ANATOMY       0.95      0.85      0.90       950\n",
      "    B-SIGNSYMPTOM       0.93      0.73      0.82       308\n",
      "    I-SIGNSYMPTOM       0.92      0.69      0.79       125\n",
      "        I-ANATOMY       0.92      0.80      0.85       363\n",
      "\n",
      "        micro avg       0.92      0.77      0.84      2498\n",
      "        macro avg       0.92      0.73      0.81      2498\n",
      "     weighted avg       0.92      0.77      0.84      2498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.flat_classification_report(y_test, y_pred, labels = labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was quite different! \n",
    "Try training this model with the BILOU scheme instead. Are results better or worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data:  https://github.com/KCL-Health-NLP/nlp_examples/blob/master/chunking/chunking_trainingdata_CAT.json?raw=true\n",
      "done\n",
      "reading data:  https://github.com/KCL-Health-NLP/nlp_examples/blob/master/chunking/chunking_testdata_CAT.json?raw=true\n",
      "done\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "B-DISEASESYNDROME       0.86      0.63      0.73       190\n",
      "I-DISEASESYNDROME       0.91      0.40      0.55        73\n",
      "U-DISEASESYNDROME       0.90      0.79      0.84       299\n",
      "        B-ANATOMY       0.88      0.76      0.81       277\n",
      "    B-SIGNSYMPTOM       0.88      0.67      0.76        87\n",
      "    L-SIGNSYMPTOM       0.92      0.70      0.80        87\n",
      "        I-ANATOMY       0.97      0.74      0.84        84\n",
      "        U-ANATOMY       0.97      0.87      0.92       673\n",
      "    U-SIGNSYMPTOM       0.95      0.76      0.85       221\n",
      "    I-SIGNSYMPTOM       0.89      0.66      0.76        38\n",
      "L-DISEASESYNDROME       0.90      0.66      0.76       190\n",
      "        L-ANATOMY       0.91      0.79      0.85       279\n",
      "\n",
      "        micro avg       0.92      0.76      0.84      2498\n",
      "        macro avg       0.91      0.70      0.79      2498\n",
      "     weighted avg       0.92      0.76      0.83      2498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_sents = get_sentences('https://github.com/KCL-Health-NLP/nlp_examples/blob/master/chunking/chunking_trainingdata_CAT.json?raw=true', bio=False)\n",
    "test_sents = get_sentences('https://github.com/KCL-Health-NLP/nlp_examples/blob/master/chunking/chunking_testdata_CAT.json?raw=true', bio=False)\n",
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]\n",
    "\n",
    "new_classes = list(set(x for l in y_test for x in l if x !='O' and x!='-'))\n",
    "\n",
    "c1=0.1\n",
    "c2=0.1\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=c1,\n",
    "    c2=c2,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    ")\n",
    "crf.fit(X_train, y_train)\n",
    "y_pred = crf.predict(X_test)\n",
    "print(metrics.flat_classification_report(y_test, y_pred, labels = new_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: cross-validation to find best parameters with crfsuite\n",
    "We have used default parameters in the above. We can try to find the best parameters on the training data by cross-validation. __This takes some time though (even with only 3 folds)!__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from: https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html#hyperparameter-optimization\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=new_classes)\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=50,\n",
    "                        scoring=f1_scorer)\n",
    "rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = rs.best_estimator_\n",
    "y_pred = crf.predict(X_test)\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=new_classes, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think? Are there other parameters that could be tested in the cross-validation setup? What about the measure used for optimisation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
