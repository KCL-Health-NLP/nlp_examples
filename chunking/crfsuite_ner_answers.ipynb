{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KCL-Health-NLP/nlp_examples/blob/master/chunking/crfsuite_ner_answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kswfly0s7Ltx"
      },
      "source": [
        "# CRF for named entity recognition of clinical concepts\n",
        "\n",
        "Named entity recognition is a structured learning problem, i.e., we want to learn sequence patterns.\n",
        "\n",
        "In the previous practical we used spaCy to build a clinical NER system. spaCy used a multilayer CNN for NER, and can also be customised to use other ANN architectures such as transformers.\n",
        "\n",
        "There are other machine learning algorithms that can be used for this sequence learning problem. In this practical we will try CRF, using crfsuite, a package developed to integrate CRF with scikit learn.\n",
        "\n",
        "We will use data from mtsamples again, and build classifiers that find clinical concepts. \n",
        "\n",
        "The 'gold' standard data is *not* manually annotated, it is the output of a clinical concept recognition system developed by Zeljko Kraljevic called 'CAT' (a predecessor to MedCAT), thus this data is not perfect. This system matches concepts to the entire UMLS. We will only use a few example concepts here.\n",
        "\n",
        "Part of this material is adapted, inspired etc from:\n",
        "\n",
        "https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html,\n",
        "\n",
        "Written by Sumithra Velupillai, March 2019, updated February 2021. Updated May 2023 by Angus Roberts acknowledgements and many thanks to Zeljko Kraljevic for the data preparations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8cmNaCP7Lt1"
      },
      "outputs": [],
      "source": [
        "# By default, pip will install the original sklearn_crfsuite package from PyPI\n",
        "# However, this is not compatible with more recent version of sklearn, and is no longer \n",
        "# being maintained. So we will install from a github fork that is being maintained.\n",
        "# You might be able to go back to the PyPI version in the future, if someone\n",
        "# starts maintaining it again.\n",
        "try:\n",
        "  import sklearn_crfsuite\n",
        "except ImportError as e:\n",
        "  !pip install git+https://github.com/MeMartijn/updated-sklearn-crfsuite.git#egg=sklearn_crfsuite\n",
        "  #!pip install sklearn_crfsuite\n",
        "  import sklearn_crfsuite\n",
        "\n",
        "\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics\n",
        "\n",
        "# We use sklearn for scoring, metrics,\n",
        "# and parameter searching\n",
        "#import sklearn\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# We use scipy to make exponential continuous random variables\n",
        "# when parameter searching\n",
        "import scipy\n",
        "\n",
        "# import random\n",
        "\n",
        "# requests is a package to submit requests to URLs\n",
        "# We will use it to fetch our data\n",
        "import requests\n",
        "\n",
        "# We use spacy to create our BILOU tags\n",
        "import spacy\n",
        "from spacy.training import offsets_to_biluo_tags\n",
        "\n",
        "# You might choose to turn off warnings - could be for\n",
        "# documents with no entities, etc\n",
        "import warnings\n",
        "#warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLnUgt997Lt-"
      },
      "source": [
        "# 1: Preparing the data for crfsuite\n",
        "Our data is in the same format as in the spaCy NER practical: a json file. We'll need a few functions to get it in to the right format for crfsuite.\n",
        "\n",
        "Our sentences will be lists of tuples, each tuple containing a token string, its POS tag, and its BIO (or BILUO) entity tag. So for the sentence \"he has cancer\", we will have something like this:\n",
        "\n",
        "```\n",
        "[\n",
        "  ('he','PRONOUN','O'),\n",
        "  ('has','VERB','O'),\n",
        "  ('cancer','NOUN','B-DISEASE')\n",
        "]\n",
        "```\n",
        "\n",
        "We therefore need to get these POS tags and BIO / BILUO tags for our training data, from the json of text and entity annotations. We will use spaCy to do this. Note that we are using spaCy to do POS tagging and to convert gold standard entity annotations in to BIO / BILUO tags. We are not using it to do the NER itself.\n",
        "\n",
        "So we will exclude the NER component from spaCy when we load it.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use a spacy pipeline to POS and BILUO tag our data.\n",
        "# We do not need to have NER, as we will use CRF for that.\n",
        "try:\n",
        "  nlp = spacy.load('en_core_web_sm', exclude=['ner'])\n",
        "except OSError as e:\n",
        "  !python -m spacy download en_core_web_sm\n",
        "  nlp = spacy.load('en_core_web_sm', exclude=['ner'])"
      ],
      "metadata": {
        "id": "_5hFUHfaoB8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the pipeline:"
      ],
      "metadata": {
        "id": "8_wpod7KRr_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(nlp.pipe_names)"
      ],
      "metadata": {
        "id": "Nz6HjFbPoufi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now write a function to load our data and get the POS tags and BILUO or BIO tags."
      ],
      "metadata": {
        "id": "OsJexJZ7SDM9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nInoO4kn7Lt_"
      },
      "outputs": [],
      "source": [
        "# This function loads data from a json file of documents\n",
        "# and their entity annotations. It uses spaCy to get POS\n",
        "# and BIO / BILUO tags for each token, and returns a list of\n",
        "# sentences, with each sentence itself being a list of\n",
        "# (token, POS-tag, BILUO-tag) tuples.\n",
        "# The parameter bio flags whether BIO tags should be used\n",
        "# instead of BILUO tags.\n",
        "def get_sentences(filename, bio=False):\n",
        "    \n",
        "    # Read in the data\n",
        "    print('reading data: ', filename)\n",
        "    r = requests.get(filename)\n",
        "    data = r.json()\n",
        " \n",
        "    # List to hold our sentences\n",
        "    sentences = []\n",
        "\n",
        "    # For each document and its entity annotations\n",
        "    for text, entities in data:\n",
        "\n",
        "        # Process with spaCy. This will POS tag, and\n",
        "        # allow us to create BILUO tags on the tokens.\n",
        "        doc = nlp(text)\n",
        "\n",
        "        # A handy spacy function to create a list of\n",
        "        # BILUO tags for a document, from a list of entities\n",
        "        tags = offsets_to_biluo_tags(doc, entities['entities'])\n",
        "\n",
        "        # Keep track of which BILUO tag we are on\n",
        "        tag_counter = 0\n",
        "\n",
        "        # Go through the sentences in the document,\n",
        "        # and the tokens in the sentence\n",
        "        for sent in doc.sents:\n",
        "            tagged_sentence = []\n",
        "            for tok in sent:\n",
        "\n",
        "                # Get the current tag\n",
        "                tag = tags[tag_counter]\n",
        "\n",
        "                # Convert to BIO if the bio flag is set\n",
        "                if bio:\n",
        "                    tag = tag.replace('L-', 'I-')\n",
        "                    tag = tag.replace('U-', 'B-')\n",
        "\n",
        "                # Make a tuple for our token\n",
        "                # and add it to the tagged sentences list\n",
        "                w = (tok.text, tok.pos_, tag)\n",
        "                tagged_sentence.append(w)\n",
        "\n",
        "                # Move to the next BILUO tag\n",
        "                tag_counter +=1\n",
        "\n",
        "            # Add the list of token tuples for this sentence\n",
        "            # to our list of all sentences\n",
        "            sentences.append(tagged_sentence)\n",
        "\n",
        "    print('done')\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use this function to read in our test and training data. There are different alternative token level representations that can be used. The BIO format (Begin, Inside, Outside) or the BILUO format (Begin, Inside, Last, Unit, Outside). What do you think is better or worse with each of these? In the function, you can choose either format with the boolean flag 'bio'. Let's start with BIO."
      ],
      "metadata": {
        "id": "KrNDgBwEU8MW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZoX9jD-7LuG"
      },
      "outputs": [],
      "source": [
        "train_sents = get_sentences('https://github.com/KCL-Health-NLP/nlp_examples/blob/master/chunking/chunking_traindata_CAT_updated_2021.json?raw=true', bio=True)\n",
        "test_sents = get_sentences('https://github.com/KCL-Health-NLP/nlp_examples/blob/master/chunking/chunking_testdata_CAT_updated_2021.json?raw=true', bio=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at some of the data. Try a few, to find some different BILUO / BIO tags"
      ],
      "metadata": {
        "id": "Oj3ByPbYVg0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_sents[7])"
      ],
      "metadata": {
        "id": "7bzbASFppo0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2: Defining features for our word instances\n",
        "\n",
        "We now need to create some features for the CRF model. With spaCy, we did not have to consider feature engineering, as the neural model uses learns the features. For CRF, we will create features to represent the orthographical and lexical properties of our word, and of the words on each side of it. We are hoping that you can tell whether a word is the beginning / inside / outside of an entity based on things like its part of speech, how it is cased, and those same properties of the words around it.\n",
        "\n",
        "The features for a word will be represented by a dictionary of feature names to feature values. We will write a function that given a sentence and an index i, will create a dictionary of features for the ith word in the sentence. The function needs the whole sentence, so it can make features representing words on either side of the ith word."
      ],
      "metadata": {
        "id": "RxLzkP7-WJTg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsMr1bBY7Lt_"
      },
      "outputs": [],
      "source": [
        "# Parameters: a sentence and an index i.\n",
        "# The sentence is a list of words, each word being\n",
        "# a tuple of the form (token, POS-tag, BILUO/BIO-tag).\n",
        "# Index i is the index of a word in the sentence list.\n",
        "# Returns: a dictionary of features for the\n",
        "# ith word in the sentence\n",
        "def word2features(sent, i):\n",
        "\n",
        "    # Get our word\n",
        "    word = sent[i][0]\n",
        "\n",
        "    # Get the POS tag for the word\n",
        "    postag = sent[i][1]\n",
        "\n",
        "    # Make some features\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'word.lower()': word.lower(),     # the lower cased word\n",
        "        'word[-3:]': word[-3:],           # the last three characters of the word\n",
        "        'word.isupper()': word.isupper(), # Is the word upper case?\n",
        "        'word.istitle()': word.istitle(), # is the word title case?\n",
        "        'word.isdigit()': word.isdigit(), # is the word a digit\n",
        "        'postag': postag,                 # pos tag of the word\n",
        "        'postag[:2]': postag[:2],         # first two characters of pos tag\n",
        "    }\n",
        "    if i > 0:                             # If the word is not the first in a sentence\n",
        "        word1 = sent[i-1][0]              # add some features from the word before it\n",
        "        postag1 = sent[i-1][1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "            '-1:postag': postag1,\n",
        "            '-1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True            # If the word is the first in a sentence\n",
        "                                          # set a Beginning Of Sentence feature\n",
        "\n",
        "    if i < len(sent)-1:                   # If the word is not the last in a sentence\n",
        "        word1 = sent[i+1][0]              # add some features from the word after it\n",
        "        postag1 = sent[i+1][1]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "            '+1:postag': postag1,\n",
        "            '+1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True            # If the word is the last in a sentence\n",
        "                                          # set a End Of Sentence feature\n",
        "\n",
        "    # We've built the features dictionary, return it\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try this out, and see what features we get. We'll try it on a dummy sentence:\n",
        "\n",
        "```\n",
        "[\n",
        "  ('An', 'DET', 'O'), \n",
        "  ('elephant', 'NOUN', 'O'), \n",
        "  ('sitting', 'VERB', 'O')\n",
        "]\n",
        "```\n",
        "\n",
        "and we will look at the features for word index 2, the last word (sitting). Take a look at the features. Why might they be useful in determining if something is an entity?\n",
        "\n"
      ],
      "metadata": {
        "id": "m9rmwV8aZlN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = [('An', 'DET', 'O'), ('elephant', 'NOUN', 'O'),  ('sitting', 'VERB', 'O')]\n",
        "features = word2features(sentence, 2)\n",
        "for k, v in features.items():\n",
        "  print(f'{k:20}{v}')"
      ],
      "metadata": {
        "id": "txYn4UD7aQQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3: Making feature and label vectors\n",
        "\n",
        "Finally, we will define two more convenience functions, to get all the features for all the words in a sentence, and to get all the BIO / BILUO labels from a sentence. We will use these to make our feature and label vectors."
      ],
      "metadata": {
        "id": "RPr4rRyAcJyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Given a sentence, returns all the features\n",
        "# for all the words in that sentence\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "# Given a sentence, return all the BIO / BILUO tags\n",
        "# for all the words in the sentence\n",
        "def sent2labels(sent):\n",
        "    return [label for token, postag, label in sent]\n"
      ],
      "metadata": {
        "id": "xQHELim3Zh_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDKEjVZ17LuG"
      },
      "source": [
        "Now let's create the feature and label vectors for the training and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMbxDNVp7LuG"
      },
      "outputs": [],
      "source": [
        "X_train = [sent2features(s) for s in train_sents]\n",
        "y_train = [sent2labels(s) for s in train_sents]\n",
        "\n",
        "X_test = [sent2features(s) for s in test_sents]\n",
        "y_test = [sent2labels(s) for s in test_sents]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at a little bit of these vectors. We will print the feature vector for a single sentence, and the label vector for the same sentence. These are our training instance features, and the class labels, for each word in a sentence."
      ],
      "metadata": {
        "id": "MG90ybKbcw7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_number = 5\n",
        "print(\"Feature vector:\\n\")\n",
        "for features in X_train[sentence_number]:\n",
        "  print(features)\n",
        "print(\"\\n\\nLabel vector:\\n\")\n",
        "print(y_train[sentence_number])"
      ],
      "metadata": {
        "id": "Vz9JYok8c2az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "876SiJa27LuG"
      },
      "source": [
        "What labels do we have? What is the set of all possible labels?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1B11WtZ7LuG"
      },
      "outputs": [],
      "source": [
        "labels = list(set(x for l in y_test for x in l))\n",
        "labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7Ehqw_17LuH"
      },
      "source": [
        "# 4: Train the model\n",
        "\n",
        "Now we have features and labels for all of our words, we can finally train a CRF model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7JdLtnN7LuH"
      },
      "outputs": [],
      "source": [
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',             # gradient descent\n",
        "    c1=0.1,                        # L1 regularisation\n",
        "    c2=0.1,                        # L2 regularisation\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True  # Consider transitions not in the training data\n",
        ")\n",
        "crf.fit(X_train, y_train);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-uAePjJ7LuH"
      },
      "source": [
        "# 5: Evaluation\n",
        "How does this model perform on our test data? Let's look at the f1 score first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Wk5k9uy7LuH"
      },
      "outputs": [],
      "source": [
        "# Make predictions for the test data\n",
        "y_pred = crf.predict(X_test)\n",
        "\n",
        "# Compare the predicitons to the gold standard labels\n",
        "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl__4Pei7LuH"
      },
      "source": [
        "We can also print a classification report with more details and metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXtFmUqo7LuI"
      },
      "outputs": [],
      "source": [
        "from sklearn_crfsuite.utils import flatten\n",
        "print(metrics.flat_classification_report(y_test, y_pred, labels=labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SL-RZrE7LuI"
      },
      "source": [
        "What do you think? There's a huge imbalance in the number of instances. Do we really want to evaluate the 'O' label? There's also one instance with an erroneous label ('-') Let's look at the results without these labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVwsR2WR7LuI"
      },
      "outputs": [],
      "source": [
        "# We'll change our list of labels to exclude O and -\n",
        "labels = list(set(x for l in y_test for x in l if x !='O' and x!='-'))\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sr9cyDhj7LuI"
      },
      "outputs": [],
      "source": [
        "# Now re-evaluate with our restricted list of labels\n",
        "print(metrics.flat_classification_report(y_test, y_pred, labels = labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoZgQRTl7LuI"
      },
      "source": [
        "This was quite different! Can you explain the difference?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6: Changing the tag scheme\n",
        "\n",
        "Try training this model with the BILUO scheme instead. We can do this by converting the BIO tags in the get_sentences function with the boolean flag 'BIO'. Are results better or worse?"
      ],
      "metadata": {
        "id": "HwgjLs9Vg6iJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Y-cIfmj7LuI"
      },
      "outputs": [],
      "source": [
        "training_file = 'https://github.com/KCL-Health-NLP/nlp_examples/blob/master/chunking/chunking_traindata_CAT_updated_2021.json?raw=true'\n",
        "test_file = 'https://github.com/KCL-Health-NLP/nlp_examples/blob/master/chunking/chunking_testdata_CAT_updated_2021.json?raw=true'\n",
        "train_sents = get_sentences(training_file, bio=False)\n",
        "test_sents = get_sentences(test_file, bio=False)\n",
        "X_train = [sent2features(s) for s in train_sents]\n",
        "y_train = [sent2labels(s) for s in train_sents]\n",
        "\n",
        "X_test = [sent2features(s) for s in test_sents]\n",
        "y_test = [sent2labels(s) for s in test_sents]\n",
        "\n",
        "new_classes = list(set(x for l in y_test for x in l if x !='O' and x!='-'))\n",
        "\n",
        "c1=0.1\n",
        "c2=0.1\n",
        "\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=c1,\n",
        "    c2=c2,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True,\n",
        ")\n",
        "crf.fit(X_train, y_train)\n",
        "y_pred = crf.predict(X_test)\n",
        "print(metrics.flat_classification_report(y_test, y_pred, labels = new_classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_48ygRc_7LuI"
      },
      "source": [
        "# Optional: cross-validation to find best parameters with crfsuite\n",
        "We have used default parameters in the above. We can try to find the best parameters on the training data by cross-validation. \n",
        "\n",
        "__This takes some time, 20 - 30 minutes (even with only 3 folds)!__ \n",
        "\n",
        "You might make it a bit faster by re-reading your data, this time reverting to BIO tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5geagZA7LuJ"
      },
      "outputs": [],
      "source": [
        "# from: https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html#hyperparameter-optimization\n",
        "\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "params_space = {\n",
        "    'c1': scipy.stats.expon(scale=0.5),\n",
        "    'c2': scipy.stats.expon(scale=0.05),\n",
        "}\n",
        "\n",
        "# use the same metric for evaluation\n",
        "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
        "                        average='weighted', labels=new_classes)\n",
        "\n",
        "# search\n",
        "rs = RandomizedSearchCV(crf, params_space,\n",
        "                        cv=3,\n",
        "                        verbose=1,\n",
        "                        n_jobs=-1,\n",
        "                        n_iter=50,\n",
        "                        scoring=f1_scorer)\n",
        "rs.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKjwDiLQ7LuJ"
      },
      "outputs": [],
      "source": [
        "print('best params:', rs.best_params_)\n",
        "print('best CV score:', rs.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceiqIXwo7LuJ"
      },
      "outputs": [],
      "source": [
        "crf = rs.best_estimator_\n",
        "y_pred = crf.predict(X_test)\n",
        "print(metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=new_classes, digits=3\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NA0GECi7LuJ"
      },
      "source": [
        "What do you think? Are there other parameters that could be tested in the cross-validation setup? What about the measure used for optimisation?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}