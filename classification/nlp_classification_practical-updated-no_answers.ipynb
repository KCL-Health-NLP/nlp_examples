{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP classification - supervised learning\n",
    "\n",
    "In this example, you will learn how you can use supervised learning algorithms for NLP classification. We will use documents from mtsamples again. The task is to classify a document into its clinical specialty, e.g. pediatrics or hematology.\n",
    "\n",
    "We will use classification algorithms as implemented in sci-kit learn, and evaluate with cross-validation before testing on unseen test data.\n",
    "\n",
    "Written by Sumithra Velupillai, March 2019. Updated August 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Corpus\n",
    "Read in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdata = pd.read_pickle('training_data.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the content. What are the labels we want to try to learn? How many instances do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdata['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What types of features do you think would be useful for the classification task? Where can we get them? Take a look at one or two of the documents. Can you guess which classification label these belong to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingtxt_example = trainingdata['txt'].tolist()[0]\n",
    "print(trainingtxt_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingtxt_example = trainingdata['txt'].tolist()[231]\n",
    "print(trainingtxt_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common baseline feature representation for text classification tasks is to use the bag-of-words representation, in a document-term matrix. Let's build a simple one using raw counts and only keeping a maximum of 500 features. We can use the CountVectorizer function from sklearn, and tokenize using a function from nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_vectorizer = CountVectorizer(ngram_range=(1,1), stop_words=None,\n",
    "                             tokenizer=word_tokenize, max_features=500)\n",
    "first_vectorizer.fit(trainingdata['txt'].tolist())\n",
    "first_fit_transformed_data = first_vectorizer.fit_transform(trainingdata['txt'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at this transformed representation for an example document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_transformed_data = first_vectorizer.transform([trainingdata['txt'].tolist()[231]])\n",
    "print (first_transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What word is represented by the different indices? Have a look at a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (first_vectorizer.get_feature_names()[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_fit_transformed_data.shape)\n",
    "print ('Amount of Non-Zero occurences: ', first_fit_transformed_data.nnz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a classifier with this feature representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomialNB_classifier = MultinomialNB().fit(first_fit_transformed_data, trainingdata['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a trained multinomial Naive Bayes model. But how do we know how well it works? Let's evaluate it on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = pd.read_pickle('test_data.pickle')\n",
    "## We need to transform this data to the same representation\n",
    "first_fit_transformed_testdata = first_vectorizer.transform(testdata['txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_fit_transformed_testdata\n",
    "multinomialNB_predicted = multinomialNB_classifier.predict(first_fit_transformed_testdata)\n",
    "multinomialNB_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(testdata['label'], multinomialNB_predicted, target_names=set(testdata['label'].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This didn't look too bad maybe? But there are probably ways of improving this, by changing the feature space or maybe trying a different classifier model. \n",
    "__There is one main problem though: we can't use this test data to try different configurations! Why?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can however try some different feature representations and classifier algorithms on the training data. Let's try finding a model we think will work well on unseen data by employing n-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomialNB_classifier = MultinomialNB().fit(first_fit_transformed_data, trainingdata['label'])\n",
    "scoring = ['precision_macro', 'recall_macro','precision_micro','recall_micro', 'f1_micro', 'f1_macro']\n",
    "scores = cross_validate(multinomialNB_classifier, first_fit_transformed_data, trainingdata['label'], scoring=scoring, cv=10, return_train_score=False)\n",
    "scoresdf = pd.DataFrame(scores)\n",
    "scoring = ['test_precision_macro', 'test_recall_macro','test_precision_micro','test_recall_micro', 'test_f1_micro', 'test_f1_macro']\n",
    "bp = scoresdf.boxplot(column=scoring, grid=False, rot=45,)\n",
    "[ax_tmp.set_xlabel('') for ax_tmp in np.asarray(bp).reshape(-1)]\n",
    "fig = np.asarray(bp).reshape(-1)[0].get_figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we try another classifier? Let's try a random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0).fit(first_fit_transformed_data, trainingdata['label'])\n",
    "scoring = ['precision_macro', 'recall_macro','precision_micro','recall_micro', 'f1_micro', 'f1_macro']\n",
    "scores = cross_validate(rf_classifier, first_fit_transformed_data, trainingdata['label'], scoring=scoring, cv=10, return_train_score=False)\n",
    "scoresdf = pd.DataFrame(scores)\n",
    "scoring = ['test_precision_macro', 'test_recall_macro','test_precision_micro','test_recall_micro', 'test_f1_micro', 'test_f1_macro']\n",
    "bp = scoresdf.boxplot(column=scoring, grid=False, rot=45,)\n",
    "[ax_tmp.set_xlabel('') for ax_tmp in np.asarray(bp).reshape(-1)]\n",
    "fig = np.asarray(bp).reshape(-1)[0].get_figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was this better or worse? Are there any parameters worth changing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used a very simple bag-of-words representation. What happens if we try something else? Let's try tf-idf. This is considered a strong baseline in many text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopWords)\n",
    "tfidf_vect.fit(trainingdata['txt'])\n",
    "second_fit_transformed_data =  tfidf_vect.transform(trainingdata['txt'])\n",
    "second_fit_transformed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What other parameters can you change in this representation? How does this look different from the CountVectorizer representation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use this with the Multinomial Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomialNB_classifier = MultinomialNB().fit(second_fit_transformed_data, trainingdata['label'])\n",
    "scoring = ['precision_macro', 'recall_macro','precision_micro','recall_micro', 'f1_micro', 'f1_macro']\n",
    "scores = cross_validate(multinomialNB_classifier, second_fit_transformed_data, trainingdata['label'], scoring=scoring, cv=10, return_train_score=False)\n",
    "scoresdf = pd.DataFrame(scores)\n",
    "scoring = ['test_precision_macro', 'test_recall_macro','test_precision_micro','test_recall_micro', 'test_f1_micro', 'test_f1_macro']\n",
    "bp = scoresdf.boxplot(column=scoring, grid=False, rot=45,)\n",
    "[ax_tmp.set_xlabel('') for ax_tmp in np.asarray(bp).reshape(-1)]\n",
    "fig = np.asarray(bp).reshape(-1)[0].get_figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks better, doesn't it? Let's try some different configurations all in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## material in parts from https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f\n",
    "\n",
    "representations = {}\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1), stop_words=None,\n",
    "                             tokenizer=word_tokenize, max_features=500)\n",
    "xtrain_countvect = vectorizer.fit_transform(trainingdata['txt'])\n",
    "representations['CountVectorizer'] = xtrain_countvect\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopWords)\n",
    "tfidf_vect.fit(trainingdata['txt'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(trainingdata['txt'])\n",
    "representations['TfidfVectorizer'] = xtrain_tfidf\n",
    "\n",
    "\n",
    "\n",
    "for representation, transformed_vector in representations.items():\n",
    "    classifier_models = [\n",
    "        RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "        LinearSVC(multi_class='ovr', C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
    "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
    "      penalty='l2', random_state=0, tol=1e-05, verbose=0),\n",
    "        MultinomialNB(),\n",
    "        #LogisticRegression(random_state=0),\n",
    "        SGDClassifier(),\n",
    "    ]\n",
    "    CV = 10\n",
    "    cv_df = pd.DataFrame(index=range(CV * len(classifier_models)))\n",
    "    score = 'f1_micro'\n",
    "    entries = []\n",
    "    for model in classifier_models:\n",
    "      model_name = model.__class__.__name__\n",
    "      accuracies = cross_val_score(model, transformed_vector, trainingdata['label'], scoring=score, cv=CV)\n",
    "      for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "    cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', score])\n",
    "\n",
    "    print('representation: ', representation)\n",
    "    bp = cv_df.boxplot(by='model_name', column=[score], grid=False, rot=45,)\n",
    "    [ax_tmp.set_xlabel('') for ax_tmp in np.asarray(bp).reshape(-1)]\n",
    "    fig = np.asarray(bp).reshape(-1)[0].get_figure()\n",
    "    fig.suptitle('Representation: '+representation)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What conclusions do you draw from this? Which classifier and which representation would you choose as your final model? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Your turn to build a classifier\n",
    "**Choose one classifier and one representation format and test it on the test data. What results do you get?** \n",
    "\n",
    "***Bonus question: What other configurations could you try before deciding on a final model? Is it appropriate to experiment with this on the test data? Why or why not?*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First step: Transform your training and test data to your chosen representation. \n",
    "\n",
    "## choose a representation: CountVectorizer or TfidfVectorizer. Do you want to add additional parameters to the vectorizer?\n",
    "\n",
    "chosen_vectorizer =\n",
    "\n",
    "## transform the training data \n",
    "transformed_training_data = chosen_vectorizer.fit_transform(trainingdata['txt'])\n",
    "\n",
    "## transform the test data\n",
    "transformed_test_data = chosen_vectorizer.transform(testdata['txt'])\n",
    "\n",
    "## Second step: Create a classifier - for instance the one you think gave best results when experimenting with cross-validation\n",
    "\n",
    "chosen_classifier = \n",
    "\n",
    "## train the classifier on the training data\n",
    "chosen_classifier.fit(fit_transformed_training_data, trainingdata['label'])\n",
    "## predict labels on the test data\n",
    "predicted = chosen_classifier.predict(transformed_test_data)\n",
    "## what results do you get?\n",
    "print(metrics.classification_report(testdata['label'], predicted, target_names=set(testdata['label'].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happens if you try to predict a label with a completely new text using your chosen trained classifier model? Does it seem to classify correctly?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = 'Patient with severe depression.'\n",
    "testX = chosen_vectorizer.transform([new_text])\n",
    "predicted = chosen_classifier.predict(testX)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = '5-year old girl with asthma.'\n",
    "testX = chosen_vectorizer.transform([new_text])\n",
    "predicted = chosen_classifier.predict(testX)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = 'Her pain is severe.'\n",
    "testX = chosen_vectorizer.transform([new_text])\n",
    "predicted = chosen_classifier.predict(testX)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus assignment\n",
    "\n",
    "**Write ten example sentences or paragraphs where you assign the correct label to each of them. Then pass them to the classifier and calculate precision, recall and f-score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_data = ## choose a representation where you have a sentence/paragraph and its 'gold' label\n",
    "testX = chosen_vectorizer.transform(#pass the new texts to the vectorizer)\n",
    "predicted = chosen_classifier.predict(testX)\n",
    "## compare the predicted labels with the gold labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
