{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QSLVKYpFLsr"
      },
      "source": [
        "# NLP classification - supervised learning\n",
        "\n",
        "In this example, you will learn how you can use supervised learning algorithms for NLP classification. We will use documents from mtsamples again. The task is to classify a document into its clinical specialty, e.g. pediatrics or hematology.\n",
        "\n",
        "We will use classification algorithms as implemented in sci-kit learn, and evaluate with cross-validation before testing on unseen test data.\n",
        "\n",
        "We will experiment with different ways of representing the documents for the classifiers.\n",
        "\n",
        "material in parts from https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f\n",
        "\n",
        "Written by Sumithra Velupillai, March 2019 - updated February 2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAf7_A_FFLss"
      },
      "source": [
        "# 1: Packages\n",
        "We will use a number of different packages for this exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9dsDjZ3FLst"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "try:\n",
        "    import xlrd\n",
        "except ImportError as e:\n",
        "    !pip install xlrd\n",
        "    import xlrd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import warnings; warnings.simplefilter('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_on5sqPFFLsu"
      },
      "outputs": [],
      "source": [
        "# We'll use scikit-learn for the classification algorithms.\n",
        "# https://scikit-learn.org/stable/\n",
        "\n",
        "#from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "#from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7L3YE4vzFLsu"
      },
      "outputs": [],
      "source": [
        "## sklearn also has some nice funtions for representations\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "## and for evaluation\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODSBKIZ-FLsu"
      },
      "outputs": [],
      "source": [
        "## Since we're working with text, we might need to tokenize for some of these representations. \n",
        "# We'll use nltk here, but there are other nlp packages available for this\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wodhl0huFLsu"
      },
      "outputs": [],
      "source": [
        "# You have also learnt about embedding representations. These can also be used for classification.\n",
        "# We will use a library called Zeugma, which allows using pre-trained embedding models\n",
        "#Zeugma library: https://github.com/nkthiebaut/zeugma\n",
        "\n",
        "try:\n",
        "    from zeugma.embeddings import EmbeddingTransformer\n",
        "except ImportError as e:\n",
        "    !pip install zeugma\n",
        "    !pip install theano\n",
        "    from zeugma.embeddings import EmbeddingTransformer\n",
        "\n",
        "from datetime import datetime\n",
        "print(datetime.now())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKc8HrRvFLsv"
      },
      "source": [
        "# 2: Corpus\n",
        "Read in the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ao--xPOsFLsv"
      },
      "outputs": [],
      "source": [
        "\n",
        "xlds_training = 'https://github.com/KCL-Health-NLP/nlp_examples/blob/master/classification/classification_trainingdata.xlsx?raw=true'\n",
        "trainingdata = pd.read_excel(xlds_training)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymTrWbm6FLsv"
      },
      "source": [
        "Take a look at the content of the training data. What are we trying to classify? What are the labels we want to try to learn? How many instances do we have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4WkM3l5FLsv"
      },
      "outputs": [],
      "source": [
        "trainingdata['label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_C5XbyDFLsw"
      },
      "source": [
        "What types of features do you think would be useful for the classification task? Where can we get them? Take a look at one or two of the documents. Can you guess which classification label these belong to?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c-Oq-KRFLsw"
      },
      "outputs": [],
      "source": [
        "trainingtxt_example = trainingdata['txt'].tolist()[0]\n",
        "print(trainingtxt_example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MExLPegGFLsw"
      },
      "outputs": [],
      "source": [
        "trainingtxt_example = trainingdata['txt'].tolist()[231]\n",
        "print(trainingtxt_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "646MrP1BFLsw"
      },
      "source": [
        "# 3: Representation - BoW\n",
        "\n",
        "The most common baseline feature representation for text classification tasks is to use the *bag-of-words* representation, in a document-term matrix. Let's build a simple one using raw counts and only keeping a maximum of 500 features. We can use the CountVectorizer function from sklearn, and tokenize using a function from nltk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BobArOMLFLsw"
      },
      "outputs": [],
      "source": [
        "first_vectorizer = CountVectorizer(ngram_range=(1,1), stop_words=None,\n",
        "                             tokenizer=word_tokenize, max_features=500)\n",
        "first_vectorizer.fit(trainingdata['txt'].tolist())\n",
        "first_fit_transformed_data = first_vectorizer.fit_transform(trainingdata['txt'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SSqN4gEFLsw"
      },
      "source": [
        "We can now look at this transformed representation for an example document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TOZgaNdFLsx"
      },
      "outputs": [],
      "source": [
        "first_transformed_data = first_vectorizer.transform([trainingdata['txt'].tolist()[231]])\n",
        "print (first_transformed_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQl9mGL7FLsx"
      },
      "source": [
        "What word is represented by the different indices? Have a look at a few examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vN1lywr9FLsx"
      },
      "outputs": [],
      "source": [
        "print (first_vectorizer.get_feature_names_out()[32])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iB428IjSFLsx"
      },
      "outputs": [],
      "source": [
        "print(first_fit_transformed_data.shape)\n",
        "print ('Amount of Non-Zero occurences: ', first_fit_transformed_data.nnz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udtv8IMpFLsx"
      },
      "source": [
        "# 4: Classification\n",
        "Let's build a classifier with this feature representation. In text classification, many classification algorithms have been shown to work well. Sci-kit learn has implementations for many different types of classification algorithms - have a look at their website!\n",
        "\n",
        "Let's try a K nearest neighbour classifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky4UmzoVFLsx"
      },
      "outputs": [],
      "source": [
        "kneighbour_classifier = KNeighborsClassifier().fit(first_fit_transformed_data, trainingdata['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j319_tJcFLsx"
      },
      "source": [
        "We now have a trained model. But how do we know how well it works? Let's evaluate it on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0wEaxcBFLsx"
      },
      "outputs": [],
      "source": [
        "\n",
        "xlds_test = 'https://github.com/KCL-Health-NLP/nlp_examples/blob/master/classification/classification_test_data.xlsx?raw=true'\n",
        "testdata = pd.read_excel(xlds_test)\n",
        "\n",
        "\n",
        "\n",
        "## We need to transform this data to the same representation\n",
        "first_fit_transformed_testdata = first_vectorizer.transform(testdata['txt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTcrDtSZFLsy"
      },
      "outputs": [],
      "source": [
        "first_fit_transformed_testdata\n",
        "kneighbour_predicted = kneighbour_classifier.predict(first_fit_transformed_testdata)\n",
        "kneighbour_predicted"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make a list of all the labels in our dataset to evaluate, and then run some standard evaluation metrics"
      ],
      "metadata": {
        "id": "mRGcsTkrjd5w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLXSlVpnFLsy"
      },
      "outputs": [],
      "source": [
        "labels = list(set(testdata['label']))\n",
        "print(metrics.classification_report(testdata['label'], kneighbour_predicted, target_names=labels))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPyd1hxdFLsy"
      },
      "source": [
        "What do you think about these results? There are probably ways of improving this, by changing the representation or maybe trying a different classifier model. \n",
        "__There is one main problem though: we can't use this test data to try different configurations! Why?__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M4_bqpIFLsy"
      },
      "source": [
        "# 5: N-fold cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ-o8TbrFLsy"
      },
      "source": [
        "We can employ n-fold cross-validation on the training data to experiment with different representations, parameters, and classifiers.\n",
        "\n",
        "There are also various metrics that can be used to evaluate classification results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2akkMilFLsy"
      },
      "outputs": [],
      "source": [
        "kneighbour_classifier = KNeighborsClassifier().fit(first_fit_transformed_data, trainingdata['label'])\n",
        "scoring = ['precision_macro', 'recall_macro','precision_micro','recall_micro', 'f1_micro', 'f1_macro']\n",
        "scores = cross_validate(kneighbour_classifier, first_fit_transformed_data, trainingdata['label'], scoring=scoring, cv=10, return_train_score=False)\n",
        "scoresdf = pd.DataFrame(scores)\n",
        "scoring = ['test_precision_macro', 'test_recall_macro','test_precision_micro','test_recall_micro', 'test_f1_micro', 'test_f1_macro']\n",
        "bp = scoresdf.boxplot(column=scoring, grid=False, rot=45,)\n",
        "[ax_tmp.set_xlabel('') for ax_tmp in np.asarray(bp).reshape(-1)]\n",
        "fig = np.asarray(bp).reshape(-1)[0].get_figure()\n",
        "fig.suptitle('K nearest neighbour, count vectorizer')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk7luyNfFLsy"
      },
      "source": [
        "What happens if we try another classifier? Let's try a random forest classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqlchQSSFLsy"
      },
      "outputs": [],
      "source": [
        "rf_classifier = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0).fit(first_fit_transformed_data, trainingdata['label'])\n",
        "scoring = ['precision_macro', 'recall_macro','precision_micro','recall_micro', 'f1_micro', 'f1_macro']\n",
        "scores = cross_validate(rf_classifier, first_fit_transformed_data, trainingdata['label'], scoring=scoring, cv=10, return_train_score=False)\n",
        "scoresdf = pd.DataFrame(scores)\n",
        "scoring = ['test_precision_macro', 'test_recall_macro','test_precision_micro','test_recall_micro', 'test_f1_micro', 'test_f1_macro']\n",
        "bp = scoresdf.boxplot(column=scoring, grid=False, rot=45,)\n",
        "[ax_tmp.set_xlabel('') for ax_tmp in np.asarray(bp).reshape(-1)]\n",
        "fig = np.asarray(bp).reshape(-1)[0].get_figure()\n",
        "fig.suptitle('Random forest, count vectorizer')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tHCIDNAFLsz"
      },
      "source": [
        "Was this better or worse? Are there any parameters worth changing?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq_j0UgjFLsz"
      },
      "source": [
        "# 6: Another representation model: Tf-idf\n",
        "We have used a very simple bag-of-words representation. What happens if we try something else? Let's try tf-idf. This is considered a strong baseline in many text classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6Pi1snbFLsz"
      },
      "outputs": [],
      "source": [
        "\n",
        "stopWords = list(stopwords.words('english'))\n",
        "tfidf_vect = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopWords)\n",
        "tfidf_vect.fit(trainingdata['txt'])\n",
        "second_fit_transformed_data =  tfidf_vect.transform(trainingdata['txt'])\n",
        "second_fit_transformed_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsE-xQ7jFLsz"
      },
      "source": [
        "What other parameters can you change in this representation? How does this look different from the CountVectorizer representation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28a8YRkuFLsz"
      },
      "source": [
        "Let's now use this with the KNN classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92RB9RcfFLsz"
      },
      "outputs": [],
      "source": [
        "kneighbour_classifier = KNeighborsClassifier().fit(second_fit_transformed_data, trainingdata['label'])\n",
        "scoring = ['precision_macro', 'recall_macro','precision_micro','recall_micro', 'f1_micro', 'f1_macro']\n",
        "scores = cross_validate(kneighbour_classifier, second_fit_transformed_data, trainingdata['label'], scoring=scoring, cv=10, return_train_score=False)\n",
        "scoresdf = pd.DataFrame(scores)\n",
        "scoring = ['test_precision_macro', 'test_recall_macro','test_precision_micro','test_recall_micro', 'test_f1_micro', 'test_f1_macro']\n",
        "bp = scoresdf.boxplot(column=scoring, grid=False, rot=45,)\n",
        "[ax_tmp.set_xlabel('') for ax_tmp in np.asarray(bp).reshape(-1)]\n",
        "fig = np.asarray(bp).reshape(-1)[0].get_figure()\n",
        "fig.suptitle('K nearest neighbour, tf-idf vectorizer')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhWZKYrTFLsz"
      },
      "source": [
        "This looks better, doesn't it? Why do you think this works better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsyFZ-1TFLsz"
      },
      "source": [
        "# 7: Representations: embeddings\n",
        "\n",
        "Last week, you learnt about embedding representations. What might be the benefit of using this type of representation instead of counts or tf-idf?\n",
        "\n",
        "As you saw, there are many pre-trained embedding models available online. Let's try using one of these on this data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bviZevdSFLs0"
      },
      "source": [
        "Zeugma is a package where you can use embeddings in sklearn.\n",
        "https://github.com/nkthiebaut/zeugma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrPbO7owFLs0"
      },
      "source": [
        "It allows you to directly download pre-trained models that have been released from the gensim website.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDljBiK_FLs0"
      },
      "source": [
        "Let's use a basic glove model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0KYsWpSFLs0"
      },
      "outputs": [],
      "source": [
        "glove = EmbeddingTransformer('glove')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mACpJeYEFLs0"
      },
      "source": [
        "We now need to transform our training data to map to this embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gehQivsOFLs0"
      },
      "outputs": [],
      "source": [
        "glove_transformed_training_data = glove.transform(trainingdata['txt'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9DTjCshFLs0"
      },
      "source": [
        "Have a look at what the data now looks like with this representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJk_-B7LFLs0"
      },
      "outputs": [],
      "source": [
        "glove_transformed_training_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HDeYTZjFLs0"
      },
      "source": [
        "Let's build a classifier with this representation and evaluate with 10-fold cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VF5k_JXlFLs1"
      },
      "outputs": [],
      "source": [
        "kneighbour_classifier = KNeighborsClassifier().fit(glove_transformed_training_data, trainingdata['label'])\n",
        "scoring = ['precision_macro', 'recall_macro','precision_micro','recall_micro', 'f1_micro', 'f1_macro']\n",
        "scores = cross_validate(kneighbour_classifier, glove_transformed_training_data, trainingdata['label'], scoring=scoring, cv=10, return_train_score=False)\n",
        "scoresdf = pd.DataFrame(scores)\n",
        "scoring = ['test_precision_macro', 'test_recall_macro','test_precision_micro','test_recall_micro', 'test_f1_micro', 'test_f1_macro']\n",
        "bp = scoresdf.boxplot(column=scoring, grid=False, rot=45,)\n",
        "[ax_tmp.set_xlabel('') for ax_tmp in np.asarray(bp).reshape(-1)]\n",
        "fig = np.asarray(bp).reshape(-1)[0].get_figure()\n",
        "fig.suptitle('K nearest neighbour, pre-trained embeddings')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVd871I_FLs1"
      },
      "source": [
        "What do you think about these results? What is happening here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqrFPFf1FLs1"
      },
      "source": [
        "There are a number of pre-trained embedding models that have been released openly, a full list of pretrained embeddings to experiment with using this library: https://github.com/RaRe-Technologies/gensim-data#models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss4fV-b-FLs1"
      },
      "source": [
        "You can play with these if you want, e.g.:\n",
        "glove = EmbeddingTransformer('glove-wiki-gigaword-50')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxj55nChFLs1"
      },
      "source": [
        "# 8: Classifiers, representations, evaluation\n",
        "\n",
        "You've now seen that you get very different results depending on which representation you use, which classifier, and also that there are many different metrics to analyse.\n",
        "\n",
        "Let's try some different configurations all in one go. We'll create a dictionary with the three different types of representations, and a list of different classification algorithms, and apply all these configurations to see what seems to yield best results according to a chosen evaluation metric using 10-fold cross validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqs2Ua0cFLs1"
      },
      "outputs": [],
      "source": [
        "\n",
        "representations = {}\n",
        "\n",
        "vectorizer = CountVectorizer(ngram_range=(1,1), stop_words=None,\n",
        "                             tokenizer=word_tokenize, max_features=500)\n",
        "xtrain_countvect = vectorizer.fit_transform(trainingdata['txt'])\n",
        "representations['CountVectorizer'] = xtrain_countvect\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopWords)\n",
        "tfidf_vect.fit(trainingdata['txt'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(trainingdata['txt'])\n",
        "representations['TfidfVectorizer'] = xtrain_tfidf\n",
        "\n",
        "x_train_glove = glove.transform(trainingdata['txt'])\n",
        "representations['pretrained_glove'] = x_train_glove\n",
        "\n",
        "\n",
        "\n",
        "CV = 10\n",
        "\n",
        "classifier_models = [\n",
        "        RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
        "        DecisionTreeClassifier(),\n",
        "        SVC(),\n",
        "        LinearSVC(multi_class='ovr', C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
        "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
        "      penalty='l2', random_state=0, tol=1e-05, verbose=0),\n",
        "        SGDClassifier(),\n",
        "        LogisticRegression(random_state=0),\n",
        "        KNeighborsClassifier(),\n",
        "]\n",
        "\n",
        "cv_df = pd.DataFrame(index=range(CV * (len(classifier_models)*len(representations))))\n",
        "entries = []\n",
        "\n",
        "\n",
        "for representation, transformed_vector in representations.items():\n",
        "    score = 'f1_micro'\n",
        "    for model in classifier_models:\n",
        "      model_name = model.__class__.__name__+'_'+representation\n",
        "      accuracies = cross_val_score(model, transformed_vector, trainingdata['label'], scoring=score, cv=CV)\n",
        "      for fold_idx, accuracy in enumerate(accuracies):\n",
        "        entries.append((model_name, fold_idx, accuracy))\n",
        "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', score])\n",
        "bp = cv_df.boxplot(by='model_name', column=[score], grid=False, rot=90, figsize=(12,8))\n",
        "[ax_tmp.set_xlabel('') for ax_tmp in np.asarray(bp).reshape(-1)]\n",
        "fig = np.asarray(bp).reshape(-1)[0].get_figure()\n",
        "fig.suptitle('10-fold cross validation results')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fwh9uZGFLs1"
      },
      "source": [
        "What conclusions do you draw from this? Which classifier and which representation would you choose as your final model? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRp5AQXoFLs2"
      },
      "source": [
        "# Assignment 1: Your turn to build a classifier\n",
        "\n",
        "Answer questions by placing your code in the code cell below, running it to provide output, and giving your written answers in this markdown cell\n",
        "\n",
        "**1.1 Choose one classifier and one representation format and test it on the blind test data. What do you think about your results, and how do they relate to the 10-fold cross validation results above?** \n",
        "\n",
        "**1.2 What other configurations could you try before deciding on a final model?**\n",
        "\n",
        "**1.3 Is it appropriate to experiment with this on the test data? Why or why not?** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VtudnBiFLs2"
      },
      "outputs": [],
      "source": [
        "## PUT YOUR CODE ANSWERS IN HERE, AND RUN IT TO GIVE OUTPUT\n",
        "## First step: Transform your training and test data to your chosen representation. \n",
        "\n",
        "## choose a representation: CountVectorizer or TfidfVectorizer, or embeddings\n",
        "\n",
        "\n",
        "## transform the training data \n",
        "# something like this: transformed_training_data = chosen_representation.transform(trainingdata['txt'])\n",
        "\n",
        "## transform the test data\n",
        "# something like this: transformed_test_data = chosen_representation.transform(testdata['txt'])\n",
        "\n",
        "## Second step: Create a classifier - the one you think gave best results when experimenting with cross-validation\n",
        "\n",
        "## train the classifier on the training data\n",
        "# e.g. chosen_classifier.fit(transformed_training_data, trainingdata['label'])\n",
        "\n",
        "## predict labels on the test data\n",
        "# e.g. predicted = chosen_classifier.predict(transformed_test_data)\n",
        "\n",
        "## what results do you get?\n",
        "# e.g. print(metrics.classification_report(testdata['label'], predicted, target_names=set(testdata['label'].tolist())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DohIbt3FLs2"
      },
      "source": [
        "# Assignment 2: use your classifier to predict labels on unseen text\n",
        "\n",
        "Answer questions by placing your code in the three code cells below, running it to provide output, and giving your written answers in this markdown cell\n",
        "\n",
        "**What happens if you try to predict a label with a completely new text using your chosen trained classifier model? Does it seem to classify correctly?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBJj0mwMFLs2"
      },
      "outputs": [],
      "source": [
        "new_text = 'Patient with severe depression.'\n",
        "\n",
        "## code will be something like this:\n",
        "# testX = chosen_representation.transform([new_text])\n",
        "# predicted = chosen_classifier.predict(testX)\n",
        "# print(predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZM7c1C2FLs2"
      },
      "outputs": [],
      "source": [
        "new_text = '5-year old girl with asthma.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOnm31H1FLs2"
      },
      "outputs": [],
      "source": [
        "new_text = 'Her pain is severe.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbKaH8iBFLs2"
      },
      "source": [
        "# Assignment 3: Evaluation\n",
        "\n",
        "**Write ten example sentences or paragraphs where you assign the correct label to each of them. Then pass them to the classifier and calculate precision, recall and f-score, by completing the skeleton code in the cell below, and running it to give output. Analyse and discuss the results, giving your answer in this markdown cell.**\n",
        "\n",
        "***Gold label values: psychiatrypsychology, hematology, pain, pediatrics***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWKCgWsHFLs2"
      },
      "outputs": [],
      "source": [
        "new_text = []\n",
        "new_gold_labels = ['pain', 'pain', 'pediatrics', 'hematology', 'psychiatrypsychology', 'pediatrics', 'psychiatrypsychology', 'hematology', 'pain', 'psychiatrypsychology']\n",
        "testX = chosen_representation.transform(new_text)\n",
        "predicted = chosen_classifier.predict(testX)\n",
        "## compare the predicted labels with the gold labels. HINT: metrics.classification_report is useful here\n",
        "print(predicted)\n",
        "\n",
        "print(metrics.classification_report(new_gold_labels, predicted, target_names=set((new_gold_labels))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7waZm1-ZFLs2"
      },
      "source": [
        "\n",
        "You've now experimented with different respresentations for text classification, different classification algorithms, and experimental setups.\n",
        "\n",
        "There are many other alternative approaches available today.\n",
        "\n",
        "\n",
        "For instance, there are pre-trained biomedical embeddings, trained on scientific literature, one example: https://www.aclweb.org/anthology/W16-2922.pdf\n",
        "https://github.com/cambridgeltl/BioNLP-2016\n",
        "\n",
        "There are also many online resources with additional examples and ideas, here's one example: https://towardsdatascience.com/text-classification-in-python-dd95d264c802"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbYUZZWTFLs3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}