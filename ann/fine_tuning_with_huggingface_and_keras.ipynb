{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyOur1B7QbWCN2ZaJbpN+bP9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KCL-Health-NLP/nlp_examples/blob/master/ann/fine_tuning_with_huggingface_and_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BN1esalPBbz"
      },
      "outputs": [],
      "source": [
        "# Hugging Face transformers and datasets installation\n",
        "!pip install transformers datasets evaluate\n",
        "!pip install git+https://github.com/huggingface/accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Restart***"
      ],
      "metadata": {
        "id": "Nx7pVsFp_tRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from datasets import load_dataset\n",
        "from datasets import ClassLabel\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "#from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import evaluate\n",
        "\n",
        "# For displaying models\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.utils import model_to_dot\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import SVG"
      ],
      "metadata": {
        "id": "AXvP3IQbTGnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ],
      "metadata": {
        "id": "JC3hBD3XQsSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#dataset = load_dataset(\"glue\", \"cola\")\n",
        "#dataset = dataset[\"train\"]  # Just take the training split for now"
      ],
      "metadata": {
        "id": "qLdjvkK5SdVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenise"
      ],
      "metadata": {
        "id": "gMSS8zWQQufh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reduce size of dataset to speed up"
      ],
      "metadata": {
        "id": "LFYMOg3LQ5uO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "#small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
      ],
      "metadata": {
        "id": "d6Xv66u0Q9CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from transformers import AutoTokenizer\n",
        "#\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "#\n",
        "#tokenized_data = tokenizer(dataset[\"sentence\"], return_tensors=\"np\", padding=True, truncation=True, max_length=400)\n",
        "#\n",
        "# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n",
        "#tokenized_data = dict(tokenized_data)\n",
        "#\n",
        "#labels = np.array(dataset[\"label\"])  # Label is already an array of 0 and 1"
      ],
      "metadata": {
        "id": "4pl5PnbUSynM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from transformers import TFAutoModelForSequenceClassification\n",
        "#from tensorflow.keras.optimizers import Adam\n",
        "#\n",
        "# Load and compile our model\n",
        "#model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
        "# Lower learning rates are often better for fine-tuning transformers\n",
        "#model.compile(optimizer=Adam(3e-5))\n",
        "#\n",
        "#model.fit(tokenized_data, labels)"
      ],
      "metadata": {
        "id": "balpSaLmTRCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train_url='https://github.com/KCL-Health-NLP/nlp_examples/raw/master/classification/classification_trainingdata.csv'\n",
        "ds = load_dataset(\"csv\", data_files=train_url, column_names=['label', 'text'], split='train')\n",
        "ds"
      ],
      "metadata": {
        "id": "LUpEc1gFYO_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = ds.train_test_split(test_size=0.2)\n",
        "ds"
      ],
      "metadata": {
        "id": "hlfKcCYcdZ7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "classlabels = ClassLabel(num_classes=4, names=['psychiatrypsychology', 'hematology', 'pediatrics', 'pain'])\n",
        "\n",
        "def tokenize(batch):\n",
        "    text = tokenizer(batch['text'], return_tensors='np', padding=True, truncation=True, max_length=128)\n",
        "    labels = [classlabels.str2int(l) for l in batch['label']]\n",
        "    return (dict(text), np.array(labels))\n",
        "\n"
      ],
      "metadata": {
        "id": "1AttjSEN5jCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pytorch\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "#\n",
        "#labels = ClassLabel(num_classes=4, names=['psychiatrypsychology', 'hematology', 'pediatrics', 'pain'])\n",
        "#\n",
        "#def tokenize(batch):\n",
        "#    tok_batch = tokenizer(batch['text'], padding=True, truncation=True, max_length=128)\n",
        "#    tok_batch['label'] = [labels.str2int(l) for l in batch['label']]\n",
        "#    return tok_batch\n"
      ],
      "metadata": {
        "id": "ES6Qbv7pBcAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_x, train_y = tokenize(ds['train'])\n",
        "val_x, val_y =  tokenize(ds['test'])\n",
        "#dev_tok = tokenize(ds['test'])\n",
        "print(train_y)\n",
        "print('\\n'*4)\n",
        "print(train_x)"
      ],
      "metadata": {
        "id": "zqoP49pT61tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# pytorch\n",
        "#model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=4)"
      ],
      "metadata": {
        "id": "r_ylvYwf8JVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch\n",
        "#training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
      ],
      "metadata": {
        "id": "9URnOVCT8ZlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch\n",
        "#metric = evaluate.load('accuracy')\n",
        "#\n",
        "#def compute_metrics(eval_pred):\n",
        "#    logits, labels = eval_pred\n",
        "#    predictions = np.argmax(logits, axis=-1)\n",
        "#    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X01xNWn8-lJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pytorch\n",
        "#trainer = Trainer(\n",
        "#    model=model,\n",
        "#    args=training_args,\n",
        "#    train_dataset=train_tok,\n",
        "#    eval_dataset=dev_tok,\n",
        "#    compute_metrics=compute_metrics,\n",
        "#)"
      ],
      "metadata": {
        "id": "eiIKabi6-_TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch\n",
        "#trainer.train()"
      ],
      "metadata": {
        "id": "FiDvtS3V_j8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from transformers import AutoTokenizer\n",
        "#\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "#\n",
        "#def tokenize_function(examples):\n",
        "#    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "#\n",
        "#\n",
        "#tokenized_ds = ds.map(tokenize_function, batched=True)\n",
        "#tokenized_ds\n",
        "\n",
        "\n",
        "#from transformers import AutoTokenizer\n",
        "#\n",
        "#tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "#\n",
        "#tok_train = tokenizer(ds['train']['text'], return_tensors='np', padding=True, truncation=True, max_length=400)\n",
        "#\n",
        "# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n",
        "#tok_train = dict(tok_train)\n",
        "#\n",
        "#labels_train = np.array(ds['train']['label'])  # Label is already an array of 0 and 1\n",
        "#\n",
        "#print(tok_train)\n",
        "#print()\n",
        "#print(labels_train)\n"
      ],
      "metadata": {
        "id": "cBtldM1hpzB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and compile our model\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "# Lower learning rates are often better for fine-tuning transformers\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=Adam(3e-5), metrics=[\"accuracy\"])\n",
        "#model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "nvfjNOLQt7Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "history = model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=5)"
      ],
      "metadata": {
        "id": "nCmXvDHNz7nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ogbzicoJGKfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_url='https://github.com/KCL-Health-NLP/nlp_examples/raw/master/classification/classification_test_data.csv'\n",
        "test_ds = load_dataset(\"csv\", data_files=test_url, column_names=['label', 'text'])\n",
        "test_ds"
      ],
      "metadata": {
        "id": "Hd_zfjCWHouT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_x, test_y = tokenize(test_ds['train'])\n",
        "print(test_y)\n",
        "print('\\n'*4)\n",
        "print(test_x)"
      ],
      "metadata": {
        "id": "tUUanxPOH-gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(test_x, test_y)\n",
        "print(f\"{'Test loss:':16}{score[0]:.2f}\")\n",
        "print(f\"{'Test accuracy:':16}{score[1]:.2f}\")"
      ],
      "metadata": {
        "id": "WnGwSyfRI7BL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}