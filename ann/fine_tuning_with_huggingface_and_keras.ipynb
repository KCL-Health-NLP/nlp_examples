{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyMQrkb1tUwqtWacxGE97t+W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KCL-Health-NLP/nlp_examples/blob/master/ann/fine_tuning_with_huggingface_and_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning a Hugging Face model with Keras\n",
        "\n",
        "**This is the \"question\" version of this notebook. It differs from the \"answer\" version in that it is not complete. You are asked to do the training and evaluate your model. This builds on basic Keras use, covered in previous notebooks**\n",
        "\n",
        "Based on this [Hugging Face tutorial](https://huggingface.co/docs/transformers/training)\n",
        "\n"
      ],
      "metadata": {
        "id": "6P_cFMtRUY-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine tuning\n",
        "\n",
        "In fine-tuning, we take an existing language model that has been pre-trained on some task, and train that model for some other, usually related, task. Pre-training followed by fine-tuning is a kind of transfer learning - learning knowledge from one task, and applying it to another.\n",
        "\n",
        "Often, pre-training will:\n",
        "* be unsupervised, e.g. predicting masked words in a corpus of sentences\n",
        "* include large amounts of training data, to ensure generalisability\n",
        "* learn a large number of parameters, to ensure a rich representation\n",
        "\n",
        "We will fine tune the popular [BERT Base](https://aclanthology.org/N19-1423.pdf) model. The original BERT Base model was trained on 11000 books and the whole of wikipedia, and has 110 million parameters.\n",
        "\n",
        "BERT base was trained on two tasks:\n",
        "* a masked language task - given a sequence with masked words, predict the missing masked words\n",
        "* given two sentences, predict whether they follow on from each other in the original data\n",
        "\n",
        "It is especially useful in tasks that use long sequences of data, e.g. text classification. The practical will fine tune BERT base to classify IMDb movie reviews, using the [Keras library](https://keras.io/) and the [Hugging Face transformers library](https://huggingface.co/docs/transformers/index).\n",
        "\n",
        "The notebook steps through the basics of setting up fine-tuning, and then asks you to complete model training and evaluaiton, using the classes and methods introduced in previous notebooks on Keras."
      ],
      "metadata": {
        "id": "R9mwesHv3dRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hugging Face\n",
        "\n",
        "[Hugging Face](https://huggingface.co/) is company that provides:\n",
        "\n",
        "* a very popular machine learning library\n",
        "* a repository for sharing trained models and datasets\n",
        "\n",
        "The Hugging Face library is especially noted for its transformer support. It has a very high level API, with just a few simple lines of code needed to perform many deep learning tasks. By default, it uses the [PyTorch tensor library](https://pytorch.org/), but there is also support for Keras / TensorFlow and for other tensor libraries.\n",
        "\n",
        "We will load the pre-trained BERT Base model from Hugging Face. We will use a version trained on case sensitive text (i.e. it includes both capital and lower case characters. It is worth reading the full model description on the [BERT Base cased model card](https://huggingface.co/bert-base-cased) in the Hugging Face repository.\n",
        "\n",
        "**Hugging Face and Keras:** We will use the Hugging Face transformers library to load the pre-trained model in to a specific TensorFlow model class. This means that we can use the same Keras methods and classes as in previous notebooks (e.g. compiling, saving history, evaluating).\n"
      ],
      "metadata": {
        "id": "BJTY3sKm3fcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using with GPUs\n",
        "\n",
        "The execution time of this code will benefit from the use of GPUs. To select a GPU runtime in colab:\n",
        "\n",
        "* Select the *Runtime* menu\n",
        "* Select the *Change runtime type* submenu\n",
        "* In the dialog that appears, under *Hardware accelerator* select *GPU*\n",
        "* Your existing runtime will disconnect, and you will be allocated and connected to a new GPU runtime."
      ],
      "metadata": {
        "id": "8FPD2qnq2oAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install packages\n",
        "We need to install several Hugging Face libraries, which are not provided by default in Colab."
      ],
      "metadata": {
        "id": "L2rUU_ND21u8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BN1esalPBbz"
      },
      "outputs": [],
      "source": [
        "# Install Hugging Face transformers anddatasets libraries\n",
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Restart your runtime***\n",
        "**You need to restart your runtime in order for the above packages to be made available for imports**\n",
        "\n",
        "* Menus\n",
        "  * Runtime\n",
        "    * Restart runtime"
      ],
      "metadata": {
        "id": "Nx7pVsFp_tRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "XR-qPnVT31ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hugging Face datasets library has facilities for\n",
        "# loading datasets direct from the Hugging Face\n",
        "# datatsets repository.\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Many pre-trained transformer models have their own\n",
        "# specific tokenisation\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# The TensorFlow / Keras version of a Hugging Face\n",
        "# sequence classifier\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "# For use in our final Keras model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# For displaying models\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.utils import model_to_dot\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import SVG"
      ],
      "metadata": {
        "id": "AXvP3IQbTGnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data\n",
        "\n",
        "As in the previous notebooks, we will use the IMDb review dataset. In this, movie reviews are labelles as either positive sentiment (label 1), or negative sentiment (label 0).\n",
        "\n",
        "A Hugging Face dataset object can hold multiple subsets (e.g. training, testing), and has methods for accessing different portions, splitting, shuffling, etc. We will load the IMDb text dataset from the Hugging Face repository.\n",
        "\n",
        "Take a look at the object created."
      ],
      "metadata": {
        "id": "JC3hBD3XQsSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "ds_imdb = load_dataset(\"imdb\")\n",
        "\n",
        "# Let's take a look at it\n",
        "ds_imdb"
      ],
      "metadata": {
        "id": "UtcvpJm9OH3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reduce size of dataset to speed up\n",
        "\n",
        "Transformers can be slow to train, even when fine-tuning for simple tasks. We will therefore fine-tune on a portion of IMDb. Run the code below and make sure you understand:\n",
        "\n",
        "* the size of this new dataset and it's subsets\n",
        "* the difference between the test subset in this new dataset, and the test subset in the above dataset"
      ],
      "metadata": {
        "id": "LFYMOg3LQ5uO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a small training dataset from the 'train' part of IMDb.\n",
        "# Shuffle the dataset using a random seed, and then select the\n",
        "# first 600 reviews.\n",
        "ds_train_sm = ds_imdb['train'].shuffle(seed=42).select(range(600))\n",
        "\n",
        "# Create a train / test split.\n",
        "ds_train_sm = ds_train_sm.train_test_split(test_size=0.2)\n",
        "\n",
        "# Let's take a look\n",
        "ds_train_sm"
      ],
      "metadata": {
        "id": "iAfPJwhSOSUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenise\n",
        "\n",
        "The BERT models have been built with their own specific method of tokenisation, WordPiece tokenisation. This means that we have to tokenise our text with exactly the same method.\n",
        "\n",
        "This is trained by starting with a vocabulary of tokens consisting of every character in the dataset, and then iteratively merging frequent combinations of these character tokens. The end result is tokenisation that splits words in to fragments, or pieces. It is especially useful when dealing with unseen words. Hugging Face has a good [explanation of WordPiece tokenisation](https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt).\n",
        "\n",
        "We start by loading the BERT Base cased pretrained tokenisation model form Hugging Face. Hugging Face has a tokenizer class that will detect the type of tokenizer to build from the model being loaded."
      ],
      "metadata": {
        "id": "gMSS8zWQQufh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "lCS9aRVCHTRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try it out on some text."
      ],
      "metadata": {
        "id": "Syp7ncG8qcGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer.encode('the quick brown fox jumped over the lazy dog')\n",
        "print(encoding)"
      ],
      "metadata": {
        "id": "a1AldDtBqftA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We had 9 words, but we get 11 integers. Why? Let's convert them back to see what tokens they represent."
      ],
      "metadata": {
        "id": "vsOP1pHgqqeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.convert_ids_to_tokens([101, 1103, 3613, 3058, 17594, 4874, 1166, 1103, 16688, 3676, 102]))"
      ],
      "metadata": {
        "id": "0lAouJAWq-CT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two extra tokens have been added, ```[CLS]``` to represent the class of a sequence, and ```[SEP]``` to separate this sequence from some output sequence.\n",
        "\n",
        "Let's look at some others:"
      ],
      "metadata": {
        "id": "lsC_gNgArFpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.convert_ids_to_tokens([101,102,103,904,905,2006,2007,3008,'12807']))"
      ],
      "metadata": {
        "id": "KF3pzmIkqaMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a mix of special tokens, single characters that have never been merged, whole words and part words.\n",
        "\n",
        "What happens if we try to tokenise some made up words?"
      ],
      "metadata": {
        "id": "0ocVV2KXr6cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer.encode('elephere')\n",
        "print(encoding)\n",
        "encoding = tokenizer.encode('protoshere')\n",
        "print(encoding)"
      ],
      "metadata": {
        "id": "iXqYHO7LTpK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you make out the individual tokens in the integer? Try converting them back to tokens:"
      ],
      "metadata": {
        "id": "VfSHrnphsJs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.convert_ids_to_tokens([101, 8468, 8043, 12807, 102]))"
      ],
      "metadata": {
        "id": "1_7NsBGUUfKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will define a simple tokenisation function that takes a dataset of text and labels as input, takes out the text part of it, and returns the tokenised text and the labels.\n",
        "\n",
        "Remember that we want to pass this to a Keras / TensorFlow model. By default, Hugging Face tokenisers will return PyTorch tensors. So we need to convert these to numpy arrays, which TensorFlow uses. Additionally, Keras will expect the text to be provided as a dictionary of arrays, so we need to do that conversion too."
      ],
      "metadata": {
        "id": "F0SdKDPtsZNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenise a dataset\n",
        "def tokenize(batch):\n",
        "\n",
        "    # Do the tokenisation.\n",
        "    # --- Truncate and pad length to 120.\n",
        "    # --- Return the tensors as numpy arrays.\n",
        "    text = tokenizer(batch['text'], return_tensors='np', padding=True, truncation=True, max_length=128)\n",
        "\n",
        "    # return a tuple of the text array cast to a dictionary\n",
        "    # and the labels as a numpy array.\n",
        "    return (dict(text), np.array(batch['label']))"
      ],
      "metadata": {
        "id": "P3EMlMwoOoRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can tokenize our training and validaiton sets. We will name them as follows:\n",
        "\n",
        "* ```train_``` - training data\n",
        "* ```val_``` - validation data\n",
        "* ```_x``` - input text features\n",
        "* ```_y``` - labels\n",
        "\n",
        "We will print out an example. Take a look - can you see what the tokenisation has done, in addition to creating token vectors?\n",
        "\n",
        "* attention mask - used to mask / select which tokens to consider, e.g. if we have padded with zeros, we might want to ignore our zero tokens\n",
        "* token type ids - used to mask / select input and output sequences, if we have both. 0 selects input, 1 selects output."
      ],
      "metadata": {
        "id": "zyyOAy2Rt0N7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenise our datasets and print some out\n",
        "train_x, train_y = tokenize(ds_train_sm['train'])\n",
        "val_x, val_y =  tokenize(ds_train_sm['test'])\n",
        "print(train_y)\n",
        "print('\\n'*4)\n",
        "print(train_x)"
      ],
      "metadata": {
        "id": "OVgb7d5UOth5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the model\n",
        "\n",
        "We will now create our model. We will use a ```TFAutoModelForSequenceClassification``` which creates a TensorFlow sequence classification model.\n",
        "\n",
        "Like the tokeniser, this is a [Hugging Face AutoModel](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html) which will load a named pre-trained model from the Hugging Face repository, and detect exactly what class to instantiate. As we are loading a BERT model, it will create a transformer."
      ],
      "metadata": {
        "id": "D92yMvimV3Rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and compile our model - this is a Keras model\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "_EefeoFMQhwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise\n",
        "\n",
        "The rest of the code - compiling and evaluating - is for you to complete, using the methods and classes from previous notebooks, and the Keras documentation. Links to documentation are provided."
      ],
      "metadata": {
        "id": "LUOT16PwxJxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compile the model\n",
        "\n",
        "* Use the [Keras compile method](https://keras.io/api/models/model_training_apis/) with these arguments:\n",
        "* [Adam optimiser](https://keras.io/api/optimizers/adam/) with a learning rate of 3e-5 (lower learning rates are often better for fine-tuning transformers)\n",
        "* evaluating against the accuracy metric during training and validation "
      ],
      "metadata": {
        "id": "VJs9UD-ZWFIW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mxVlrKOaWzGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n",
        "Fit the model to the training data, validating against the validation data, over 10 epochs.\n",
        "\n",
        "In previous notebooks, we provided our data as tuples of text and labels. Here, we have it split up in to separate objects. You will need to combine, or pass it to the ```fit``` method as different arguments. Take a look at the [documentation](https://keras.io/api/models/model_training_apis/#fit-method) if you are not sure what to do."
      ],
      "metadata": {
        "id": "Ej-zIZUv4fjI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "atw59lSGQst-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display training summary\n",
        "\n",
        "* Plot the training history\n",
        "* Can you explain the plots?\n",
        "* What will your next step be?"
      ],
      "metadata": {
        "id": "hOvI2--94i1L"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MXq0Z903RQrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate\n",
        "\n",
        "* Evaluate against the IMDb test corpus (not the validation one you split out earlier, which was also referred to as ```test``` in some data objects)\n",
        "* You might want to shuffle and select a portion of the data to evaluate against."
      ],
      "metadata": {
        "id": "BiDtinN64oLb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cfGwOeYlRcyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EfTVTlcBRtLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bhgL4wlASDjc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}