{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Credit: Leon Derczynski, IT University of Copenhagen)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a little perceptron! It'll be on its own, which means it can only really do linearly separable problems. But that's OK; it'll try as hard as it can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll set our version of Python so other coders (and shell interpreters) can see what we're doing, and import two handy things: a random numbers module; and some extensions that help with many different kinds of numerical math. You might even say mathS, in fact. Together they're called numpy, pronounced Numb Pie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from random import choice \n",
    "from numpy import array, dot, random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define our training data. The format is thus, for each example:\n",
    "* an array containng the two input features, often together called *X*, followed with a bias value, which'll be 1\n",
    "* the output label, *y*\n",
    "\n",
    "(Some other ways of doing this hide the bias away. But then we might forget that there's a bias value there at all, which would be less instructive.)\n",
    "\n",
    "In our first case, we will model a boolean function, OR. In the OR function, if either one of the two inputs is 1 (true), then the output will also be 1 (true):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [ \n",
    "\t(array([0,0,1]), 0), \n",
    "\t(array([0,1,1]), 0), \n",
    "\t(array([1,0,1]), 0), \n",
    "\t(array([1,1,1]), 1),\n",
    "\t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll set up our activation function. The function itself is a simple one line function, otherwise known as a lambda function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_step = lambda x: 0 if x < 0 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally come the hyperparameters, and a little list where we'll keep track of how well training has gone. Oh, and while we're at it, let's initialise the weights too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100              # the number of training iterations, which we call epochs\n",
    "eta = 0.8            # the learning rate - scales how much we update on each iteration\n",
    "errors = []          # an array to store the errors in\n",
    "w = random.rand(3)   # an array of initial weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So next, we have the training process. For each epoch, we randomly select a training example. With that example,we work out the [dot product](https://www.mathsisfun.com/algebra/vectors-dot-product.html) of the features and our current weights. This gives us the activation potential - how much our neuron is trying to fire.\n",
    "\n",
    "Next, we put this through our activation function to see what our neuron really does, and compare that to what the answer should be, for this example. The difference is our error; how far wrong were we? We'll store that error so we can view them later.\n",
    "\n",
    "In the mean time, we'll update our weights, so they become closer to where they should have been. i.e. we try to reduce the error to zero. The learning rate scales how big that update is. Here's the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n): \n",
    "\tx, expected = choice(training_data) \n",
    "\tresult = dot(w, x) \n",
    "\terror = expected - unit_step(result) \n",
    "\terrors.append(error) \n",
    "\tw += eta * error * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how did we do? Let's go through the examples in the training set, and fire our weighted perceptron - using the learning weights, $w$ - for each eaxmple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]: -2.5430786428224437 -> 0\n",
      "[0 1]: -1.1095042447796468 -> 0\n",
      "[1 0]: -0.7818046198789497 -> 0\n",
      "[1 1]: 0.6517697781638472 -> 1\n"
     ]
    }
   ],
   "source": [
    "for x, _ in training_data:\n",
    "\tresult = dot(x, w)\n",
    "\tprint(\"{}: {} -> {}\".format(x[:2], result, unit_step(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it look? Did we nail it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's print a graph of those errors, to see how the process went."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2060cafb00>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib\n",
    "from pylab import plot, ylim \n",
    "ylim([-1,1]) \n",
    "plot(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises for you to try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try adapting the training data array to model these functions, running again for each one:\n",
    "* AND  - output is 1 if and only if both inputs are 1\n",
    "* NAND -  output is 1 if and only if both inputs are 0 (not and)\n",
    "* XOR  -  ouput is 1 if only one of the inputs is 1, not if both or none are 1 (exclusive or)\n",
    "\n",
    "What did you find? Did they all work? If not, why not? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
