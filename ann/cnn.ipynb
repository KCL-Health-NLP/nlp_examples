{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KCL-Health-NLP/nlp_examples/blob/master/ann/cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSBv4-xQTDC-"
      },
      "source": [
        "## A simple CNN for text classification\n",
        "\n",
        "Based on an [example from the Keras team](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/nlp/ipynb/text_classification_from_scratch.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0lwH-FmTDDD"
      },
      "source": [
        "\n",
        "\n",
        "This notebook uses a popular neural network API, [Keras](https://keras.io/), to build a simple CNN classifer, and runs it over movie reviews from [IMDb - the Internet Movie Database](https://www.imdb.com/). These reviews are available as a pre-prepared dataset that can be downloaded by the Keras distribution. The dataset is also available from [here](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
        "\n",
        "The dataset is constructed from very polarised reviews, and has been used in text classification evaluations for several years.\n",
        "\n",
        "Here's an example positive review:\n",
        "\n",
        "> I went to an advance screening of this movie thinking I was about to embark on 120 minutes of cheezy lines, mindless plot, and the kind of nauseous acting that made \"The Postman\" one of the most malignant displays of cinematic blundering of our time. But I was shocked. Shocked to find a film starring Costner that appealed to the soul of the audience. Shocked that Ashton Kutcher could act in such a serious role. Shocked that a film starring both actually engaged and captured my own emotions. Not since 'Robin Hood' have I seen this Costner: full of depth and complex emotion. Kutcher seems to have tweaked the serious acting he played with in \"Butterfly Effect\". These two actors came into this film with a serious, focused attitude that shone through in what I thought was one of the best films I've seen this year. No, its not an Oscar worthy movie. It's not an epic, or a profound social commentary film. Rather, its a story about a simple topic, illuminated in a way that brings that audience to a higher level of empathy than thought possible. That's what I think good film-making is and I for one am throughly impressed by this work. Bravo!\n",
        "\n",
        "And here's a negative review example:\n",
        "\n",
        "> It hurt to watch this movie, it really did... I wanted to like it, even going in. Shot obviously for very little cash, I looked past and told myself to appreciate the inspiration. Unfortunately, although I did appreciate the film on that level, the acting and editing was terrible, and the last 25-30 minutes were severe thumb-twiddling territory. A 95 minute film should not drag. The ratings for this one are good so far, but I fear that the friends and family might have had a say in that one. What was with those transitions? Dear Mr. Editor, did you just purchase your first copy of Adobe Premiere and make it your main goal to use all the goofy transitions that come with that silly program? Anyway... some better actors, a little more passion, and some more appealing editing and this makes a decent movie.\n",
        "\n",
        "\n",
        "### A note on performance\n",
        "From the original code comments: This example demonstrates the use of Convolution1D for text classification. It gets to 0.89 test accuracy after 2 epochs. Speed:\n",
        "* 90s/epoch on Intel i5 2.4Ghz CPU.\n",
        "* 10s/epoch on Tesla K40 GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrY427aTTDDE"
      },
      "source": [
        "### Packages\n",
        "\n",
        "First, the import - you will need keras, but even though it is not needed as an import, you will also need a neural net backend installed for Keras to use, i.e. Tensorflow. Make sure you have this available, and make sure it is compatible with the version of Keras you are using. If you use the latest Keras and the latest Tensroflow, you should be ok. \n",
        "\n",
        "Note if running locally: in order for the visualisation to work, you will need to have pydot and graphviz installed, e.g. \n",
        "\n",
        "```sudo apt-get install graphviz\n",
        "pip3 install pydot```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tk_4g6X6TDDH"
      },
      "outputs": [],
      "source": [
        "from keras.utils.data_utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from keras.datasets import imdb\n",
        "\n",
        "# For displaying\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For processing example texts into one-hot vectors\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing import text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9Cr9KKiTDDN"
      },
      "source": [
        "### Parameters\n",
        "\n",
        "Now let's set up some parameters, such as number of features, embedding dimensions, batch size, epocchs etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnprWe3ETDDO"
      },
      "outputs": [],
      "source": [
        "max_features = 5000\n",
        "maxlen = 400\n",
        "batch_size = 32\n",
        "embedding_dims = 50\n",
        "filters = 250\n",
        "kernel_size = 3\n",
        "hidden_dims = 250\n",
        "epochs = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1hXqqEeTDDR"
      },
      "source": [
        "### Data\n",
        "\n",
        "Let's load the data, and pad it out so all are the same length. In the data each review is labelled with an integer value of either 0 (negative review), or 1 (a positive review)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egtxVdtoTDDS"
      },
      "outputs": [],
      "source": [
        "# We load our training examples in to x_train, and their lables in to y_train\n",
        "# We also have somne test data (which we will use in development), in x_test\n",
        "# and y_test\n",
        "print('Loading data...\\n')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print('\\nData loaded.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How much data do we have? What does it look like? Write some code below to take a look."
      ],
      "metadata": {
        "id": "l_-x2HjS0WVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write code here to see how many examples we have in each dataset,\n",
        "# and to see what the examples and labels look like"
      ],
      "metadata": {
        "id": "esibxNJ00Ysf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why does the data look like this? What is going on? If you are not sure, take a look at the [Keras imdb dataset documentation](https://keras.io/api/datasets/imdb/)."
      ],
      "metadata": {
        "id": "lCS4L4CI1GLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will pad our data so that eacb example is the same lenght for our CNN."
      ],
      "metadata": {
        "id": "4cm3OOnb1dbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Pad sequences (samples x time)')\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n"
      ],
      "metadata": {
        "id": "aJCIImjh0V2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write some code to take a look at the padded data."
      ],
      "metadata": {
        "id": "n-n_m-Qr1oX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write code here to look at the padded data"
      ],
      "metadata": {
        "id": "_B6TVL8f1sWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMtzPSdLTDDV"
      },
      "source": [
        "## Building the model\n",
        "\n",
        "Next we build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkcFB_B-TDDV"
      },
      "outputs": [],
      "source": [
        "print('Build model...\\n')\n",
        "model = Sequential()\n",
        "\n",
        "# we start off with an efficient embedding layer which maps\n",
        "# our vocab indices into embedding_dims dimensions\n",
        "model.add(Embedding(max_features,\n",
        "                    embedding_dims,\n",
        "                    input_length=maxlen))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# we add a Convolution1D, which will learn filters\n",
        "# word group filters of size filter_length:\n",
        "model.add(Conv1D(filters,\n",
        "                 kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "# we use max pooling:\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "model.add(Dense(hidden_dims))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('Finished building model.\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWL9L1kRTDDY"
      },
      "source": [
        "## Take a look at the model\n",
        "\n",
        "Keras can print out textual and graphical representations of a model, that tells us:\n",
        "\n",
        "* The layers in the model, in the order in which they appear in the model\n",
        "* The output shape - i.e. the size of the matrices passed between layers. In some layers, the final dimension will be the number of units, in CNN laters, it will be the number of filters.\n",
        "* Parameters - this is the number of weights in each layer\n",
        "\n",
        "Let's take a look at our model...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vz5FwTN_TDDZ"
      },
      "outputs": [],
      "source": [
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFG1RFrtTDDb"
      },
      "source": [
        "We can also visualise this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH7vLp-7TDDc"
      },
      "outputs": [],
      "source": [
        "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxSy-C1KTDDe"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Now let's train it. Keras will validate against our test data, showing us loss and accuracy as it goes. We will save our metrics so we can display them afterwards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNGKyFTLTDDf"
      },
      "outputs": [],
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op0gE-PwTDDi"
      },
      "source": [
        "## Visualise the training process\n",
        "\n",
        "OK, but how did that change over time?\n",
        "(Thanks to [Jason Brownlee](https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/) for this next bit of code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTVsLX0KTDDi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do you think of the results? How good was it after 1 epoch? Is it going to improve much more if you run more epochs?"
      ],
      "metadata": {
        "id": "_V-lje-Z12oc"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "10-cnn.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}