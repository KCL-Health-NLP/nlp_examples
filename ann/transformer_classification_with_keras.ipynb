{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMgFnMvqYFTi33+RuvrYsZK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KCL-Health-NLP/nlp_examples/blob/master/ann/transformer_classification_with_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adapted from a [Keras team example](https://keras.io/examples/nlp/text_classification_with_transformer/)"
      ],
      "metadata": {
        "id": "Y00Jsv8jBnxa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySRW95bh8OV0"
      },
      "outputs": [],
      "source": [
        "# Basics\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Keras package to handle directories of text\n",
        "from tensorflow.keras.utils import text_dataset_from_directory\n",
        "\n",
        "# Model layers - we need these!\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# We use these next two when pre-processing string\n",
        "import string\n",
        "import re\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How many documents in a batch?\n",
        "batch_size = 32\n",
        "\n",
        "# Maximum or padded length (in tokens) of a text sequence\n",
        "sequence_length = 200\n",
        "\n",
        "# Maximum number of features in our text vector space.\n",
        "# i.e. how many different tokens in our vocabulary\n",
        "max_features = 20000\n",
        "\n",
        "# Dimensions in text embedding\n",
        "embedding_dim = 32\n",
        "\n",
        "# Number of training epochs\n",
        "epochs = 2\n",
        "\n",
        "# Prediction threshold, above which an output probability\n",
        "# will indicate class 1.\n",
        "#pred_threshold = 0.5"
      ],
      "metadata": {
        "id": "7UP70XpMBLoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a transformer block"
      ],
      "metadata": {
        "id": "Bi0piH7d-833"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "OARvHa3E-0Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build an embedding layer\n",
        "\n",
        "This will contain two separate embedding layers\n",
        "\n",
        "* Token embedding\n",
        "* Token position embedding"
      ],
      "metadata": {
        "id": "atEBbAgr_AIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "metadata": {
        "id": "SNZW0bl2-6QI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model"
      ],
      "metadata": {
        "id": "xVzcqH1y_t7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(sequence_length,))\n",
        "embedding_layer = TokenAndPositionEmbedding(sequence_length, max_features, embedding_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embedding_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "wRoe3wb2_tiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "**For the exercise at the end of this notebook, you will need to comment out the below cell**"
      ],
      "metadata": {
        "id": "P5GGSUix_U7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=max_features)\n",
        "#print(len(x_train), \"Training sequences\")\n",
        "#print(len(x_val), \"Validation sequences\")\n",
        "#x_train = keras.utils.pad_sequences(x_train, maxlen=sequence_length)\n",
        "#x_val = keras.utils.pad_sequences(x_val, maxlen=sequence_length)"
      ],
      "metadata": {
        "id": "pTVBzUZh_qYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train\n",
        "\n",
        "**For the exercise at the end of this notebook, you will need to comment out the below cell**"
      ],
      "metadata": {
        "id": "j0ziaGQ9_1UR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "#history = model.fit(\n",
        "#    x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_val, y_val)\n",
        "#)"
      ],
      "metadata": {
        "id": "ftzlOiMI_3TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise\n",
        "\n",
        "* Comment out the above two cells (Dataset and Training.\n",
        "* Write new code to get the IMDb text dataset from [https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz](https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz)\n",
        "* Read it in to Keras datasets, one each for training, validation and held out testing.\n",
        "* Preprocess the text, vectorize it, and use to train the model.\n",
        "* Evaluate the model against the held out test set."
      ],
      "metadata": {
        "id": "dGqEL_mAF4vS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the text"
      ],
      "metadata": {
        "id": "e2Z9ChJqAjCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "id": "Uf4weW6dAa1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't need the ``unsup``` directory of unsupervised training data"
      ],
      "metadata": {
        "id": "0aW-1QqrAzX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unix command to remove directory recursively\n",
        "# check it has worked!\n",
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "Iimi0e33Ax_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read in the text"
      ],
      "metadata": {
        "id": "vad8OaiKA9dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data, 80% of the train directory\n",
        "train_raw = text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,           # batches for use in future processing\n",
        "    validation_split=0.2,            # proportion of data to put in dev set\n",
        "    subset=\"training\",               # which train / val subset is this?\n",
        "    seed=1337,                       # you need to set the same seed here\n",
        "                                     # and in the val data to avoid overlap\n",
        ")\n",
        "\n",
        "# Validation / dev data - the remaining 20%\n",
        "val_raw = text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        ")\n",
        "\n",
        "# Held-out test data, all of it\n",
        "test_raw = text_dataset_from_directory(\n",
        "    \"aclImdb/test\",\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "-eDJK_xyA_B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess"
      ],
      "metadata": {
        "id": "fprYpAKpBmyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process text to standardise\n",
        "def preprocess_text(input_data):\n",
        "\n",
        "    # lowercase everything\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "\n",
        "    # remove html line breaks\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "\n",
        "    # remove escaped punctuation characters (e.g. \\' and \\xc3)\n",
        "    esc_removed = tf.strings.regex_replace(\n",
        "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
        "        )\n",
        "    \n",
        "    return esc_removed"
      ],
      "metadata": {
        "id": "5CZ-9i7TBotF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorization layer"
      ],
      "metadata": {
        "id": "xx9afR7tBrlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a TextVectorization layer, using the preprocess_text\n",
        "# function that we wrote above.\n",
        "# output_mode=\"int\" - builds an integer index, each unique\n",
        "#                     token mapped to an integer\n",
        "# max_tokens: I think this will restrict the integer index\n",
        "#             to the given number of most frequent tokens\n",
        "# output_sequence_length: restrict and pad output to this length\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=preprocess_text,\n",
        "    output_mode=\"int\",\n",
        "    max_tokens=max_features,\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "# We need to \"adapt\" the layer to our corpus of texts,\n",
        "# i.e. fit it to the vocabulary, computing the integer\n",
        "# mappings. Note this does not vectorize the text,\n",
        "# just computes the vocabulary\n",
        "\n",
        "# To do this we first need a text-only dataset with no labels.\n",
        "# Dataset.map(function) maps the values in a dataset using\n",
        "# the function. Here we use a simple lambda expression for\n",
        "# our function\n",
        "train_texts = train_raw.map(lambda x, y: x)\n",
        "\n",
        "# Now we can adapt to this text\n",
        "vectorize_layer.adapt(train_texts)\n"
      ],
      "metadata": {
        "id": "eSp_RZzdBugC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorize"
      ],
      "metadata": {
        "id": "nZ2LQsrqB-sC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to use out vectorize_layer to vectorize a text tensor\n",
        "# and return it with the label tensor\n",
        "def vectorize_text(text, label):\n",
        "\n",
        "    # add an innermost (right hand) dimension to the text\n",
        "    text = tf.expand_dims(text, -1)   \n",
        "    return vectorize_layer(text), label\n",
        "\n",
        "\n",
        "# Now we can vectorize the data.\n",
        "train_clean = train_raw.map(vectorize_text)\n",
        "val_clean = val_raw.map(vectorize_text)\n",
        "test_clean = test_raw.map(vectorize_text)"
      ],
      "metadata": {
        "id": "PS96v5KcDSYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improve performance"
      ],
      "metadata": {
        "id": "1JJviF_CDamw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Do async prefetching / buffering of the data for best performance on GPU.\n",
        "train_clean = train_clean.cache().prefetch(buffer_size=10)\n",
        "val_clean = val_clean.cache().prefetch(buffer_size=10)\n",
        "test_clean = test_clean.cache().prefetch(buffer_size=10)"
      ],
      "metadata": {
        "id": "eW8rgRfIDeBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "r95dW3bjDyGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model to the training data, validating against our validation\n",
        "# data on each epoch. Save the results to a History object.\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(train_clean, validation_data=val_clean, epochs=epochs)"
      ],
      "metadata": {
        "id": "SII3I9x9Dz37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot"
      ],
      "metadata": {
        "id": "ImajY2miD2hI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0Wi-X0H4D37U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {
        "id": "717RP1rgD-I0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(test_clean)\n",
        "print(f\"{'Test loss:':16}{score[0]:.2f}\")\n",
        "print(f\"{'Test accuracy:':16}{score[1]:.2f}\")"
      ],
      "metadata": {
        "id": "96SIAHbOD_8A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}