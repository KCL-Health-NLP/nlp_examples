{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOAj+GPhPYknrL7xVX7WxGR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KCL-Health-NLP/nlp_examples/blob/master/ann/transformer_classification_with_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers for text classification\n",
        "\n",
        "***This is the \"question\" version of this notebook. It differs from the \"answer\" version in that it runs on a pre-vectorized version of IMDb. You are asked to amend it to run on raw text input.***\n",
        "\n",
        "Adapted from a [Keras team example](https://keras.io/examples/nlp/text_classification_with_transformer/)\n",
        "\n",
        "The original has been changed as follows:\n",
        "\n",
        "* More text cell explanations and code comments\n",
        "* Inspection of intermediate steps\n",
        "* Plotting of training loss and accuracy\n",
        "* Seperated out parameters in to a single cell\n",
        "* Some renaming of variables\n",
        "* Small differences in use of imports\n",
        "* Code added to adapt to run on raw text input\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.\n",
        "\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "[http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ],
      "metadata": {
        "id": "Y00Jsv8jBnxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This practical uses Keras to build a transformer based text classifier, and trains it on the IMDb movie review dataset. It is very similar to the previous CNN practical, and assumes that you have run and understood the CNN practical. Explanations are not always repeated when they were give in the CNN practical.\n",
        "\n",
        "The initial model does not classigy raw text. Instead, it classifies a version of the IMDb dataset that has already been represented as integer vectors. You are asked to amend it to use raw text, by adapting the code from the previous CNN notebook."
      ],
      "metadata": {
        "id": "hBDUsHg0Lw0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using with GPUs\n",
        "\n",
        "The execution time of TensorFlow based code will benefit from the use of GPUs. To select a GPU runtime in colab:\n",
        "\n",
        "* Select the *Runtime* menu\n",
        "* Select the *Change runtime type* submenu\n",
        "* In the dialog that appears, under *Hardware accelerator* select *GPU*\n",
        "* Your existing runtime will disconnect, and you will be allocated and connected to a new GPU runtime.\n",
        "\n",
        "We will also improve execution time through the way in which we fetch and cache data, in one of the steps below."
      ],
      "metadata": {
        "id": "y_jWx-m2L_zC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages\n",
        "\n",
        "First, the import. You will need Keras. Keras is the default high-level API for TensorFlow, which is itself the most popular neural net libray. \n",
        "\n",
        "**Note if running locally:** in order for the visualisation to work, you will need to have pydot and graphviz installed, e.g. \n",
        "\n",
        "```sudo apt-get install graphviz\n",
        "pip3 install pydot```"
      ],
      "metadata": {
        "id": "G32FBnqzLsCu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySRW95bh8OV0"
      },
      "outputs": [],
      "source": [
        "# Basics\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Keras package to handle directories of text\n",
        "from tensorflow.keras.utils import text_dataset_from_directory\n",
        "\n",
        "# Model layers - we need these!\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# We use these next two when pre-processing string\n",
        "import string\n",
        "import re\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters\n",
        "\n",
        "Now let's set up some parameters, such as number of features, embedding dimensions, batch size, epochs etc."
      ],
      "metadata": {
        "id": "66Pq0tEyMJfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many documents in a batch?\n",
        "batch_size = 32\n",
        "\n",
        "# Maximum or padded length (in tokens) of a text sequence\n",
        "sequence_length = 200\n",
        "\n",
        "# Maximum number of features in our text vector space.\n",
        "# i.e. how many different tokens in our vocabulary\n",
        "max_features = 20000\n",
        "\n",
        "# Dimensions in text embedding\n",
        "embedding_dim = 32\n",
        "\n",
        "# Number of training epochs\n",
        "epochs = 2"
      ],
      "metadata": {
        "id": "7UP70XpMBLoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a transformer block\n",
        "\n",
        "We will create new classes to model a transformer block and a transformer embedding layer. Some of you will be familiar with creating classes in Python, but for those of you who are not, here is a brief explanation.\n",
        "\n",
        "We will define new layers as classes, as this will encapsulate and hide the details. In our code, we will be able to refer to a whole transformer block in one line, without having to write them each time. If we packaged up the classes, we could use them in other code by importing.\n",
        "\n",
        "We define a class like this:\n",
        "\n",
        "```class TransformerBlock(layers.Layer):```\n",
        "\n",
        "Inside the class definition we can put methods and data attributes. In the parenthese, we can define any *superclasses* i.e. classes from which our new class inherits functionality. There are some special methods defined in the class.\n",
        "\n",
        "The ```__init__(...)``` method is called whenever an object of this class is made. We define the different layers of our class in here. When you do the following, the ```__init__(...)``` method is run:\n",
        "\n",
        "\n",
        "```newLayer = TransformerBlock(...)```\n",
        "\n",
        "\n",
        "The ```call(...)``` method defines the structure of and computation of our class. We connect the layers to each other and the input here. It is called (a) when you add another layer to this class and (b) when the network is run. (Internally, it calls another method, ```__call__ ``` ).\n",
        "\n",
        "\n",
        "When you do the following, the code in ```__call__(...)``` is run in the last step:\n",
        "\n",
        "\n",
        "```\n",
        "x = SomeLayer(...)\n",
        "y = TransformerBlock(...)\n",
        "z = y(x)\n",
        "```\n",
        "\n",
        "\n",
        "Alternatively, when you run the following functional style of Keras code, an object of the class is created and ```__init__(...)``` is run, then ```__call__ ```  is run on that object:\n",
        "\n",
        "```\n",
        "x = SomeLayer(...)\n",
        "x = TransformerBlock(...)(x)\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bi0piH7d-833"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A transformer block, inherits from Layer\n",
        "class TransformerBlock(layers.Layer):\n",
        "\n",
        "    # __init__(...) is called when you create a new object of\n",
        "    # this class.\n",
        "    #\n",
        "    # __init(...)__ creates the layers we will need in our block.\n",
        "    #\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # We define some layers on initialisation, but leave building\n",
        "        # the netwrok until we now what the input is, in call()\n",
        "\n",
        "        # A multihead attenction layer\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "\n",
        "        # Feed forward consisting of two dense layers\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "\n",
        "        # Normalization layers\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    # call(...) is called by __call__(...) whic is itself called\n",
        "    # when you use an object of this class as a function,\n",
        "    # e.g. someTransformerBlock(x)\n",
        "    #\n",
        "    # call(...) combines the layers created by __init__(...)\n",
        "    # with the inputs from other layers.\n",
        "    #\n",
        "    def call(self, inputs, training):\n",
        "\n",
        "        # attention then dropout\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "\n",
        "        # normalize: takes input and concatenates attention output\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "\n",
        "        # Feed forward then dropout\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "\n",
        "        # normalize: takes output from feed forward and concatenates\n",
        "        # previous normaised attention\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "OARvHa3E-0Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build an embedding layer\n",
        "\n",
        "We have already seen token embeddings. In transformers, we extend this to also create an embedding that encodes the position of the token. We do this beacause transformers have no information about the order of words.\n",
        "\n",
        "A very simple approach is to learn an embedding for each position in the same way we learn a token embedding. This is what we will do here. There are more sophisticated approaches.\n",
        "\n",
        "Our embedding layer therefore contains two separate embeddings:\n",
        "\n",
        "* Token embedding\n",
        "* Token position embedding\n",
        "\n",
        "We will concatenate thes."
      ],
      "metadata": {
        "id": "atEBbAgr_AIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# An embedding layer for tokens and their positions.\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "\n",
        "    # Define the two embeddings\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    # Create an embedding for the position, and\n",
        "    # one for the token. Concatenate them\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "metadata": {
        "id": "SNZW0bl2-6QI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model\n",
        "\n",
        "We build the model layers using Keras [functional programming syntax](https://keras.io/guides/functional_api/), as in the CNN practical."
      ],
      "metadata": {
        "id": "xVzcqH1y_t7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will create a network including a transformer block\n",
        "# and other layer.\n",
        "\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "# Input layer, to take vectorized text\n",
        "inputs = layers.Input(shape=(sequence_length,))\n",
        "\n",
        "# Token and position embedding\n",
        "embedding_layer = TokenAndPositionEmbedding(sequence_length, max_features, embedding_dim)\n",
        "x = embedding_layer(inputs)\n",
        "\n",
        "# Trannsformer\n",
        "transformer_block = TransformerBlock(embedding_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "\n",
        "# Pooling / dropout / dense / dropout / dense layers\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)"
      ],
      "metadata": {
        "id": "wRoe3wb2_tiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have all of our layers, let's put them in to a model, by using our input layer and our final predictions layer as parameters. The model wraps up the layers, adding training and inference functionality.\n",
        "\n",
        "We can then compile our model, i.e. configure it for training by providing parameters for the loss function, optimisatiom, and metrics we will use."
      ],
      "metadata": {
        "id": "WNmjpydsVGno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "Q-I3XVFOVIgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "**For the exercise at the end of this notebook, you will need to comment out the below cell. For now, leave it uncommented and run it.**\n",
        "\n",
        "The previous CNN practical used raw text from the IMDb dataset. In this practical, we will start by using a [version of the IMDb dataset that ships with Keras](https://keras.io/api/datasets/imdb/). This has already pre-processed and reviews encoded as vectors of integers. Using such a dataset makes our job a bit easier when developing and experimenting with model architectures, as we do not need to deal with text pre-processing.\n",
        "\n",
        "Once you have got the model working with the pre-vectorized version of IMDb, you are asked to adapt it to work with raw text.\n",
        "\n",
        "We read in vectors named as follows:\n",
        "* ```x_``` : text features\n",
        "* ```y_``` : labels\n",
        "* ```_train``` : training\n",
        "* ```_val``` : validation"
      ],
      "metadata": {
        "id": "P5GGSUix_U7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the IMDb dataset from Keras\n",
        "\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")\n",
        "\n",
        "# Pad sequences to our maximum length\n",
        "x_train = keras.utils.pad_sequences(x_train, maxlen=sequence_length)\n",
        "x_val = keras.utils.pad_sequences(x_val, maxlen=sequence_length)"
      ],
      "metadata": {
        "id": "pTVBzUZh_qYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model\n",
        "\n",
        "**For the exercise at the end of this notebook, you will need to comment out the below cell. For now, leave it uncommented and run it.**\n",
        "\n",
        "Now let's train it. Keras will validate against our test data, showing us loss and accuracy as it goes, and saving these in to a ```History``` object. We can use this ```History``` to display the results of each epoch, after we have finished all training."
      ],
      "metadata": {
        "id": "j0ziaGQ9_1UR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_val, y_val)\n",
        ")"
      ],
      "metadata": {
        "id": "ftzlOiMI_3TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise\n",
        "\n",
        "Using the example from the previous CNN notebook,\n",
        "\n",
        "* Comment out the above two cells (**Dataset** and **Training the model**).\n",
        "* Write new code to get the IMDb ***text*** dataset from [https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz](https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz)\n",
        "* Read it in to Keras datasets, one each for training, validation and held out testing.\n",
        "* Preprocess the text, vectorize it, and use to train the model.\n",
        "* Evaluate the model against the held out test set.\n",
        "* **Optional:** visualise models, and write an end-to-end model that will accept text as input and return a classification."
      ],
      "metadata": {
        "id": "dGqEL_mAF4vS"
      }
    }
  ]
}