{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOJlmGWjDEA1Fu11NGdRczb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KCL-Health-NLP/nlp_examples/blob/master/ann/evaluating_spacy_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing spaCy NER CNN and transformer models\n",
        "\n",
        "**This is the \"question\" version of this notebook. It differs from the \"answer\" version in that you are asked to complete some of the code cells.  It will not work without these cells being completed correctly.**\n",
        "\n",
        "The default spaCy models are CNN based, trained on several datasets. spaCy also makes available transformer models, fine-tuned on the same datasets. This allows us to compare the two. We will carry out such a comparison for NER.\n",
        "\n",
        "The notebook uses the same classes and methods as introduced in the earlier spaCy NER practical notebook. spaCy hides the trnasoformer models behind the same API as used in the rest of the library, making transformer use completely transparent to the end user. The approach is very different to Hugging Face / Keras / PyTorch / TensorFlow, but depending on what you are trying to do with your NLP, it is worth considering.\n",
        "\n",
        "Under the hood, spaCy uses a machine learning library called thinc, which itself sits on top of PyTorch tensors (by deafult - other libraries supported). \n",
        "\n",
        "We will test the spaCy models on a standard NER dataset, [the CoNLL NER shared task dataset](https://www.clips.uantwerpen.be/conll2003/ner/). This was built for a community challenge - an NLP competition between different research teams, run to push forward the state of the art. Such challenges are common in the NLP research world, giving us many datasets to use for testing ideas."
      ],
      "metadata": {
        "id": "jlwa3VYgPqPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using with GPUs\n",
        "\n",
        "The execution time of this code will benefit from the use of GPUs. To select a GPU runtime in colab:\n",
        "\n",
        "* Select the *Runtime* menu\n",
        "* Select the *Change runtime type* submenu\n",
        "* In the dialog that appears, under *Hardware accelerator* select *GPU*\n",
        "* Your existing runtime will disconnect, and you will be allocated and connected to a new GPU runtime.\n",
        "\n",
        "We will also improve execution time through the way in which we fetch and cache data, in one of the steps below."
      ],
      "metadata": {
        "id": "M-kCxPd3RQO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install spaCy packages\n",
        "\n",
        "The standard spaCy installation does not include transformer support. We need to install packages needed by spaCy for this, including ther spaCy transformers package, and the spaCy CUDA package. CUDA is a library for paralellising code on GPUs.\n",
        "\n",
        "See the [spaCy usage notes](https://spacy.io/usage) for mode details on installing extra packages.\n",
        "\n",
        "*Note: as of May 2023, the below install was giving a couple of error. However, the notebook ran succesfully despite these.*"
      ],
      "metadata": {
        "id": "xeVzgYUVRf2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install spaCy transformer packages\n",
        "# Gives some errors, but seems ok to ignore these\n",
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U 'spacy[transformers,cuda-autodetect]'"
      ],
      "metadata": {
        "id": "BEthIoJNtQdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Restart your runtime***\n",
        "**You need to restart your runtime in order for the above packages to be made available for imports**\n",
        "\n",
        "* Menus\n",
        "  * Runtime\n",
        "    * Restart runtime"
      ],
      "metadata": {
        "id": "GstuuWxLJkqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "42E0GDThTIm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We are using spaCy for NER.\n",
        "import spacy\n",
        "\n",
        "# DocBin is a serialisable collection of spacy\n",
        "# Documents.\n",
        "from spacy.tokens import DocBin\n",
        "\n",
        "# Displacy provides a graphic display of\n",
        "# documents and annotations, and Scorer scores...\n",
        "from spacy import displacy\n",
        "from spacy.scorer import Scorer\n",
        "\n",
        "# Example holds spacy documents,\n",
        "# one with predicted annotations\n",
        "# and one with gold standard .\n",
        "# We will use it when evalusating.\n",
        "from spacy.training import Example\n"
      ],
      "metadata": {
        "id": "ZqUzRghzoi18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checks and bug hacks\n",
        "\n",
        "We'll just do a quick check, to make sure we are using GPUs. We will do this with the torch library."
      ],
      "metadata": {
        "id": "h8ABlSSjTS4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check we are using torch.cuda\n",
        "import torch\n",
        "print('Torch available:', torch.cuda.is_available())\n",
        "print('Number of torch devices:', torch.cuda.device_count())\n",
        "print('Torch current device:', torch.cuda.current_device())"
      ],
      "metadata": {
        "id": "dM0C7p1vyd87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will provide a fix for a bug that happens when initialising spaCy on GPUs. This is apprently a bug with the CUDA Library. The below is a temporary work around until CUDA is fixed."
      ],
      "metadata": {
        "id": "7IebOs7uTtA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a locale error on spacy init with GPU - here's a quick fix\n",
        "# Code from https://github.com/explosion/spaCy/issues/11909\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "gsobF2vrsGJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download spaCy models\n",
        "\n",
        "We will download two models:\n",
        "\n",
        "* The ```en_core_web_lg``` model is a CNN based entity recogniser, trained on several datasets.\n",
        "* The ```en_core_web_trf``` model is based on the [roBERTa](https://huggingface.co/docs/transformers/model_doc/roberta) pre-trained transformer model, fine-tuned with the same datasets as used for ```en_core_web_lg``` training. roBERTa is a varient on BERT. spaCy sources its transformer models from Hugging Face.\n",
        "\n",
        "You can find out more about the models and the data used to train them in the [spaCy model documentation](https://spacy.io/models/en).\n"
      ],
      "metadata": {
        "id": "KtSQqXrb7nfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download spacy models\n",
        "!python -m spacy download en_core_web_lg\n",
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "id": "z1X_QC0Kq1jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get the data\n",
        "\n",
        "* The dataset is sourced from here: [CoNLL 2003 dataset](https://www.clips.uantwerpen.be/conll2003/ner/)\n",
        "* Its original construction and use is described in this paper: [CoNLL 2003 NER shared task](https://aclanthology.org/W03-0419/)\n"
      ],
      "metadata": {
        "id": "SrL1RYeq9Yvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the data -  wget is a web client, useful for\n",
        "# downloading web pages or files hosted on the web\n",
        "!wget https://github.com/KCL-Health-NLP/nlp_examples/raw/master/ann/conll2003_test.txt"
      ],
      "metadata": {
        "id": "FQXgoHE6E9kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at our test data - head is a unix command to\n",
        "# retrive the first n rows of a file\n",
        "!head -100 ./conll2003_test.txt"
      ],
      "metadata": {
        "id": "q8nMLst3oxD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall the spaCy NER labels, and compare them to the CoNLL ones shown above. spaCy covers more entity types, uses different names, and slightly different definitions to CoNLL. The following list shows the equivalences:\n",
        "\n",
        "* **Location:** CoNLL LOC == most are spaCy GPE, with some  being spaCy LOC\n",
        "* **Person:** CoNLL PER == spaCy PERSON\n",
        "* **Organisation:** CoNLL ORG == spaCy ORG\n",
        "\n",
        "So we can evaluate against the CoNLL data, we will map CoNLL entity types to spaCy types. We could do this in Python, but it is quicker and easier to use another Unix command line tool, ```sed```. This is a stream editor which will replace one string with another in a strema of text. The below shows this for converting ```-LOC``` to ```-GPE```.\n",
        "\n",
        "**Exercise**\n",
        "\n",
        "Complete the below code cell to add another ```sed``` line that converts CoNLL PER tags to PERSON tags."
      ],
      "metadata": {
        "id": "MXp_OedGBuOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sed stream editing\n",
        "# -i means \"inplace\", i.e. edit and save back to the same file\n",
        "# g at the end of the pattern means replace globally\n",
        "!sed -i 's/-LOC/-GPE/g' conll2003_test.txt\n",
        "!sed -i 's/-PER/-PERSON/g' conll2003_test.txt"
      ],
      "metadata": {
        "id": "KLAbs_XeEthj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did it work?"
      ],
      "metadata": {
        "id": "GVzYzv-9biRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take another look at our test data\n",
        "!head -100 ./conll2003_test.txt"
      ],
      "metadata": {
        "id": "zHpSOUUZNebu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we convert the data to .spacy serialised DocBin format, as in the earlier spaCy NER practical notebook."
      ],
      "metadata": {
        "id": "oUsoL7UGbnHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to .spacy DocBin format\n",
        "!python -m spacy convert ./conll2003_test.txt . -c ner -n 10\n"
      ],
      "metadata": {
        "id": "V2UFRx3WU3kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the spaCy models\n",
        "\n",
        "**NB if you did not restart your runtime after installing spaCy (first code cell), then you will not have the spaCy transformers library, and loading the trf model will fail.**"
      ],
      "metadata": {
        "id": "OnmPqO_sJwOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spacy models\n",
        "nlp_lg = spacy.load('en_core_web_lg')\n",
        "nlp_tr = spacy.load('en_core_web_trf')"
      ],
      "metadata": {
        "id": "Xmjfj85ApUbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data in to spaCy\n",
        "\n",
        "We now de-serialise the data in to a DobBin, and take a look at it."
      ],
      "metadata": {
        "id": "qoQrufv-cvbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = DocBin().from_disk(\"./conll2003_test.spacy\")"
      ],
      "metadata": {
        "id": "G8Pl04KowwxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(docs))"
      ],
      "metadata": {
        "id": "GFL_j9EVw7-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in docs.get_docs(nlp_lg.vocab):\n",
        "  print(doc.ents)\n"
      ],
      "metadata": {
        "id": "zHPyk7Qz-8Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Socring the Named Entity Recognition\n",
        "\n",
        "We will write a function to run a pipeline over all of the documents in a DocBin, and to compare each one to the CoNLL gold standard named entities. This is very similar to the scoring in our previous spaCy NER notebook.\n",
        "\n",
        "**Exercise**\n",
        "\n",
        "Complete the scoring function given below, using the comments to guide you."
      ],
      "metadata": {
        "id": "DuKwu-PG5Y3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run pipeline over the text each gold standard document\n",
        "# in docs. For each document, add both the predicted\n",
        "# version and the gold standard version to an Example\n",
        "# object.\n",
        "# Return a tuple containing:\n",
        "# (1) the results of running a Scorer over all examples\n",
        "# (2) a list of all examples \n",
        "def run_and_score_nlp(docs, pipeline):\n",
        "\n",
        "  # A Scorer to do our scoring at the end\n",
        "  scorer = Scorer()\n",
        "\n",
        "  # A list in which to store the Examples\n",
        "  examples = []\n",
        "\n",
        "  # Iterate over the documents\n",
        "  for gold_doc in docs.get_docs(pipeline.vocab):    # COMPLETE THIS LINE\n",
        "\n",
        "    # Create the predicted document from\n",
        "    # the gold standard text\n",
        "    pred_doc = pipeline(gold_doc.text)              # COMPLETE THIS LINE\n",
        "\n",
        "    # Create the Example\n",
        "    ex = Example(pred_doc, gold_doc)                # COMPLETE THIS LINE\n",
        "\n",
        "    # Add the Example to the examples list\n",
        "    examples.append(ex)                             # COMPLETE THIS LINE\n",
        "\n",
        "  return (scorer.score(examples), examples)         # COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "2Rd6bEngNu8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can score our two pipelines, using the function we wrote above. Remember, it returns a tuple: the scorres and the examples.\n",
        "\n",
        "We've also added in a few line to time each one, in nano seconds. Timing like this is a bit crude, but may be interesting."
      ],
      "metadata": {
        "id": "G256t8TJKFl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start = time.process_time_ns()\n",
        "scores_lg, examples_lg = run_and_score_nlp(docs, nlp_lg)\n",
        "print(\"LG time:\", time.process_time_ns() - start)\n",
        "\n",
        "start = time.process_time_ns()\n",
        "scores_tr, examples_tr = run_and_score_nlp(docs, nlp_tr)\n",
        "print(TR time:\", time.process_time_ns() - start)\n"
      ],
      "metadata": {
        "id": "7OdbhUjrzZUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the scores: let's print them out.\n",
        "\n",
        "**Exercise**\n",
        "\n",
        "The scores data structure is a nested dictionary, like this:\n",
        "\n",
        "```scores['ents_per_type'][LABEL-NAME][METRIC-NAME]```\n",
        "\n",
        " Comlpete the code below to print our the scores for each of the listed labels and metrics.\n"
      ],
      "metadata": {
        "id": "8p3kMnNh5gPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists of labels and metrics to scofre\n",
        "labels = ['GPE', 'ORG', 'PERSON']\n",
        "metrics = ['p', 'r', 'f']\n",
        "\n",
        "print(f'{\"label\": <18}{\"score\": <8}{\"lg\": <6}{\"tr\": <6}')\n",
        "\n",
        "# Iterate over the labels\n",
        "for l in labels:                                          # COMPLETE THIS CODE\n",
        "\n",
        "  # Iterate over the metrics\n",
        "  for m in metrics:                                       # COMPLETE THIS CODE\n",
        "\n",
        "    # Retrieve the large models metric\n",
        "    lg = scores_lg['ents_per_type'][l][m]                 # COMPLETE THIS CODE\n",
        "\n",
        "    # Retrieve the transformer model metric\n",
        "    tr = scores_tr['ents_per_type'][l][m]                 # COMPLETE THIS CODE\n",
        "\n",
        "    # Print the two scores\n",
        "    print(f'{l: <18}{m: <8}{lg: <6.2f}{tr: <6.2f}')       # COMPLETE THIS CODE"
      ],
      "metadata": {
        "id": "DMn4iEZmHKyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display some documents\n",
        "\n",
        "Finally, let's take a look at some documents, using displacy. We will choose a document number, and render the transformer, large, and gold standard annotations for this document.\n",
        "\n",
        "Before rendering, we will add tiles to our document data. displacy will render this as a heading on eahc document, to help us distinguish themm"
      ],
      "metadata": {
        "id": "Hbmrnlfi5wR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve documents\n",
        "# Add titles to documents - displacy will render these titles.\n",
        "\n",
        "doc_num = 150\n",
        "\n",
        "# transformer model\n",
        "doc_tr = examples_tr[doc_num].predicted\n",
        "doc_tr.user_data[\"title\"] = \"Transformer model predictions\"\n",
        "\n",
        "# large model\n",
        "doc_lg = examples_lg[doc_num].predicted\n",
        "doc_lg.user_data[\"title\"] = \"Large model predictions\"\n",
        "\n",
        "# gold standard\n",
        "doc_ref = examples_tr[doc_num].reference\n",
        "doc_ref.user_data[\"title\"] = \"Gold standard\"\n",
        "\n"
      ],
      "metadata": {
        "id": "GhqSJWJzJ3hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**\n",
        "\n",
        "Write code to render all three documents. You will need to make sure you only display the entity labels we are interested in. See the [displacy documentation](https://spacy.io/usage/visualizers#ent) for information on how to do this. Put a line of dashes or some newlines between each document."
      ],
      "metadata": {
        "id": "3qZuDtSO6Enu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display in displacy\n",
        "displacy.render(doc_tr, style='ent', jupyter=True, options={'ents':labels})\n",
        "print('\\n'*2)\n",
        "displacy.render(doc_lg, style='ent', jupyter=True, options={'ents':labels})\n",
        "print('\\n'*2)\n",
        "displacy.render(doc_ref, style='ent', jupyter=True, options={'ents':labels})"
      ],
      "metadata": {
        "id": "Qq1D8tSO53RA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}