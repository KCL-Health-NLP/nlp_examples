{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt engineering with LangChain - an NER example\n",
        "\n",
        "*Written by Jaya Chaturvedi and Angus Roberts, May 2024*\n",
        "\n",
        "---\n",
        "\n",
        "###Generative models\n",
        "In pevious practicals, we have used the BERT transfomer model. To recap, this model is:\n",
        "\n",
        "* the encoder layer of an encoder-decoder\n",
        "* trained on a masked word prediction task, and\n",
        "* trained on a next sentence prediction task\n",
        "* trained on 3.3 billion words\n",
        "* 110 million parameters\n",
        "\n",
        "We are now going to look at generative large language models, such as the GPT models. Such models are:\n",
        "\n",
        "* the decoder layer of an encoder-decoder\n",
        "* trained on a text generation task (predict the next word), and\n",
        "* conditioned on other tasks, such as following instructions\n",
        "* trained on terrabytes or petabytes of text\n",
        "* Billions of parameters\n",
        "\n",
        "This practical has been written for use with an 8 billion parameter version of Llama 3, so more than 70 times larger then BERT. It has been instruction tuned and optimized for dialogue use cases. There is also an 80 billion parameter version available.\n",
        "\n",
        "###Prompting\n",
        "These models generate text. You can play with them, asking them to write songs, poems etc etc, typing text at a **prompt**, with the model completing the text you have started by generating the next N words. What you get depends partly on the model, partly on model parameters you set, but also partly on the way in which you ask the question - i.e. your **prompt engineering**.\n",
        "\n",
        "We will look at how we might design prompts to generate output in a consistent format a task, specifically for NER.\n",
        "\n",
        "###Hosting the model - Hugging Face\n",
        "\n",
        "These models are large! You could put them on your laptop, but (a) they will take time to download and (b) they might dwarf it's memory. So, we will use a remotely hosted model, on Hugging Face. Hugging Face has several publically availabel models. For Llama 3, we are using a paid-for model hosting service. This gives us more control over availability, scaling etc.\n",
        "\n",
        "###Interacting with the model - LangChain\n",
        "\n",
        "All models have different ways of interacting with them, whether via a prompt or an API. We will be using a widely-used Python library that hides these differences behind a common API, LangChain. LangChain will work with many different models, both remotely hosted and locally hosted. So you should be able to re-purpose this code for other models and situations.\n",
        "\n",
        "LangChain is big - we will only look at a few of LangChain's features, and some basic prompting strategies.\n",
        "\n",
        "There are often other ways, but we. hope that this practical will show you the basics.\n",
        "\n",
        "###Further information and resources\n",
        "\n",
        "We would like to recommend Chapter 12 in the 3rd edition of Jurafsky and Martin's \"Speech and Language Processing\" - but it hasn't yet been written! Try these instead:\n",
        "\n",
        "* A\n",
        "* B\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "1itC_4Z1tdtg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "LyQ3t2iXVou7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFVXCbyIUImN"
      },
      "outputs": [],
      "source": [
        "# LangChain needs to have the Hugging Face transformers package installed,\n",
        "# and we need several LangChain packages\n",
        "%pip install --upgrade --quiet transformers\n",
        "%pip install --upgrade --quiet langchain langchain_community\n",
        "%pip install --upgrade --quiet langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The most important imports are her, though we will import\n",
        "# a small number of other packages later, for specific pieces of code\n",
        "import langchain_community\n",
        "from langchain_huggingface.llms import HuggingFaceEndpoint\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "I_UanP0nUdV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Hugging Face endpoints - getting a access token\n",
        "\n",
        "We will use a model hosted at a Hugging Face endpoint. To use this, you will need to have. Hugging Face account, and generate a Hugging Face access token. This can all be done from the [Hugging Face website](https://huggingface.co/) and is fairly self explanatory, but for details see the previous practical, and at this link:\n",
        "\n",
        "[Hugging Face security tokens](https://huggingface.co/docs/hub/en/security-tokens)\n",
        "\n",
        "Once you have a token, you need to run the code cell below, and when asked paste your token in to the prompt. It will remain hidden."
      ],
      "metadata": {
        "id": "TTrGwafqt4tB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "Details on setting up endpoints\n",
        "\n",
        "(You don't need to know this - some links different types of Hugging Face endpoints and setting them up, useful when creating new endpoints for this practical)\n",
        "\n",
        "\n",
        "* [LangChain integration with Hugging Face endpoints](https://python.langchain.com/docs/integrations/llms/huggingface_endpoint)\n",
        "* [Hugging Face serverless inference](https://huggingface.co/docs/api-inference/index)\n",
        "* [Setting up Hugging Face dedicated endpoints](https://huggingface.co/inference-endpoints/dedicated)\n",
        "* The UI for starting and stopping and configuring endpoints is here: `https://ui.endpoints.huggingface.co/[YOUR USER NAME]/endpoints`\n",
        "* Types of endpoint\n",
        " * Protected - need the owner's token\n",
        " * Public - needs any user's token\n",
        "</details>"
      ],
      "metadata": {
        "id": "IAsWAX1n3bxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# You need to get your access token from huggingface, run this cell and paste\n",
        "# it in to the resulting prompt, for use in later sections\n",
        "# How to get a token is described here:\n",
        "# https://huggingface.co/docs/api-inference/quicktour#get-your-api-token\n",
        "\n",
        "# getpass provides an obscured password prompt.\n",
        "# os gives access to operating ssytem functionality, which\n",
        "# we need to set an environment-wide variable to hold our token\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "HUGGINGFACEHUB_API_TOKEN = getpass()\n",
        "\n",
        "# We put the token in an environment variable, from where LangChain will access it when needed\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
      ],
      "metadata": {
        "id": "-st7mi8CyvOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Querying a Hugging Face LLM server directly\n",
        "\n",
        "Now you have an access token, you can use model endpoint hosted by Hugging Face in your code. There are many APIs that allow you to do this. The first method we will use is to call it directly as a web server, using a standard post method sent to the web server with the normal HTTP(S) protocol, just like your web browser uses."
      ],
      "metadata": {
        "id": "Ws1Wj5PfaCP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This example uses the Hugging Face service API direct\n",
        "# to access a freely available GPT2 model hosted\n",
        "# on Hugging Face\n",
        "\n",
        "# requests is a package to send requets to web servers\n",
        "import requests\n",
        "\n",
        "# the Hugging Face API server for GPT2\n",
        "API_URL = \"https://api-inference.huggingface.co/models/gpt2\"\n",
        "\n",
        "# Headers for our request - the token\n",
        "headers = {\"Authorization\": f\"Bearer {HUGGINGFACEHUB_API_TOKEN}\"}\n",
        "\n",
        "# function to post the request and return the response\n",
        "# takes a json query as the payload\n",
        "def query(payload):\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    return response.json()\n",
        "\n",
        "# Send a query and print the results\n",
        "data = query({\"inputs\": \"Can you please let us know more details about your \"})\n",
        "print(data)"
      ],
      "metadata": {
        "id": "2sXMlU0lyhVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can of course try different pieces of text, getting the model to generate the sequence of words it predicts will come next."
      ],
      "metadata": {
        "id": "Awa_wAIB5c6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A simple LangChain LLM, using a Hugging Face free endpoint\n",
        "\n",
        "Instead of using the http web protocol to communicate with our serer, we will use the LangChain API. This wraps many different types of remote and local model. The abstraction we will use is the LangChain LLM class, which you can learn more about in the [LangChain documentation](https://python.langchain.com/v0.1/docs/modules/model_io/llms/quick_start/).\n",
        "\n",
        "We will create a LangChain LLM that wraps a Hugging Face endpoint, using the LangChain [HuggingFaceEndpoint class](https://api.python.langchain.com/en/v0.1/llms/langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain-community-llms-huggingface-endpoint-huggingfaceendpoint), which implements the LangChain LLM interface.\n",
        "\n",
        "What is the advantage of this approach, over querying directly as in the above example?\n",
        "\n",
        "We will start by querying a free public model with a simple prompt, as before."
      ],
      "metadata": {
        "id": "S4Km8r82aLHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also use these free model endpoints via Lang Chain, an API that wraps up\n",
        "# many different ways of accessing models in a common interface.\n",
        "# Here we use it to access a public Hugging Face endpoint which has limited models,\n",
        "# and no guarantee on performance or availability\n",
        "\n",
        "# You could find out about other models hosted on Hugging Face, and replace\n",
        "# the next line with the repository ID of another\n",
        "repo_id = \"openai-community/gpt2\"\n",
        "\n",
        "# Make the LLM\n",
        "llm = HuggingFaceEndpoint(\n",
        "   endpoint_url=\"https://api-inference.huggingface.co/models/\" + repo_id,\n",
        "   task=\"text-generation\",\n",
        "   temperature = 0.1,\n",
        "   model_kwargs={\"max_length\": 128}\n",
        ")\n",
        "\n",
        "# Invoke the model directly\n",
        "llm(\"When I went to Paris, I \")\n"
      ],
      "metadata": {
        "id": "rfXMhhdyrJmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using a paid-for endpoint - Llama 3\n",
        "\n",
        "We can use exactly the same code for other free models, or for a paid-for endpoint hosted on HuggingFace. We could also use the same approach to create LLMs from local models, by using a different LangChain subclass of LLM.\n",
        "\n",
        "Here, we connect to a paid-for endpoint, which we will use for the rest of the practical.\n",
        "\n",
        "What are the pros and cons of using these different methods of model delivery?"
      ],
      "metadata": {
        "id": "kx2mKqAWa2va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the paid for model endpoint, which can host a wider range of models\n",
        "# This is the url of a paid for endpoint - replace with whichever you are using\n",
        "# You need to enter the URL provided for the practical!\n",
        "endpoint_url = \"PUT THE PROVIDED ENDPOINT URL HERE\""
      ],
      "metadata": {
        "id": "haa9Lmuo1blM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the model - we are starting to use more parameters\n",
        "llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=endpoint_url,\n",
        "    max_new_tokens=256,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    typical_p=0.95,\n",
        "    temperature=0.01,\n",
        "    repetition_penalty=1.03,\n",
        ")\n",
        "\n",
        "# Invoke the model directly\n",
        "llm(\"My favourite joke is \")"
      ],
      "metadata": {
        "id": "M-4UmI9U-Gy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does the output of Llama 3 compare to the GPT 2 model used in the previous exercise?"
      ],
      "metadata": {
        "id": "LsOmQXpO9K0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing prompts\n",
        "\n",
        "So far we have written very simple throwaway prompts for our models. But, many prompting tasks follow repeated patterns. LangChain has abstractions for writing prompt templates, that we can reuse. You can read more about this in the documentation for the base class, [PromptTemplate](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/).\n",
        "\n",
        "Once we have a PromptTemplate, we can use it with our model. There are various ways of doing this, perhaps the most useful being [LangChain Expression Language (LCEL)](https://python.langchain.com/v0.1/docs/expression_language/). This is a simple language that allows you to chain together different components that can be run by LangChain, such as LLMs and PromptTenmplates (because they implement a specific interface, Runnable). Here's a simple example, creating a chain from a prompt variable called `question` and a model called `llm`:\n",
        "\n",
        "`chain = question | llm`\n",
        "\n",
        "We can then invoke our chain, which will run the prompt and pass it to the model:\n",
        "\n",
        "`chain.invoke({dictionary of parameters})`\n",
        "\n",
        "There are lots of other LangChain interfaces and classes that can be used in these chains, such as output parsers which take the output and parse it in to something useful, and retrieval augmented generators, which retrieve sets of documents to pass to the model for use when answering. See the [LCEL documentation](https://python.langchain.com/v0.1/docs/expression_language/) for lots more examples.\n",
        "\n",
        "Try these two prompt examples, varying your input, to see how prompts might be reused:"
      ],
      "metadata": {
        "id": "V8xXh27QbWJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------\n",
        "# EXAMPLE 1\n",
        "#-------------------------------------------\n",
        "\n",
        "# Creating a string template for our prompt,\n",
        "# with a placeholder for a varianle that we\n",
        "# will change each time we use it\n",
        "template = \"\"\"Where is {city}\"\"\"\n",
        "\n",
        "# Now we can instantiate this prompt\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# And chain it with the LLM we created before\n",
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "E19s-mco-bZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try it - have a go with a few city names\n",
        "city_name = \"Paris\"\n",
        "print(chain.invoke({\"city\": city_name}))"
      ],
      "metadata": {
        "id": "uXv-NA09-8Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------\n",
        "# EXAMPLE 2\n",
        "#-------------------------------------------\n",
        "\n",
        "# The prompt string\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "# Make the prompt\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# Chain it to the LLM\n",
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "6fT155Xnz9-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try it out\n",
        "question = \"Why is the sun so hot?\"\n",
        "print(chain.invoke({\"question\": question}))"
      ],
      "metadata": {
        "id": "M1I_ck_cbcuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try this last prompt with a few other questions:\n",
        "\n",
        "* What is the French for cheese?\n",
        "* What is 2+2 ?\n",
        "* How many dollars to the euro?\n",
        "* Why is the sun so hot?\n",
        "\n",
        "**Things to think about and try**\n",
        "\n",
        "* What do you notice about the answers? Could we use this as a translation app, calculator, or currency converter?\n",
        "\n",
        "* How does varying the parameters vary the response? What do each of the parameters do?\n",
        "\n",
        "* What about varying the prompt? Can you get it to format your answers in different ways?"
      ],
      "metadata": {
        "id": "3bANSSiAv-e3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A simple few shot example\n",
        "\n",
        "The above examples are all cases of **zero shot learning** - we did not train the model in any way for our specific task (though of course it might hacve had lots of relevant training and conditioning when it was built).\n",
        "\n",
        "For the above examples, although we did get reasonable answers, we also got lots of text that we didn't neccesarily want. We can train the model to give us more specific answers, by providing examples. This is one case of **few shot learning**"
      ],
      "metadata": {
        "id": "Ha1SIlnZbmmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple few shot template, with three examples\n",
        "template = \"\"\"Translate English to French:\n",
        "\n",
        "        sea otter => loutre de mer\n",
        "        peppermint => menthe poivrée\n",
        "        plush girafe => girafe peluche\n",
        "        {word} =>\"\"\"\n",
        "\n",
        "# Make the prompt and chain\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "chain = prompt | llm\n",
        "\n",
        "# Invoke it\n",
        "english_word = \"cheese\"\n",
        "print(chain.invoke({\"word\": english_word}))\n"
      ],
      "metadata": {
        "id": "KgJN_fxXxDiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Can you improve the prompt?\n",
        "This works to some extent, but our prompt is not ideal. Can you change the prompt, so it gives us just one answer, the translation for the word we provided?"
      ],
      "metadata": {
        "id": "3c5epxMpzGlW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Medication Extraction using zero-shot learning\n",
        "\n",
        "We will now look at how we might use a generative model for medical Named Entity Recognition (NER). We will start with a simple zero-shot medication extraction experiment. Is it any use? How might we improve it?"
      ],
      "metadata": {
        "id": "qCyNMQyJ9QWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example document\n",
        "clinical_document = \"\"\"\n",
        "The patient was initially prescribed Metoprolol 50 mg twice daily for hypertension.\n",
        "During the first follow-up visit, the dosage of Metoprolol was increased to 100 mg twice daily.\n",
        "Later, the patient developed side effects and Metoprolol was switched to Atenolol 50 mg once daily.\n",
        "In the subsequent visit, Amlodipine 5 mg was added to the treatment plan.\n",
        "At the final follow-up, the Atenolol dosage was increased to 100 mg once daily, and Amlodipine was continued.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nG4lMpkz9WVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simple zero shot prompt\n",
        "def get_meds_response(text):\n",
        "\n",
        "    template = \"\"\"\n",
        "    Extract all medications and their doses from the following clinical document:\n",
        "\n",
        "    {text}\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate.from_template(template)\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({\"text\": text})\n",
        "    return response"
      ],
      "metadata": {
        "id": "ieYA_hUu-S8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_response = get_meds_response(clinical_document)\n",
        "print(\"Response:\\n\", extraction_response)"
      ],
      "metadata": {
        "id": "VZ12yOQX-h5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Llama 3's prompt format\n",
        "\n",
        "You can greatly improve the results of prompting if you know a little about the way in which a model was trained. We are using an **instruction trained** version of Llama 3. In addition to the next word prediciton taask, it has been conditioned on a large set of instructions and answers, in which the model has been rewarded for generating good output for instructions that have been provided in a pre-specified format. If we follow the same format in our prompting, we will be able to take advantage of this conditioning. Such instruction tuning is a common feature of LLMs, each having its own specific instruction format. Llama 3's looks like this:\n",
        "\n",
        "`<|begin_of_text|><|start_header_id|>system<|end_header_id|>`\n",
        "\n",
        "`{system message}<|eot_id|>`\n",
        "`<|start_header_id|>user<|end_header_id|>`\n",
        "\n",
        "`{user message}<|eot_id|>`\n",
        "`<|start_header_id|>assistant<|end_header_id|>`\n",
        "\n",
        "The prompt is split in to three parts, each part relating to a particular role in the \"conversation\"\n",
        "\n",
        "* **system** - here we can provide a message to tell the system how to behave, how to format it's output etc\n",
        "* **user** - the prompt that we want answering\n",
        "* **assistant** - the \"reply\" generated by Llama. This is the last role provided, and after seeing this, Llama will generate output.\n",
        "\n",
        "\n",
        "You can read more about this in the [Llama 3 prompt format documentation](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/)"
      ],
      "metadata": {
        "id": "oavwCDyaE7w0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXNxTlVvFLR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A function that will use the Llama 3 prompt format and zero-shot\n",
        "# learning to extract medications from the provided text.\n",
        "def get_llama3_meds_response(text):\n",
        "\n",
        "  # Create a template using the Llama 3 prompting format\n",
        "  template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "{system_prompt}<|eot_id|>\n",
        "<|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
        "\n",
        "  system_prompt = \"\"\"You are an AI assistant that extracts medications and their doses from health record text.\n",
        "  When you are given a piece of text, you will list all of the medications that are in the text.\"\"\"\n",
        "\n",
        "  prompt = PromptTemplate(\n",
        "      input_variables=[\"system_prompt\", \"user_prompt\"],\n",
        "      template=template\n",
        "  )\n",
        "\n",
        "  # Invoke the model\n",
        "  response = llm(prompt.format(system_prompt=system_prompt, user_prompt=text))\n",
        "  chain = prompt | llm\n",
        "  response = chain.invoke({\"system_prompt\":system_prompt, \"user_prompt\":text})\n",
        "  return response"
      ],
      "metadata": {
        "id": "E_Gnvz9vnE1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's try the prompt with our example document\n",
        "extraction_response = get_llama3_meds_response(clinical_document)\n",
        "print(\"Response:\\n\", extraction_response)"
      ],
      "metadata": {
        "id": "oUDDrwyspj7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try changing your prompt in different ways, to get:\n",
        "\n",
        "* A list with one item for each time a medication is mentioned, instead of one item for each uique medication\n",
        "* A list with both medications and doses?\n",
        "* A response without any leading or trailing commentary?\n",
        "* Try to get Llama to structure the output, so that medications and doses are marked in different ways (e.g. put brackets around the doses)\n",
        "* Can you get the output as JSON?"
      ],
      "metadata": {
        "id": "mNdEmQZywcNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Medications extraction with few-shot learning\n",
        "\n",
        "Let's try to improve our medications extraction by providing some examples. We will first use a basic, unformatted prompt."
      ],
      "metadata": {
        "id": "-eWyKYiLFuAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Few-shot examples for medication extraction. Each example is a dictionary with a piece of text, and\n",
        "# then what we would expect the model to generate for this example.\n",
        "med_examples = [\n",
        "    {\n",
        "        \"text\": \"The patient was prescribed Metformin 500 mg twice daily for diabetes. After two months, the dose was increased to 1000 mg twice daily.\",\n",
        "        \"extracted_medications\": \"[Metformin 500 mg twice daily], [Metformin 1000 mg twice daily]\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Initially, Atorvastatin 20 mg was prescribed. During the first follow-up, the dosage was increased to 40 mg. Later, Ezetimibe 10 mg was added.\",\n",
        "        \"extracted_medications\": \"[Atorvastatin 20 mg], [Atorvastatin 40 mg], [Ezetimibe 10 mg]\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "vtOggIjq--Dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A function that defines our few-shot prompt from some examples\n",
        "# and then runs the model with that prompt over the provided text.\n",
        "def few_shot_medication_extraction(text, examples):\n",
        "\n",
        "    # Create the prompt using list comprehension in a format string (!)\n",
        "    example_prompts = \"\\n\\n\".join([f\"Text: {ex['text']}\\nExtracted Medications: {ex['extracted_medications']}\" for ex in med_examples])\n",
        "\n",
        "    # Add the examples in to our full prompt\n",
        "    prompt = f\"Use the following examples to guide you on how to extract medications and their doses from the given clinical document.\\n\\nExamples:\\n{example_prompts}\\n\\nClinical document: {text}\\nExtracted Medications:\"\n",
        "\n",
        "    # Invoke the model with the prompt - we are not using chaining here, just plain invocation\n",
        "    response = llm(prompt)\n",
        "    return response"
      ],
      "metadata": {
        "id": "lmWz83Sa-6z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's run the function on our clinical document\n",
        "few_shot_extraction_result = few_shot_medication_extraction(clinical_document, med_examples)\n",
        "print(\"Response:\\n\", few_shot_extraction_result)"
      ],
      "metadata": {
        "id": "FeIsPRic_Cig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few shot prompting with Llama's prompt format\n",
        "\n",
        "The results in the last exercise were quite impressive. Can we improve on them with Llama's prompt format. Let's have a go."
      ],
      "metadata": {
        "id": "vCNY2HwqF6fC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function using Llama 3 prompt format to extract medications\n",
        "#m from the provided text, using the provoded examples\n",
        "def get_llama3_few_shot_meds_response(text, ecamples):\n",
        "\n",
        "  # Create a template using the Llama 3 prompting format\n",
        "  template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "{system_prompt}<|eot_id|>\n",
        "<|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
        "\n",
        "  # The system prompt - how the LLM should respond\n",
        "  system_prompt = \"\"\"You are an AI assistant that extracts medications and their doses from health record text.\n",
        "  When you are given a piece of text, you will list all of the medications that are in the text.\n",
        "  Here are some examples, showing how you should format the list:\\n\\n\"\"\"\n",
        "\n",
        "  # Few shot examples\n",
        "  example_prompts = \"\\n\\n\".join([f\"Text: {ex['text']}\\nExtracted Medications: {ex['extracted_medications']}\" for ex in med_examples])\n",
        "\n",
        "  # Make the full prompt\n",
        "  system_prompt = system_prompt + example_prompts\n",
        "  prompt = PromptTemplate(\n",
        "      input_variables=[\"system_prompt\", \"user_prompt\"],\n",
        "      template=template\n",
        "  )\n",
        "\n",
        "  # Invoke the model\n",
        "  response = llm(prompt.format(system_prompt=system_prompt, user_prompt=text))\n",
        "  chain = prompt | llm\n",
        "  response = chain.invoke({\"system_prompt\":system_prompt, \"user_prompt\":text})\n",
        "  return response"
      ],
      "metadata": {
        "id": "_HsZ1t0M2kNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Few-shot medication extraction with our Llama prompt format\n",
        "few_shot_extraction_result = get_llama3_few_shot_meds_response(clinical_document, med_examples)\n",
        "print(\"Response:\\n\", few_shot_extraction_result)"
      ],
      "metadata": {
        "id": "OYzXKQ6L3e3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Elaborating the prompt\n",
        "Can you elaborate the prompt further, to separate out the drug, does and frequency into separate features in the output that might be easily parsed in to e.g, a Python dictionary?"
      ],
      "metadata": {
        "id": "_4UaDWQD57Ou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: identifying and annotating symptoms\n",
        "\n",
        "Write and test prompts to mark mentions of symptoms in an input text"
      ],
      "metadata": {
        "id": "l_VL9zv5AZVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# An input document to annotate document\n",
        "clinical_document = \"\"\"\n",
        "The patient was admitted with severe headache, nausea, and dizziness.\n",
        "They also reported experiencing fatigue and joint pain.\n",
        "Additionally, there was mention of occasional shortness of breath and a persistent cough.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HfwBhX1nAhTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some examples of how we would like the output to be formatted\n",
        "examples = [\n",
        "    {\n",
        "        \"text\": \"The patient was given acetaminophen and complained of headaches and fatigue.\",\n",
        "        \"annotated\": \"The patient was given acetaminophen and complained of [headaches] and [fatigue].\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"She was treated with amoxicillin and experienced a rash and itching.\",\n",
        "        \"annotated\": \"She was treated with amoxicillin and experienced a [rash] and [itching].\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "7LHZqbC2AoZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: extending the symptom extractor to include character offsets\n",
        "Can you get your symptom annotation example to tell you the start and finish character offsets of the symptoms, like in these examples?\n",
        "\n"
      ],
      "metadata": {
        "id": "6hFckUtGBOPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"text\": \"The patient was given acetaminophen and complained of headaches and fatigue.\",\n",
        "        \"annotations\": \"[symptom: headaches, start_offset: 54, end_offset: 63], [symptom: fatigue, start_offset: 68, end_offset: 75]\"\n",
        "\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"She was treated with amoxicillin and experienced a rash and itching.\",\n",
        "        \"annotations\": \"[symptom: rash, start_offset: 51, end_offset: 55], [symptom: itching, start_offset: 60, end_offset: 67]\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "N6TWTwZdBf43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The assignment\n",
        "\n",
        "Can you develop prompts to provide an answer for the assignment?\n",
        "\n",
        "You will probably have to use a free online LLM, such as ChatGPT, a free Hugging Face or [Perplexity online model](https://www.perplexity.ai/hub/blog/introducing-pplx-online-llms?utm_source=labs&utm_medium=labs&utm_campaign=online-llms). This means you are unlikley to be able to evaluate agaist a large dataset. But, you might be able to show some results with minimal training, and you might be able to do a very small evaluation - a limitation. What other limitations might there be? Will the test data be blind?"
      ],
      "metadata": {
        "id": "CJAfWyDaEIKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next steps\n",
        "This notebook has shown you some of the basics of prompt engineering. Recent models and APIs are much more sophisticated than the exmaples we have given, and often provide facilities for more detailed interaction. There is much more to learn about if you are interested:\n",
        "\n",
        "* output parsing\n",
        "* tool and function calls\n",
        "* direct support for e.g. JSON\n",
        "* different capabilities of different models"
      ],
      "metadata": {
        "id": "xRcOznlORgVw"
      }
    }
  ]
}