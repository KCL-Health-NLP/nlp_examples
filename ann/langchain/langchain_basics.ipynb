{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt engineering with LangChain - an NER example\n",
        "\n",
        "*Written by Jaya Chaturvedi and Angus Roberts, May 2024*\n",
        "\n",
        "---\n",
        "\n",
        "###Generative models###\n",
        "In pevious practicals, we have used the BERT transfomer model. To recap, this model is:\n",
        "\n",
        "* the encoder layer of an encoder-decoder\n",
        "* trained on a masked word prediction task, and\n",
        "* trained on a next sentence prediction task\n",
        "* trained on 3.3 billion words\n",
        "* 110 million parameters\n",
        "\n",
        "We are now going to look at generative large language models, such as the GPT models. Such models are:\n",
        "\n",
        "* the decoder layer of an encoder-decoder\n",
        "* trained on a text generation task (predict the next word), and\n",
        "* conditioned on other tasks, such as following instructions\n",
        "* trained on terrabytes or petabytes of text\n",
        "* Billions of parameters\n",
        "\n",
        "This practical has been written for use with an 8 billion parameter version of Llama 3, so more thsana 70 times larger then BERT. It has been instruction tuned and optimized for dialogue use cases. There is also an 80 billion parameter version available.\n",
        "\n",
        "###Prompting###\n",
        "These models generate text. You can play with them, asking them to write songs, poems etc etc, typing text at a **prompt**, with the model completing the text you have started by generating the next N words. What you get depends partly on the model, partly on model parameters you set, but also partly on the way in which you ask the question - i.e. your **prompt engineering**.\n",
        "\n",
        "We will look at how we might design prompts to generate output in a consistent format a task, specifically for NER.\n",
        "\n",
        "###Hosting the model - Hugging Face###\n",
        "\n",
        "These models are large! You could put them on your laptop, but (a) they will take time to download and (b) they might dwarf it's memory. So, we will use a remotely hosted model, on Hugging Face. Hugging Face has several publically availabel models. For Llama 3, we are using a paid-for model hosting service. This gives us more control over availability, scaling etc.\n",
        "\n",
        "###Interacting with the model - LangChain###\n",
        "\n",
        "All models have different ways of interacting with them, whether via a prompt or an API. We will be using a widely-used Python library that hides these differences behind a common API, LangChain. LangChain will work with many different models, both remotely hosted and locally hosted. So you should be able to re-purpose this code for other models and situations.\n",
        "\n",
        "LangChain is big - we will only look at a few of LangChain's features, and some basic prompting strategies.\n",
        "\n",
        "There are often other ways, but we. hope that this practical will show you the basics.\n",
        "\n",
        "###Further information and resources###\n",
        "\n",
        "* We would like to recommend Chapter 12 in the 3rd edition of Jurafsky and Martin's \"Speech and Language Processing\" - but it hasn't yet been written!\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "1itC_4Z1tdtg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "LyQ3t2iXVou7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFVXCbyIUImN"
      },
      "outputs": [],
      "source": [
        "# LangChain needs to have the Hugging Face transformers package installed,\n",
        "# and we need several LangChain packages\n",
        "%pip install --upgrade --quiet transformers\n",
        "%pip install --upgrade --quiet langchain langchain_community\n",
        "%pip install --upgrade --quiet langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The most important imports are her, though we will import\n",
        "# a small number of other packages later, for specific pieces of code\n",
        "import langchain_community\n",
        "from langchain_huggingface.llms import HuggingFaceEndpoint\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "I_UanP0nUdV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Huggingface endpoints\n",
        "\n",
        "Adapted from https://python.langchain.com/docs/integrations/llms/huggingface_pipelines\n",
        "\n",
        "\n",
        "From https://python.langchain.com/docs/integrations/llms/huggingface_endpoint\n",
        "\n",
        "Huggingface serverless inference is described here:\n",
        "\n",
        "https://huggingface.co/docs/api-inference/index\n",
        "\n",
        "This explains how to get an API token from your account, and how to construct the model URL. It also gives code for using a model, but we will instead used Langchain to interface to the Huggingface API.\n",
        "\n",
        "You can set up your own dedicated endpoint to which you deploy a model, giving better availability than the public endpoints. There is a cost:\n",
        "\n",
        "https://huggingface.co/inference-endpoints/dedicated\n",
        "\n",
        "UI for starting and stopping and configuring endpoints is here:\n",
        "\n",
        "https://ui.endpoints.huggingface.co/angusroberts/endpoints\n",
        "\n",
        "\n",
        "protected endpoint - seems to need own token\n",
        "public endpoint - still needs a token, but can be any\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TTrGwafqt4tB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# You need to get your API token from huggingface, run this cell and paste\n",
        "# it in to the resulting prompt, for use in later sections\n",
        "# How to get a token is described here:\n",
        "# https://huggingface.co/docs/api-inference/quicktour#get-your-api-token\n",
        "\n",
        "# getpass provides an obscured password prompt.\n",
        "# os gives access to operating ssytem functionality, which\n",
        "# we need to set an environment-wide variable to hold our token\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "HUGGINGFACEHUB_API_TOKEN = getpass()\n",
        "\n",
        "# We put the token in an environment variable, from where LangChain will access it when needed\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
      ],
      "metadata": {
        "id": "-st7mi8CyvOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Querying a Hugging Face LLM server directly"
      ],
      "metadata": {
        "id": "Ws1Wj5PfaCP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This example uses the Hugging Face service API direct\n",
        "# to access a freely available GPT2 model hosted\n",
        "# on Hugging Face\n",
        "\n",
        "# requests is a package to send requets to web servers\n",
        "import requests\n",
        "\n",
        "# the Hugging Face API server for GPT2\n",
        "API_URL = \"https://api-inference.huggingface.co/models/gpt2\"\n",
        "\n",
        "# Headers for our request - the token\n",
        "headers = {\"Authorization\": f\"Bearer {HUGGINGFACEHUB_API_TOKEN}\"}\n",
        "\n",
        "# function to post the request and return the response\n",
        "# takes a json query as the payload\n",
        "def query(payload):\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    return response.json()\n",
        "\n",
        "# Send a query and print the results\n",
        "data = query({\"inputs\": \"Can you please let us know more details about your \"})\n",
        "print(data)"
      ],
      "metadata": {
        "id": "2sXMlU0lyhVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A simple LangChain LLM, using a Hugging Face free endpoint\n",
        "\n",
        "Next we will see how to create a LangChain LLM that wraps a Hugging Face endpoint\n",
        "\n",
        "What is the advantage of this approach, over querying directly as in the above example?"
      ],
      "metadata": {
        "id": "S4Km8r82aLHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also use these free model endpoints via Lang Chain, an API that wraps up\n",
        "# many different ways of accessing models in a common interface.\n",
        "# Here we use it to access a public Hugging Face endpoint which has limited models,\n",
        "# and no guarantee on performance or availability\n",
        "\n",
        "repo_id = \"openai-community/gpt2\"\n",
        "llm = HuggingFaceEndpoint(\n",
        "   endpoint_url=\"https://api-inference.huggingface.co/models/\" + repo_id,\n",
        "   task=\"text-generation\",\n",
        "   temperature = 0.1,\n",
        "   model_kwargs={\"max_length\": 128}\n",
        ")\n",
        "llm(\"When I went to Paris, I \")\n"
      ],
      "metadata": {
        "id": "rfXMhhdyrJmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using a paid-for endpoint\n",
        "\n",
        "We can use exactly the same code for a paid-for endpoint, or for a local model. Here, we connect to a paid-for endpoint, which we will use for the rest of the practical.\n",
        "\n",
        "What are the pros and cons of using these different methods of model delivery?"
      ],
      "metadata": {
        "id": "kx2mKqAWa2va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the paid for model endpoint, which can host a wider range of models\n",
        "# This is the url of a paid for endpoint - replace with whichever you are using\n",
        "# You need to enter the URL provided for the practical!\n",
        "endpoint_url = \"PUT THE PROVIDED ENDPOINT URL HERE\"\n",
        "\n"
      ],
      "metadata": {
        "id": "haa9Lmuo1blM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=endpoint_url,\n",
        "    max_new_tokens=256,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    typical_p=0.95,\n",
        "    temperature=0.01,\n",
        "    repetition_penalty=1.03,\n",
        ")\n",
        "llm(\"My favourite joke is \")"
      ],
      "metadata": {
        "id": "M-4UmI9U-Gy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing prompts\n",
        "\n",
        "Reuse"
      ],
      "metadata": {
        "id": "V8xXh27QbWJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a prompt / LLM chain and running it\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "chain = prompt | llm\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6fT155Xnz9-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Why is the sun so hot?\"\n",
        "print(chain.invoke({\"question\": question}))"
      ],
      "metadata": {
        "id": "M1I_ck_cbcuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try a few other questions:\n",
        "\n",
        "* What is the French for cheese?\n",
        "* What is 2+2 ?\n",
        "* How many dollars to the euro?\n",
        "* Why is the sun so hot?\n",
        "\n",
        "What do you notice about the answers? Could we use this as a translation app, calculator, or currency converter?\n",
        "\n",
        "How does varying the parameters vary the response? What do each of the parameters do?\n",
        "\n",
        "What about varying the prompt? Can you get it to format your answers in different ways?"
      ],
      "metadata": {
        "id": "3bANSSiAv-e3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A simple few shot example\n",
        "\n",
        "building on the above"
      ],
      "metadata": {
        "id": "Ha1SIlnZbmmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple few shot learning\n",
        "\n",
        "template = \"\"\"Translate English to French:\n",
        "\n",
        "        sea otter => loutre de mer\n",
        "        peppermint => menthe poivrÃ©e\n",
        "        plush girafe => girafe peluche\n",
        "        {word} =>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "chain = prompt | llm\n",
        "\n",
        "english_word = \"cheese\"\n",
        "print(chain.invoke({\"word\": english_word}))\n"
      ],
      "metadata": {
        "id": "KgJN_fxXxDiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you get it to give us just one answer?"
      ],
      "metadata": {
        "id": "3c5epxMpzGlW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Medication Extraction using zero-shot and few-shot learning"
      ],
      "metadata": {
        "id": "qCyNMQyJ9QWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example document\n",
        "clinical_document = \"\"\"\n",
        "The patient was initially prescribed Metoprolol 50 mg twice daily for hypertension.\n",
        "During the first follow-up visit, the dosage of Metoprolol was increased to 100 mg twice daily.\n",
        "Later, the patient developed side effects and Metoprolol was switched to Atenolol 50 mg once daily.\n",
        "In the subsequent visit, Amlodipine 5 mg was added to the treatment plan.\n",
        "At the final follow-up, the Atenolol dosage was increased to 100 mg once daily, and Amlodipine was continued.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nG4lMpkz9WVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simple prompt\n",
        "def get_meds_response(text):\n",
        "\n",
        "    template = \"\"\"\n",
        "    Extract all medications and their doses from the following clinical document:\n",
        "\n",
        "    {text}\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate.from_template(template)\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({\"text\": text})\n",
        "    return response"
      ],
      "metadata": {
        "id": "ieYA_hUu-S8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_response = get_meds_response(clinical_document)\n",
        "print(\"Response:\\n\", extraction_response)"
      ],
      "metadata": {
        "id": "VZ12yOQX-h5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Llama 3 prompt format\n",
        "\n",
        "We can improve our interaction by using a pre-specified prompt format with which Llama has been trained. This forces the model to constrain outputm and gives much cleaner results"
      ],
      "metadata": {
        "id": "oavwCDyaE7w0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXNxTlVvFLR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using Llama 3 prompt format\n",
        "def get_llama3_meds_response(text):\n",
        "\n",
        "  # Create a template using the Llama 3 prompting format\n",
        "  template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "{system_prompt}<|eot_id|>\n",
        "<|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
        "\n",
        "  system_prompt = \"\"\"You are an AI assistant that extracts medications and their doses from health record text.\n",
        "  When you are given a piece of text, you will list all of the medications that are in the text.\"\"\"\n",
        "\n",
        "  prompt = PromptTemplate(\n",
        "      input_variables=[\"system_prompt\", \"user_prompt\"],\n",
        "      template=template\n",
        "  )\n",
        "\n",
        "  # Invoke the model\n",
        "  response = llm(prompt.format(system_prompt=system_prompt, user_prompt=text))\n",
        "  chain = prompt | llm\n",
        "  response = chain.invoke({\"system_prompt\":system_prompt, \"user_prompt\":text})\n",
        "  return response"
      ],
      "metadata": {
        "id": "E_Gnvz9vnE1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_response = get_llama3_meds_response(clinical_document)\n",
        "print(\"Response:\\n\", extraction_response)"
      ],
      "metadata": {
        "id": "oUDDrwyspj7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try changing your prompt in different ways, to get:\n",
        "\n",
        "* A list with one item for each time a medication is mentioned, instead of one item for each uique medication\n",
        "* A list with both medications and doses?\n",
        "* A response without any leading or trailing commentary?\n",
        "* Try to get Llama to structure the output, so that medications and doses are marked in different ways (e.g. put brackets around the doses)\n",
        "* Can you get the output as JSON?"
      ],
      "metadata": {
        "id": "mNdEmQZywcNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few shot prompting with output examples"
      ],
      "metadata": {
        "id": "-eWyKYiLFuAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Few-shot examples for medication extraction\n",
        "med_examples = [\n",
        "    {\n",
        "        \"text\": \"The patient was prescribed Metformin 500 mg twice daily for diabetes. After two months, the dose was increased to 1000 mg twice daily.\",\n",
        "        \"extracted_medications\": \"[Metformin 500 mg twice daily], [Metformin 1000 mg twice daily]\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Initially, Atorvastatin 20 mg was prescribed. During the first follow-up, the dosage was increased to 40 mg. Later, Ezetimibe 10 mg was added.\",\n",
        "        \"extracted_medications\": \"[Atorvastatin 20 mg], [Atorvastatin 40 mg], [Ezetimibe 10 mg]\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "vtOggIjq--Dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# few-shot\n",
        "def few_shot_medication_extraction(text, examples):\n",
        "    example_prompts = \"\\n\\n\".join([f\"Text: {ex['text']}\\nExtracted Medications: {ex['extracted_medications']}\" for ex in med_examples])\n",
        "    prompt = f\"Use the following examples to guide you on how to extract medications and their doses from the given clinical document.\\n\\nExamples:\\n{example_prompts}\\n\\nClinical document: {text}\\nExtracted Medications:\"\n",
        "    response = llm(prompt)\n",
        "    return response"
      ],
      "metadata": {
        "id": "lmWz83Sa-6z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Few-shot medication extraction\n",
        "few_shot_extraction_result = few_shot_medication_extraction(clinical_document, med_examples)\n",
        "print(\"Response:\\n\", few_shot_extraction_result)"
      ],
      "metadata": {
        "id": "FeIsPRic_Cig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few shot prompting with Llama's prompt format"
      ],
      "metadata": {
        "id": "vCNY2HwqF6fC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using Llama 3 prompt format\n",
        "def get_llama3_few_shot_meds_response(text):\n",
        "\n",
        "  # Create a template using the Llama 3 prompting format\n",
        "  template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "{system_prompt}<|eot_id|>\n",
        "<|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
        "\n",
        "  system_prompt = \"\"\"You are an AI assistant that extracts medications and their doses from health record text.\n",
        "  When you are given a piece of text, you will list all of the medications that are in the text.\n",
        "  Here are some examples, showing how you should format the list:\\n\\n\"\"\"\n",
        "\n",
        "  example_prompts = \"\\n\\n\".join([f\"Text: {ex['text']}\\nExtracted Medications: {ex['extracted_medications']}\" for ex in med_examples])\n",
        "\n",
        "  system_prompt = system_prompt + example_prompts\n",
        "\n",
        "\n",
        "  prompt = PromptTemplate(\n",
        "      input_variables=[\"system_prompt\", \"user_prompt\"],\n",
        "      template=template\n",
        "  )\n",
        "\n",
        "  # Invoke the model\n",
        "  response = llm(prompt.format(system_prompt=system_prompt, user_prompt=text))\n",
        "  chain = prompt | llm\n",
        "  response = chain.invoke({\"system_prompt\":system_prompt, \"user_prompt\":text})\n",
        "  return response"
      ],
      "metadata": {
        "id": "_HsZ1t0M2kNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Few-shot medication extraction\n",
        "few_shot_extraction_result = get_llama3_few_shot_meds_response(clinical_document)\n",
        "print(\"Response:\\n\", few_shot_extraction_result)"
      ],
      "metadata": {
        "id": "OYzXKQ6L3e3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you elaborate the prompt further, to separate out the drug, does and frequency into separate features in the output that might be easily parsed in to e.g, a dictionary?"
      ],
      "metadata": {
        "id": "_4UaDWQD57Ou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identifying and annotating symptoms\n",
        "\n",
        "Write and test prompts to mark mentions of symptoms in an input text"
      ],
      "metadata": {
        "id": "l_VL9zv5AZVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# An input document to annotate document\n",
        "clinical_document = \"\"\"\n",
        "The patient was admitted with severe headache, nausea, and dizziness.\n",
        "They also reported experiencing fatigue and joint pain.\n",
        "Additionally, there was mention of occasional shortness of breath and a persistent cough.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HfwBhX1nAhTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some examples of how we wopuld like the output to be formatted\n",
        "examples = [\n",
        "    {\n",
        "        \"text\": \"The patient was given acetaminophen and complained of headaches and fatigue.\",\n",
        "        \"annotated\": \"The patient was given acetaminophen and complained of [headaches] and [fatigue].\"\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"She was treated with amoxicillin and experienced a rash and itching.\",\n",
        "        \"annotated\": \"She was treated with amoxicillin and experienced a [rash] and [itching].\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "7LHZqbC2AoZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extending\n",
        "Can you get your symptom annotation example to tell you the start and finish character offsets of the symptoms, like in these examples?\n",
        "\n"
      ],
      "metadata": {
        "id": "6hFckUtGBOPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"text\": \"The patient was given acetaminophen and complained of headaches and fatigue.\",\n",
        "        \"annotations\": \"[symptom: headaches, start_offset: 54, end_offset: 63], [symptom: fatigue, start_offset: 68, end_offset: 75]\"\n",
        "\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"She was treated with amoxicillin and experienced a rash and itching.\",\n",
        "        \"annotations\": \"[symptom: rash, start_offset: 51, end_offset: 55], [symptom: itching, start_offset: 60, end_offset: 67]\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "N6TWTwZdBf43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next steps\n",
        "This notebook has shown you some of the basics of prompt engineering. Recent models and APIs are much more sophisticated than the exmaples we have given, and often provide facilities for more detailed interaction:\n",
        "\n",
        "* output parsing\n",
        "* tool and function calls\n",
        "* direct support for e.g. JSON"
      ],
      "metadata": {
        "id": "CJAfWyDaEIKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The assignment\n",
        "\n",
        "Can you develop prompts to provide an answer for the assignment?\n",
        "\n",
        "You will probably have to use a free online LLM, such as ChatGPT, a free Hugging Face or perplexity online model. This means you are unlikley to be able to evaluate agaist a large dataset. But, you might be able to show some results with minimal training, and you might be able to do a very small evaluation - a limitation. What other limitations might there be? Will the test data be blind?"
      ],
      "metadata": {
        "id": "xRcOznlORgVw"
      }
    }
  ]
}