{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Loading local Huggingface models"
      ],
      "metadata": {
        "id": "1itC_4Z1tdtg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From https://python.langchain.com/docs/integrations/llms/huggingface_pipelines\n"
      ],
      "metadata": {
        "id": "LyQ3t2iXVou7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFVXCbyIUImN"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet  transformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community"
      ],
      "metadata": {
        "id": "t6SOYvqcUnpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain_community"
      ],
      "metadata": {
        "id": "I_UanP0nUdV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this version works on the public servers without an API token, but is slower\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "\n",
        "hf = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"gpt2\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\"max_new_tokens\": 100},\n",
        ")"
      ],
      "metadata": {
        "id": "k4nz1-EzUWkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "chain = prompt | hf\n",
        "\n",
        "question = \"How much is that doggy in the window?\"\n",
        "\n",
        "print(chain.invoke({\"question\": question}))"
      ],
      "metadata": {
        "id": "Z6E8Mrm1U4-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Huggingface endpoints\n",
        "\n",
        "From https://python.langchain.com/docs/integrations/llms/huggingface_endpoint\n",
        "\n",
        "Huggingface serverless inference is described here:\n",
        "\n",
        "https://huggingface.co/docs/api-inference/index\n",
        "\n",
        "This explains how to get an API token from your account, and how to construct the model URL. It also gives code for using a model, but we will instead used Langchain to interface to the Huggingface API.\n",
        "\n",
        "You can set up your own dedicated endpoint to which you deploy a model, giving better availability than the public endpoints. There is a cost:\n",
        "\n",
        "https://huggingface.co/inference-endpoints/dedicated\n",
        "\n",
        "UI for starting and stopping and configuring endpoints is here:\n",
        "\n",
        "https://ui.endpoints.huggingface.co/angusroberts/endpoints\n",
        "\n",
        "\n",
        "protected endpoint - seems to need own token\n",
        "public endpoint - still needs a token, but can be any\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TTrGwafqt4tB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-token\n",
        "\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "HUGGINGFACEHUB_API_TOKEN = getpass()\n",
        "\n",
        "# We put the token in an environment variable, from where Langchain will access it when needed\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
      ],
      "metadata": {
        "id": "-st7mi8CyvOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This example uses the Huggingface API direct\n",
        "import requests\n",
        "API_URL = \"https://api-inference.huggingface.co/models/gpt2\"\n",
        "headers = {\"Authorization\": f\"Bearer {HUGGINGFACEHUB_API_TOKEN}\"}\n",
        "def query(payload):\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    return response.json()\n",
        "data = query(\"When it rains it \")\n",
        "print(data)"
      ],
      "metadata": {
        "id": "2sXMlU0lyhVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community\n",
        "import langchain_community\n",
        "from langchain_community.llms import HuggingFaceEndpoint"
      ],
      "metadata": {
        "id": "UJQpQy0nt9_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet huggingface_hub"
      ],
      "metadata": {
        "id": "kdJ8MvXc0HZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "crUSpppn0OQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint_url = \"https://j278zkynwm0b3dky.eu-west-1.aws.endpoints.huggingface.cloud\""
      ],
      "metadata": {
        "id": "eT8dB6qODpsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l-zUwNdL0enT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w91QGctn1D47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "iJbQV1LW1Hm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How much is that doggy in the window? \"\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "z3GLT_871P6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Using the free model endpoint, which has limited models\n",
        "#\n",
        "# repo_id = \"openai-community/gpt2\"\n",
        "# llm = HuggingFaceEndpoint(\n",
        "#    endpoint_url=\"https://api-inference.huggingface.co/models/\" + repo_id,\n",
        "#    task=\"text-generation\",\n",
        "#    model_kwargs={\"max_length\": 128, \"temperature\": 0.1}\n",
        "#)\n",
        "\n",
        "# Using the paid for model endpoint, which can host a wider range of models\n",
        "#llm = HuggingFaceEndpoint(\n",
        "#    endpoint_url=endpoint_url,\n",
        "#    task=\"text-generation\",\n",
        "#    temperature=0.1,\n",
        "#    model_kwargs={\"max_length\": 128}\n",
        "#)\n",
        "\n",
        "\n",
        "# TRY THIS NEXT:\n",
        "llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=endpoint_url,\n",
        "    max_new_tokens=512,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    typical_p=0.95,\n",
        "    temperature=0.01,\n",
        "    repetition_penalty=1.03,\n",
        ")\n",
        "llm(\"What did foo say about bar?\")\n",
        "\n",
        "\n",
        "#llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "#print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "haa9Lmuo1blM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}